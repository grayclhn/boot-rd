\documentclass[12pt,fleqn]{article}

% Add commit information from Git to the pdf. It is automatically
% generated with the R script 'tex/git and can be run from
% the command line in the project's top directory via:
%
% $ tex/git
%
% If VERSION.tex does not exist we can't add information from
% Git, so we'll use today's date as a fallback.

\IfFileExists{./VERSION.tex}{\input{VERSION}}{%
\providecommand\VERSION{\today}}

\input{tex/setup}
\input{tex/macros}

\title{Bootstrap Confidence Intervals for Sharp Regression Discontinuity Designs}

\author{Ot\'avio Bartalotti \and Gray Calhoun \and Yang He\thanks{%
  All authors: Department of Economics, Iowa State University.
  260 Heady Hall, Ames, IA 50011.
  Bartalotti: \protect\url{bartalot@iastate.edu};
  Calhoun: \protect\url{gcalhoun@iastate.edu} and
  \protect\url{http://gray.clhn.org};
  He: \protect\url{yanghe@iastate.edu}. Source code for the Monte Carlo
  experiments and empirical component of this paper are available online
  at \protect\url{https://github.com/grayclhn/boot-rd}. We would like to
  thank Matias Cattaneo, Juan Carlos Escanciano, Sebastian Calonico, Max Farrell, Helle Bunzel, Brent Kreider, Quentin Brummet, seminar participants at
  the Advances in Econometrics conference on Regression Discontinuity
  Designs, Third Annual Conference of the International Association for Applied Econometrics,
 2016 North American Summer Meeting of the Econometric Society and the $69^{th}$ European Meeting of the Econometric Society, and especially the two anonymous referees for
  substantial help and advice on earlier versions of this paper.}}

\begin{document}
\maketitle

\begin{abstract}\noindent
  This paper develops a novel bootstrap procedure to obtain robust
  bias-corrected confidence intervals in regression discontinuity (RD) designs. The procedure uses a wild bootstrap from a
  second order local polynomial to estimate the bias of the local linear RD
  estimator; the bias is then subtracted from the original estimator. The
  bias-corrected estimator is then bootstrapped itself to generate valid
  confidence intervals. The confidence intervals generated by this procedure
  are valid under conditions similar to Calonico, Cattaneo and Titiunik's
  (2014, \textit{Econometrica}) analytical correction---i.e.  when the bias of
  the naive regression discontinuity estimator would otherwise prevent valid
  inference. This paper also provides simulation evidence that our method is
  as accurate as the analytical corrections and we demonstrate its use through
  a reanalysis of Ludwig and Miller's (2008) Head Start dataset.
\end{abstract}

\section{Introduction}
Regression Discontinuity (RD) designs have emerged in the last decade as an
important and popular research design strategy for analyzing the causal impact
of policies and interventions in several fields of the social sciences,
including economics, political science, public policy, and sociology.  This
research strategy exploits the fact that many programs use a threshold based
on a numeric score to determine whether or not to provide a
treatment.\footnote{This score is often referred to as the \textit{running variable} in this
  literature.}
In its basic version, \textit{sharp RD}, individuals or groups with score
above the threshold are treated while those below the threshold are left
untreated. The identification of the treatment effect at the threshold is then
based on comparing treated and untreated units at the cutoff. When a subject's
position just above or below the cutoff is credibly not related to unobserved
characteristics that would affect the outcome of interest, differences between
treated and untreated individuals at the cutoff can be plausibly attributed to
the treatment alone. As a practical matter this involves comparing units
within a bandwidth just above and just below the threshold. Other RD
strategies exist that can exploit different forms of discontinuities as well.

The RD design strategy was introduced by \cite{thistlethwaite1960} to study
educational outcomes and many of its recent applications in economics were to
estimate the effects of other educational policies: evaluating the impact of
investments in school facilities, class sizes, remedial education, early
childhood education, and financial aid effects on student achievement and
later outcomes, for example.\footnote{%
  See, for example, \cite{vdk2002}, \cite{jacoblefgren2004},
  \cite{ludwig2007}, \cite{urquiola2009}, and \cite{cellini2010}.} %
But the underlying identification strategy has proven to apply much more
widely and RD has been used in health economics,\footnote{%
  \cite{Card2009}, \cite{barreca2011saving}, and \cite{KeeleEtAl}.} %
political science,\footnote{%
  \cite{lee2008}, \cite{caughey2011elections}, \cite{keele2014geographic},
  \cite{erikson2015}, and \cite{Fujiwara2011,Fujiwara2015}.} %
and labor economics,\footnote{%
  \cite{schmieder2012} and \cite{frandsen}.} %
among other fields. \cite{imbens2008} and \cite{lee2010} provide recent
overviews of this literature with many more examples.

In these studies, identification occurs exactly at the cutoff, so the
treatment effect is typically estimated by fitting separate local linear
models above and below the cutoff, then extrapolating the models to the exact
point of discontinuity. The difference between the estimated outcomes at that
point is taken to be an estimate of the treatment effect. As a practical
matter, a key econometric issue is determining the bandwidth for the local
linear models.  One very popular choice is the bandwidth estimator proposed by
\cite{IK} and extended by \cite{calonico2014}, which minimizes the Asymptotic
Mean Squared Error (AMSE) of difference in the models' point estimators at the
cutoff.  But, as observed by \cite{calonico2014}, henceforth ``CCT,''
the AMSE-optimal bandwidth has the serious drawback that it produces invalid
confidence intervals and hypothesis tests. Local polynomial estimators of the
treatment effect are generally biased in finite samples because the functional
form of the local conditional expectation that they need to approximate is unknown.
The unmodeled component of the conditional expectation becomes smaller as the
bandwidth itself becomes smaller, so the estimator's bias vanishes
asymptotically as long as the bandwidth shrinks as the sample size increases.
AMSE-optimal bandwidth shrinks as the sample size increases, so its estimator
of the treatment effect is consistent, but the bandwidth shrinks slowly enough
that the remaining bias term is large enough to affect the asymptotic
distribution of the estimator. Consequently the usual ``naive'' confidence
intervals for the RD treatment effects are invalid and can have coverage well
below their nominal level.

CCT show that the bias resulting from undersmoothing can be estimated and they
provide a bias-corrected treatment effect estimator that remains
asymptotically unbiased even when the bandwidth converges to zero at the
AMSE-optimal rate. They also show that this bias-correction term contributes
to the asymptotic variance of the resulting treatment effect estimator and
provide a new formula for the asymptotic variance of the bias-corrected
estimator. The resulting confidence intervals have accurate coverage even when
the naive RD interval does not.
Furthermore, as shown by \citet{CCF15}, these intervals can achieve faster
convergence rates than previously believed, outperforming intervals produced
by undersmoothing in some settings while maintaining grater robustness to
the choice of bandwidth parameter. Previous research by \citet{Hall93} and
\citet{Neumann97} has found that undersmoothing outperforms naive
bias-corrected confidence intervals, but \citet{CCF15} show that the naive
variance estimator implicitly requires more restrictions than CCT's
expression and those restrictions artificially limit the intervals'
performance.

In this paper, we propose a bootstrap alternative to CCT's analytical
corrections. CCT motivate their estimator by showing that the bias and
variance components for the local linear estimator can be accounted for by
estimating a local second order polynomial with bandwidth of the same
order.\footnote{%
  More generally, they show that the bias and variance of a local polynomial
  of order $p$ can be accounted for by estimating the $p+1$ local
  polynomial. We will restrict our analysis to the case with $p = 1$ in this
  paper because of its widespread use.} %
They use a Taylor expansion around the cutoff to show that the bias associated
with the second order polynomial converges to zero at a faster rate, fast
enough that the bias of the local linear model can be estimated and removed
using the second order polynomial. Additionally, that approximation provides
fast enough convergence that it can be used to estimate the correct variance
correction as well.

Our approach exploits CCT's theoretical insight through a new wild
bootstrap. In particular, we propose estimating the local linear model as usual,
but generating bootstrap datasets from an approximation provided by a local
second order polynomial. Since the second order polynomial is the true Data
Generating Process (DGP) for the bootstrapped dataset,
its estimate of the treatment effect is the true value of the treatment effect
under the distribution induced by this bootstrap. The bias of the linear model
is therefore known
under this distribution and can be calculated by averaging the error of the
linear model's estimates across many bootstrap replications. This approach is
described in detail by our Algorithm~\ref{Alg1} and the resulting bias corrected
estimator is shown to be equivalent to CCT's, up to simulation error in the
bootstrap, and asymptotically normal with mean zero in our
Theorem~\ref{T1} under AMSE-optimal bandwidth rates.

Just as in CCT, our bias correction step introduces additional
variability. However, the second order polynomial again adequately estimates the
features of the true DGP that are necessary for estimating and accommodating that
additional variability. So we propose
an iterated bootstrap procedure \citep{hall1988}: use the second order
polynomial bootstrap to produce many bootstrap replications of the bias
corrected estimator, and then use the resulting bootstrap distribution to
produce confidence intervals. This procedure, which requires bootstrapping the
datasets produced by an initial bootstrap, is described in Algorithm~\ref{Alg2}
and the resulting confidence intervals are shown to be asymptotically valid in
Theorem~\ref{T2}.\footnote{%
  Since our bootstrap resamples the residuals but not the running variable, many
  of the quantities used to produce the estimates do not change under the
  bootstrap distribution and can be reused from the original
  estimate. Exploiting this feature makes the iterated bootstrap much less
  computationally burdensome; see the discussion following
  Algorithm~\ref{Alg2}.} %
Simulation results presented in Section~\ref{sim} indicate that this iterated
bootstrap provides very similar results in finite samples to CCT's analytical
confidence intervals as well.

The methods presented in this paper are intended as a ``proof of concept'' that
demonstrates that the analytical correction proposed by CCT can also be
implemented through a new bootstrap procedure.  This procedure is promising
because it, like other bootstrap methods, can allow applied researchers to
construct accurate confidence intervals in settings where correct formulas for
the asymptotic variance have not yet been derived, as can be the case when there
is an unusual form of dependence---the dependence
can be accommodated in principle by modifying the resampling algorithm to one
that matches the dependence in the dataset. The specific version of the bootstrap
presented in this paper, however, does not achieve that level of generality,
and one limitation should be noted in particular. The confidence
intervals produced by this bootstrap depend on two separate bandwidth parameters
that must be generated elsewhere. Since these bandwidths are generated using
analytical methods, the confidence intervals could easily be produced using the
same methods. Moreover, in this paper we focus on the baseline case of
sharp RD with a local linear model, however, the results and procedures can be
easily extended to the general local polynomial and higher order derivatives
cases (e.g., Regression Kink Designs, see \citealp{card2009b}, and
\citealp{CardEtAl1,CardEtAl2}).

The paper is organized as follows. Section~\ref{background} describes the basic
RD approach, its usual implementation, and the explicit analytical bias
correction approach in the literature. Section~\ref{boot} presents our proposed
bootstrap bias corrected RD algorithm and discusses its asymptotic
properties. Simulation evidence that the bootstrap procedure provides valid CIs
and its relative performance to the analytical bias correction are presented in
Section~\ref{sim} and Section~\ref{application} demonstrates the estimator's
usage by applying it to the Head Start dataset used
by~\cite{ludwig2007}.\footnote{%
  The simulations and empirical analysis were carried out in the R programming
  language \citep{R} and rely on the \textit{rdrobust}
  \citep{rdrobust2,rdrobust3},
  \textit{doParallel}, \textit{foreach} \citep{doparallel,foreach}, and
  \textit{doRNG} \citep{dorng} packages. Our source code is available online
  at \url{https://github.com/grayclhn/boot-rd}.} %
Finally, Section~\ref{conclusion} concludes.

\section{Background}\label{background}

This section provides additional details of RD estimators in general and of
CCT's proposed bias correction. It also defines some of the notation and
presents the assumptions that will be used for our theoretical analysis in
Section~\ref{boot}. We have adopted CCT's notation where possible to aid
readers familiar with that paper.

In the typical sharp RD setting, a researcher wishes to estimate the local
causal effect of treatment at a given threshold. A running variable, $X_{i}$,
determines treatment assignment.  Given a known threshold, which we
will set to zero without loss of generality, the $i$th subject receives the
treatment of interest if $X_{i} \geq 0$ and does not receive treatment if
$X_{i} < 0$.

Subject $i$'s potential outcomes are denoted by the variable $Y_i(\cdot)$;
$Y_i(1)$ is the subject's outcome under treatment and $Y_i(0)$
is the outcome without treatment. Since only one of the two
outcomes is observed, the sample is comprised of the running variable, $X_{i}$,
and the observed outcome $Y_i$, where
\begin{equation*}
  Y_{i}=Y_{i}(0) \1\{X_{i}<0\}+Y_{i}(1) \1\{X_{i} \geq 0\}
\end{equation*}
and $\1\{ \cdot\}$ denotes the indicator function.

In most cases, the population parameter of interest is the Average Treatment
Effect (ATE) at the cutoff, which we will denote $\tau$. This parameter is the
difference in expected potential outcomes given $X_i = 0$; formally,
\begin{equation*}
  \tau=\E(Y(1)-Y(0) \mid X=0)
\end{equation*}
with $\E$ representing the expectation.
\cite{HTV2001} show that the effect $\tau$ is identified under continuity and
smoothness conditions on the joint distribution of $X_i$, $Y_i(0)$, and $Y_i(1)$
around the cutoff $X_i = 0$. Under these conditions, which are made precise
in our Assumption~\ref{A1}, $\tau$ is equal to
\begin{equation*}
  \tau = \lim_{x \to 0+} \mu(x) - \lim_{x \to 0-} \mu(x)
\end{equation*}
where
\begin{equation*}
  \mu(x)= \E(Y_{i} \mid X_{i}=x).
\end{equation*}
For later convenience, also define the derivatives
\begin{equation*}
  \mu^{(\eta)}(x)=\frac{d^{\eta}\mu(x)}{dx^{\eta}}
\end{equation*}
and let
\begin{align*}
  \mu_{+}(x)
  &= \E( Y_{i}(1) \mid X_{i}=x )
  &\mu_{-}(x)
  &= \E( Y_{i}(0) \mid X_{i}=x ) \\
  \sigma^{2}_{+}(x) &= \V( Y_{i}(1) \mid X_{i}=x )
  &\sigma^{2}_{-}(x)&=\V( Y_{i}(0) \mid X_{i}=x ) \\
\intertext{and}
  \mu^{(\eta)}_{+}
  &= \lim_{x \rightarrow 0^{+}}\mu^{(\eta)}(x),
  &\mu^{(\eta)}_{-}
  &= \lim_{x \rightarrow 0^{-}}\mu^{(\eta)}(x),
\end{align*}
where the symbol $\V(\cdot)$ represents the variance. Assumption~\ref{A1} lists
standard conditions in the RD literature which ensure that both $\mu_-$ and
$\mu_+$ can be estimated consistently.

\begin{assumption}[Behavior of the DGP near the cutoff]\label{A1}
  The random variables $\{Y_i, X_i\}_{i=1}^n$ form a random sample of size $n$.
  There exists a positive number $\kappa_0$ such that the following
  conditions hold for all $x$ in the neighborhood $(-\kappa_{0},\ \kappa_{0})$
  around zero:
  \begin{enumerate}
  \item The density of $X_i$ is continuous and bounded away from zero at $x$.
  \item $\E(Y_{i}^{4} \mid X_{i}=x)$ is bounded.
  \item $\mu_+(x)$ and $\mu_-(x)$ are both 3 times continuously differentiable.
  \item $\sigma_+^2(x)$ and $\sigma_-^2(x)$ are both continuous and bounded away
    from zero.
 \end{enumerate}
\end{assumption}

Part 1 and the continuity component of Part 3 of Assumption~\ref{A1} are crucial
for identifying $\tau$ --- informally, they ensure that the number of points
arbitrarily close to the cutoff increases with the sample size and that there is
a small neighborhood around the cutoff in which the change in the conditional
expectation can be attributed to the change in treatment status.  (See
\citealp{HTV2001}, and \citealp{Porter03}, for more detailed discussion of
identification in RD designs.) The additional smoothness conditions in Part 3
allow higher order polynomials to be used for estimation and Parts 2 and 4
ensure that the estimated local polynomials are well behaved.

Since these conditions only need to hold in a neighborhood
around the cutoff, $\mu_+$ and $\mu_-$ can be estimated by extrapolating from a
local polynomial regression.  We will focus here on local linear
regression.\footnote{%
  See \cite{HTV2001}, \cite{Porter03} or \cite{FanGijbels92} for discussions of
  the properties of local polynomial regressions for boundary problems. The
  bootstrap algorithm proposed in this paper can be extended to accommodate
  higher order polynomials discontinuities in the derivatives of the conditional
  expectation, at the cost of stronger moment and smoothness assumptions.} %
For this model, if $h$ represents a bandwidth parameter and $K(\cdot)$ a density
function, the estimator of $\tau$, $\hat\tau(h)$, is defined as
\begin{equation*}
  \hat{\tau}(h) = \hat {\mu}_{+}(h) -\hat{\mu}_{-}(h)
\end{equation*}
with
\begin{align*}
  \hat {\mu}_{+}(h)
  &= \argmin_{\beta_0} \min_{\beta_1} \sum_{i=1}^{n}
  \1\{X_{i} \geq 0\} (Y_{i} - \beta_0 - X_{i} \beta_1)^{2} \tfrac{1}{h}K(X_{i}/h)
\intertext{and}
  \hat {\mu}_{-}(h)
  &= \argmin_{\beta_0} \min_{\beta_1} \sum_{i=1}^{n}
  \1\{X_{i}<0\} (Y_{i} - \beta_0 - X_{i} \beta_1)^{2} \tfrac{1}{h} K(X_{i}/h).
\end{align*}

Conventional (naive) confidence intervals can be calculated by using an
asymptotic approximation for $\hat\tau(h)$. In particular, if
\begin{gather}
  \label{eq:1}
  \frac{\hat{\tau}(h)-\tau}{\sqrt{V(h)}} \to^d N(0,1),
  \intertext{with}
  \notag
  V(h) = \V(\hat\tau(h) \mid X_{1},\dots,X_{n}),
\end{gather}
then valid confidence intervals can be constructed through the usual method of
inverting the $t$-test.\footnote{%
  The mathematical appendix of this paper gives a precise definition for
  $V(h)$.} %
This procedure gives the widely-used interval estimator
\begin{equation*}
  \hat{\tau}(h) \pm q_{1-\alpha/2} V(h)^{1/2}
\end{equation*}
where $q_{1 - \alpha/2}$ is the $1 - \alpha/2$ quantile of the standard normal
distribution.

The statistical properties of these estimators, however, clearly depend on the
bandwidth parameter $h$, and bandwidths that have desirable properties for point
estimation may not have desirable properties for hypothesis testing or interval
estimation. In particular, for~\eqref{eq:1} to hold, $h$ must satisfy
$n h \to \infty$ and $n h^5 \to 0$. (\citealp{HTV2001}; \citealp{Porter03})
Otherwise, the finite-sample bias of $\hat\tau(h)$ does not converge in
probability to zero quickly enough and it contributes to the
asymptotic distribution in~\eqref{eq:1}. This result holds even though
$\hat\tau(h)$ can be consistent under those conditions.  These bandwidth issues
are relevant in practice because many widely-used bandwidth selection
procedures, most notably the AMSE-optimal bandwidth and cross-validation
bandwidth \citep{IK} do not produce $o_{p}(n^{-1/5})$ bandwidths.

CCT solve this problem by deriving the analytical form of the first-order bias
and explicitly recentering $\hat\tau(h)$. Under weaker assumptions on the
asymptotic behavior of the bandwidth, which we will specify in
Assumption~\ref{A2}, CCT show that the approximate bias of $\hat\tau(h)$ has the
form
\begin{equation*}
  \E(\hat{\tau}(h) \mid X_1,\dots,X_n) - \tau =
  h^{2}\Big[ \tfrac{\mu_{+}^{(2)}}{2}\Bf_{+}(h)
  - \tfrac{\mu_{-}^{(2)}}{2}\Bf_{-}(h) \Big]
  (1+o_{p}(1))
\end{equation*}
where $\Bf_{+}(h)$ and $\Bf_{-}(h)$ are observed quantities that depend on the
kernel, bandwidth, and running variables $X_1,\dots,X_n$; formal definitions of
these terms are given in the Mathematical Appendix. The plug-in bias-corrected
estimator then requires estimates
for the second derivatives of the conditional mean from above and below the
cutoff, $\mu_{+}^{(2)}$ and $\mu_{-}^{(2)}$, and CCT show that these derivatives
can be estimated by fitting a second order local polynomial, i.e. one order
higher than the polynomial used to obtain $\hat{\tau}$, using a (potentially)
different pilot bandwidth $b$. Their procedure gives the bias-corrected
estimator
\begin{gather*}
  \hat{\tau}^{bc}(h, b) = \hat{\tau}(h) - h^{2}
  \Big[\tfrac{\hat{\mu}_{+}^{(2)}(b)}{2} \Bf_{+}(h)
  - \tfrac{\hat{\mu}_{-}^{(2)}(b)}{2}\Bf_{-}(h) \Big]
\end{gather*}
The variance introduced by the bias-correction term does not vanish, so the
naive confidence interval needs not only to be re-centered to correct the bias,
but also rescaled to allow for the additional variability introduced by the bias
correction, resulting in the following asymptotic approximation:
\begin{equation*}
  \frac{\hat{\tau}^{bc}(h, b) - \tau}{V^{bc}(h, b)^{1/2}} \to^d N(0,1)
\end{equation*}
with $V^{bc}(h, b) = V(h) + C(h, b)$ and $C(h, b)$ an additional variance component
generated by the bias-correction term.\footnote{%
  $C(h,b)$ and $V(h)$ are defined precisely in the mathematical appendix.} %
This new approximation can be used
instead of~\eqref{eq:1} to construct ``bandwidth robust'' confidence intervals,
and CCT provide simulation evidence that their intervals perform well in finite
samples even when the naive interval performs badly.

Assumption~\ref{A2} specifies the bandwidth and kernel conditions assumed in this
paper, which are standard in this literature.

\begin{assumption}[Bandwidth and kernel]\label{A2}
  Let $h$ be the bandwidth used to estimate the local linear model and let
  $b$ be the bandwidth used to estimate a second local quadratic model. Then
  $n h \to \infty$, $n b \to \infty$, and
  $n \times \min(h, b)^{5} \times \max(h, b)^2 \to 0$ as $n \to \infty$.\footnote{%
    Unless otherwise stated, all limits in this paper are assumed to hold as
    $n \to \infty$.}
  The kernel function $K$ is positive, bounded, and continuous on the interval
  $[-\kappa,~\kappa]$ and zero outside that interval for some $\kappa > 0$.
\end{assumption}
To simplify notation, let $m = \min(h,b)$ and
define the scaled and truncated kernel functions
\begin{align*}
  K_{+,h}(x) &= \tfrac{1}{h} K(x/h) \1\{x \geq 0\} &
  K_{-,h}(x) &= \tfrac{1}{h} K(x/h) \1\{x < 0\}
\intertext{and}
  K_{+,b}(x) &= \tfrac{1}{b} K(x/b) \1\{x \geq 0\} &
  K_{-,b}(x) &= \tfrac{1}{b} K(x/b) \1\{x < 0\}.
\end{align*}
Assumption~\ref{A2} allows for the vast majority kernel functions commonly used
in practice. Note that different kernel functions could be used for the
treatment effect estimator and the higher order approximation used for the
bootstrap.\footnote{%
  It would be a slight abuse of notation for $K_{+,h}$, $K_{-,h}$,
  $K_{+,b}$, and $K_{-,b}$ to all use different underlying kernels but
  would not affect our proofs.} %
For simplicity, however, we use the same function for all steps of
estimation in this paper.  Potentially different kernel functions and bandwidths
could also be used for estimation below and above the cutoff, but we again focus
on the simpler case here.

In the next section, we build upon the insight provided by CCT bias-corrected
estimator and propose a simple bootstrap procedure that can directly construct
the robust CIs without requiring the derivation of analytical formulas and
direct estimators for the bias, variance and covariance terms, while relying on
the same first-order bias correction approximation. Assumption~\ref{A2}
requirements are identical to CCT.

\section{Bootstrap Bias Correction}\label{boot}

This section presents our theoretical contributions. We propose two algorithms
in this section. The first uses a wild bootstrap based on a second
order local polynomial to estimate the bias of the local linear model. That
estimate can be subtracted from the biased original estimator to provide an
asymptotically unbiased estimator with the same asymptotic distribution as
CCT's. As in CCT, this bias correction term introduces a new source of variance,
invalidating standard (naive) critical values. Consequently, the second
algorithm we propose uses an iterated bootstrap to estimate the correct
critical values of the bias corrected estimator.
The intuition behind both procedures is straightforward. CCT show that a
local second order polynomial captures the aspects of the DGP
necessary for constructing valid confidence intervals. Our proposed algorithms
estimate and embed the second order behavior in the bootstrap DGP through a
wild bootstrap.

Throughout this section and the rest of the paper, we will
let $\E^{*}$, $\Pr^{*}$, etc. denote expectations and probabilities taken with
respect to the distribution induced by the bootstrap (which implicitly
conditions on $X_{1},\dots,X_{n}$ and $Y_{1},\dots,Y_{n}$) and let parameters with
$^{*}$ superscripts be the parameter values under the distribution induced by
the bootstrap. Two $^{*}$ superscripts indicate that the parameter or probability
measure corresponds to a secondary bootstrap distribution.

Algorithm~\ref{Alg1} explains the bias-correction steps in detail.

\begin{algorithm}[Bias estimation]\label{Alg1}
  Assume $h$ and $b$ are bandwidths as defined by Assumption~\ref{A2}.
  \begin{enumerate}
  \item Estimate local second order polynomials $\hat g_{-}$ and $\hat g_{+}$
    with least squares using $K_{-,b}$ and $K_{+,b}$ for weights:
    \begin{align}
      \label{eq:2}
      \hat g_{-}(x)
      &= \hat\beta_{-,0} + \hat\beta_{-,1} x + \hat\beta_{-,2} x^{2},
      &\hat g_{+}(x)
      &= \hat\beta_{+,0} + \hat\beta_{+,1} x + \hat\beta_{+,2} x^{2}
    \end{align}
    with
    \begin{align*}
      (\hat\beta_{-,0}, \hat\beta_{-,1}, \hat\beta_{-,2})' &=
      \argmin_{\beta_0, \beta_1, \beta_2}
      \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 X_i - \beta_2 X_i^2)^{2} K_{-,b}(X_{i}) \\
      (\hat\beta_{+,0}, \hat\beta_{+,1}, \hat\beta_{+,2})' &= \argmin_{\beta_0, \beta_1, \beta_2}
      \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 X_i - \beta_2 X_i^2)^{2} K_{+,b}(X_{i}).
    \end{align*}
    Let
    \[
      \hat g(x) = \begin{cases}
        \hat g_-(x) & \textif\ x < 0 \\
        \hat g_+(x) & \otherwise
      \end{cases}
    \]
    and calculate the residuals $\hat\varepsilon_{i} = Y_i - \hat g(X_i)$ for all $i$.
  \item Repeat the following steps $B_{1}$ times to produce the
    bootstrap estimates $\hat{\tau}_{1}^{*}(h),\dots,\hat\tau_{B_{1}}^{*}(h)$. For the
    $k$th replication:
    \begin{enumerate}
    \item Draw i.i.d.\ random variables $e_i^{*}$ with mean zero, variance one,
      and bounded fourth moments independent of the original data and
      construct
      \begin{align*}
        \varepsilon_i^{*} &= \hat{\varepsilon}_i e^{*}_i,
        \intertext{and}
        Y_i^{*} &= \hat g(X_i) + \varepsilon_i^{*}
      \end{align*}
      for all $i$.
    \item Calculate $\hat\mu_+^*(h)$ and $\hat\mu_-^*(h)$ by estimating the
      local linear model on the bootstrap data set using $K_{+,h}$ and $K_{-,h}$ for
      weights:
      \begin{align*}
        \hat\mu_-^*(h)
        &= \argmin_{\mu} \min_{\beta} \sum_{i =1}^n
          (Y_i^* - \mu - \beta X_i)^2 K_{-,h}(X_{i}) \\
        \hat\mu_+^*(h)
        &= \argmin_{\mu} \min_{\beta} \sum_{i = 1}^n
          (Y_i^* - \mu - \beta X_i)^2 K_{+,h}(X_{i}).
      \end{align*}
    \item Save $\hat\tau^*_k(h) = \hat\mu_+^*(h) - \hat\mu_-^*(h)$.
    \end{enumerate}
  \item Estimate the bias as
    \begin{equation}
      \label{eq:9}
      \Delta^*(h,b) = \tfrac{1}{B_1} \sum_{k=1}^{B_1} \hat\tau^*_k(h) -
      \big[\hat g_+(0) - \hat g_-(0)\big].
    \end{equation}
  \end{enumerate}
\end{algorithm}
Note that $\hat g_+(0) - \hat g_-(0)$ is the true treatment effect under the
distribution induced by this bootstrap. The bootstrap estimator works by
constructing an approximate DGP with known properties. As the dataset gets
larger, the approximate DGP mimics the unknown real DGP more closely, and the
population parameter values in the bootstrap DGP can become accurate estimates
of the true parameter values in the real DGP.

Under Assumptions~\ref{A1} and~\ref{A2} the procedure described by
Algorithm~\ref{Alg1} provides a consistent estimator of the bias component that
converges fast enough in probability that it can be be used as a correction. As
is standard in the bootstrap literature, we will assume that the number of
bootstrap replications, $B_{1}$, is large enough that the simulation error can
be ignored. Theorem~\ref{T1} presents the result formally.

\begin{theorem}\label{T1}
  Under Assumptions~\ref{A1} and~\ref{A2},
\begin{equation}
  \label{eq:18}
  \Delta^*(h,b) = h^{2} \Big[\tfrac{\hat{\mu}_{+}^{(2)}(b)}{2} \Bf_{+}(h)
  - \tfrac{\hat{\mu}_{-}^{(2)}(b)}{2}\Bf_{-}(h) \Big]
\end{equation}
almost surely and
\begin{equation}
  \label{eq:4}
  \frac{\hat\tau(h) - \Delta^{*}(h,b) - \tau}{ V^{bc}(h, b)^{1/2}}
  \to^{d} N(0,1),
\end{equation}
where $\Delta^*(h,b)$ is defined by Equation~\eqref{eq:9} and
$V^{bc}(h, b) = V(h) + C(h, b)$, both which are defined in Equations~\eqref{eq:8}
and~\eqref{eq:13} of the Mathematical Appendix.
\end{theorem}

For practical purposes, Theorem~\ref{T1} implies that our bias-corrected
estimator has the same asymptotic distribution as CCT's. This equivalence should
be unsurprising; both estimators use a second order polynomial to directly estimate
the bias of the local linear model, so they should behave very similarly.
Equation~\eqref{eq:18} implies an even stronger relationship between the two
results, that the bias correction is equivalent to CCT's up to simulation error
in the bootstrap.

Since the second order polynomial captures the relevant aspects of the DGP for
estimating the variance as well as the bias, the asymptotic distribution of the
bias corrected estimator $\hat\tau(h) - \Delta^*(h,b)$ can also be approximated
with a bootstrap. We propose bootstrapping $\hat\tau(h) - \Delta^*(h,b)$ using
the same wild bootstrap method used in Algorithm~\ref{Alg1}. Algorithm~\ref{Alg2} provides the details of our procedure
and Theorem~\ref{T2} establishes its theoretical properties.

\begin{algorithm}[Distribution]\label{Alg2}
  Assume $h$ and $b$ are bandwidths as defined by Assumption~\ref{A2} and
  Algorithm~\ref{Alg1}.
  \begin{enumerate}
  \item Estimate $\hat{g}_{+}$ and $\hat{g}_{-}$ and generate $\hat g(\cdot)$
    and the residuals $\hat\varepsilon_i$ just as in Algorithm~\ref{Alg1}.
  \item Repeat the following steps $B_{2}$ times to produce
    bootstrap estimates of the bias-corrected estimate. For the
    $k$th replication:
    \begin{enumerate}
    \item Draw i.i.d.\ random variables $e_i^{*}$ with mean zero, variance one,
      and bounded fourth moments independent of the original data and
      construct
      \begin{align*}
        \varepsilon_i^{*} &= \hat{\varepsilon}_i e^{*}_i,
        \intertext{and}
        Y_i^{*} &= \hat g(X_i) + \varepsilon_i^{*}.
      \end{align*}
      for all $i = 1,\dots,n$.
    \item Calculate $\hat\mu_+^*(h)$ and $\hat\mu_-^*(h)$ by estimating the
      local linear model on the bootstrap data set using $K_{+,h}$ and $K_{-,h}$
      for weights:
      \begin{align*}
        \hat\mu_-^*(h)
        &= \argmin_{\mu} \min_{\beta} \sum_{i = 1}^n
          (Y_{i}^{*} - \mu - \beta X_{i})^{2}K_{-,h}(X_{i}), \\
        \hat\mu_+^*(h)
        &= \argmin_{\mu} \min_{\beta} \sum_{i = 1}^n
          (Y_{i}^{*} - \mu - \beta X_{i})^{2}K_{+,h}(X_{i}).
      \end{align*}
    \item Apply Algorithm~\ref{Alg1} to the bootstrapped data set
      $(Y_1^*,\, X_1),\dots,(Y_n^*,\, X_n)$ set using the same bandwidths
      $h$ and $b$ that are used in the rest of this
      algorithm but reestimating all of the local polynomials on the bootstrap
      data. Generate $B_1$ new bootstrap samples and let $\Delta^{**}(h,b)$
      represent the bias estimator returned by Algorithm~\ref{Alg1}.
    \item Save the estimator of $\tau$ and its bias,
      $\hat\tau_k^{*}(h) = \hat\mu_+^*(h) - \hat\mu_-^*(h)$ and $\Delta_k^{**}(h,b)$.
    \end{enumerate}
  \item Define $\tau^* = \hat g_+(0) - \hat g_-(0)$ and use the empirical CDF of
    $\hat\tau_1^{*}(h) - \Delta_1^{**}(h,b)- \tau^*,\dots, \hat\tau_{B_2}^{*}(h)  - \Delta_{B_2}^{**}(h,b) - \tau^*$
    as the sampling distribution of $\hat \tau(h) - \Delta^*(h,b) - \tau$.
  \end{enumerate}
\end{algorithm}

Theorem~\ref{T2} establishes that this iterated bootstrap approximates the
asymptotic distribution of the bias-corrected statistic proposed by
Algorithm~\ref{Alg1} and justifies this second algorithm.  As before, we assume
that $B_1$ and $B_2$ are large enough that simulation error can be ignored.

\begin{theorem}\label{T2}
  Under Assumptions~\ref{A1} and~\ref{A2},
  \begin{equation*}
    \V^*(\hat\tau^{*}(h) - \Delta^{**}(h,b))/V^{bc}(h,b) \to^p 1
  \end{equation*}
  and
  \begin{equation*}
    \sup_{x}
    \Bigg\rvert \Pr^*\Bigg[
    \frac{\hat\tau^{*}(h) - \Delta^{**}(h,b) - \tau^*}{\V^*(\hat\tau^{*}(h) - \Delta^{**}(h,b))^{1/2}}
    \leq x \Bigg]
    - \Pr\Bigg[\frac{\hat\tau(h) - \Delta^*(h,b) - \tau}{V^{bc}(h,b)^{1/2}}
    \leq x \Bigg] \Bigg\lvert \to^p 0.
  \end{equation*}
\end{theorem}

Two brief remarks on implementing these algorithms. First, many of the matrix operations
required to estimate the local polynomials only involve the regressors and,
therefore, only need to be computed once. For example, the parameter estimates
for the linear model on the treated group are equal to
\[
  (\mathbf{X}' \mathbf{K}_+ \mathbf{X})^{-1} \mathbf{X}' \mathbf{K}_+ \mathbf{Y}
  \equiv \mathbf{W}_{+,1}\mathbf{Y}
\]
where
\begin{align*}
\mathbf{X} &= \begin{pmatrix*} 1 & X_1 \\ 1 & X_2 \\ \vdots & \vdots \\ 1 & X_n \end{pmatrix*} &
\mathbf{Y} &= \begin{pmatrix*} Y_1 \\ Y_2 \\ \vdots \\ Y_n \end{pmatrix*} &
\mathbf{K}_+ &= \begin{pmatrix*}
  K_{+,h}(X_1) & 0 & \cdots & 0 \\
  0 & K_{+,h}(X_2) & \cdots & 0 \\
  0 & 0 & \ddots & 0 \\
  0 & 0 & \cdots & K_{+,h}(X_n) \end{pmatrix*}.
\end{align*}
The matrix $\mathbf{W}_{+,1}$ is not affected by the bootstrap so it can be
computed once initially and reused in the bootstrap calculations. Then each
bootstrap replication just requires a single matrix-vector multiplication. The
other parameter estimates have this property as well. Consequently, the
computational burden of this iterated bootstrap is much lower than might
appear.\footnote{%
  An open-source implementation of this algorithm written in R, which was used
  for the empirical and Monte Carlo components of this paper, is available
  online at \url{https://github.com/grayclhn/boot-rd}.} %

Second, if the error terms have pronounced heteroskedasticity, it can be
advantageous to transform the residuals before applying the bootstrap, as
discussed by \citet{DF08} --- these transformations typically do not affect the
first-order asymptotic properties of the bootstrap distribution but can provide
improvements in finite samples analogous to alternative
heteroskedasticity-robust variance estimators. (\citealp{mackinnon1985some}, and
\citealp{mackinnon2013thirty}.) Since RD estimators rely on a small subset of
the data available in any application, these finite-sample adjustments can be
especially effective in improving performance, and we use the ``HC3''
transformation proposed by \citet{DF08} in this paper's Monte Carlo simulations
and empirical exercise; i.e.\ Algorithms~\ref{Alg1} and~\ref{Alg2} use the
transformed residuals
\begin{equation}\label{eq:14}
  \frac{\hat\varepsilon_i}{1 - H_{ii}}
\end{equation}
instead of $\hat\varepsilon_i$, with $H_{ii}$ the $(i, i)$ element of the matrix
\begin{equation}\label{eq:15}
  \mathbf{H} = \mathbf{K}_+^{1/2} \mathbf{X} (\mathbf{X}' \mathbf{K}_+ \mathbf{X})^{-1} \mathbf{X}' \mathbf{K}_+^{1/2}.
\end{equation}
The matrix $\mathbf{H}$ is the projection matrix onto the kernel-weighted design
matrix.

Evidence of the usefulness of the procedures proposed above and their relative performance to the analytical bias correction proposed in CCT are presented in a series of Monte Carlo simulations in Section \ref{sim}.

\section{Simulation Evidence}\label{sim}
This section presents evidence from Monte Carlo simulations that the bootstrap
procedures proposed in Section~\ref{boot} produce valid, robust confidence
intervals (CIs) similar to those obtained by the analytical procedures established in CCT. 
 We also observe that all of the bias corrected methods can offer substantial improvements over estimators that do not bias correct.

The Monte Carlo experiments have a similar structure. For all of them, we generate 500 i.i.d.\ observations from the DGP
\begin{align*}
Y_{i}           &= \mu_{j}(X_{i}) + \varepsilon_{i} \\
X_{i}           &\sim  2 \times \betarv(2,4) - 1 \\
\varepsilon_{i} &\sim N(0, 0.1295^2 \times \sigma_i^2) \\
\sigma_i &= z(\gamma) \mu_{j}(X_{i})^\gamma ,
\end{align*}
where $j$ indexes the specific DGP and $z(\gamma)$ is a scale parameter that
ensures that the average variance in a neighborhood around the cutoff
equals the unconditional value under homoskedasticity; i.e. $z(\gamma)$
satisfies
\[
  z(\gamma)^2
    = \frac{    {\#\{X_i\ :\ \min(-\bar h, -\bar b) \leq X_i \leq \max(\bar h, \bar b)\}}}
  {\sum_{i = 1}^n \mu_{j}(X_{i})^{2 \gamma}
  \1\{\min(-\bar h, -\bar b) \leq X_i \leq \max(\bar h,\bar b)\}}.
\]
For $\gamma = 0$ we have $z(\gamma) = 1$ and the errors are
homoskedastic.\footnote{%
  The values $\bar h$ and $\bar b$ are taken as the average values over 500
  homoskedastic simulations of the bandwidths returned by CCT's optimal
  bandwidth selector. This ensures that the relevant variance around the cutoff
  is approximately the same for the heteroskedastic and homoskedastic DGPs.} %
This is the experimental design used by \citet{IK} and CCT for homoskedastic
simulations, which we adopt here to make our simulation results directly
comparable with theirs and with the rest of the literature.  The form of
heteroskedasticity is motivated by \cite{mackinnon2013thirty}, and we consider
$\gamma$ equal to 0 and 2 for the simulations.
We use the same three functional forms for $\mu_{j}$ as CCT, which we describe below.

\nocite{lee2008rand}%
The first DGP is designed to match features of Lee's (2008) analysis of U.S.\
congressional elections. Lee estimates the incumbency advantage in electoral 
races for the House of Representatives --- parties that received the largest 
vote share in the previous election are the incumbents, which creates the 
discontinuity. The conditional expectation is a fifth order polynomial fit to 
that dataset, (after excluding a small number of extreme observations; see 
\citealp{IK}, or CCT for further details) giving
\begin{equation*}
  \mu_{1}(x) =
  \begin{cases}
    0.48 + 1.27x + 7.18x^{2} + 20.21x^{3} + 21.54x^{4} + 7.33x^{5}
    & \textif\ x < 0 \\
    0.52 + 0.84x - 3.00x^{2} + 7.99x^3 - 9.01x^4 + 3.56x^{5}
    & \otherwise.
  \end{cases}
\end{equation*}
The population ATE for this DGP is $0.04$ $(= 0.52 - 0.48)$.

\nocite{ludwig2007}%
The second DGP is based on Ludwig and Miller's (2007) analysis of the Head Start program.
As described in Section \ref{application}, eligibility to grant-writing assistance is determined
 at the county level using the county's historical poverty rate, with a sharp threshold that
 determines the provision of services. We use the fifth order polynomial estimated on Ludwig
 and Miller's dataset as the conditional expectation for the second DGP:
\begin{equation*}
  \mu_{2}(x) =
  \begin{cases}
    3.71 + 2.30x + 3.28x^2 + 1.45x^3 + 0.23x^4 + 0.03x^5
    & \textif\ x < 0, \\
    0.26 + 18.49x - 54.81x^2 + 74.30x^3 - 45.02x^4 + 9.83x^5
    & \otherwise
  \end{cases}
\end{equation*}
and the population ATE is $-3.45$ $(= 0.26 - 3.71)$.

Finally, for the third DGP, we use CCT's modification of $\mu_1$, given by 
\begin{equation*}
  \mu_{3}(x) =
  \begin{cases}
    0.48 + 1.27x + 3.59 x^{2} + 14.147 x^3 + 23.694 x^4 + 10.995 x^5
    & \textif\ x < 0 \\
    0.52 + 0.84x - 0.30 x^{2} + 2.397 x^3 - 0.901 x^4 + 3.56 x^5
    & \otherwise
\end{cases}
\end{equation*}
and the population ATE is again $0.04$. CCT introduce this DGP because it has
high curvature and local linear models are likely to exhibit high bias, making
it a natural test case for both their analytical corrections and our bootstrap.

To estimate the finite-sample coverage of our new bootstrap based confidence interval, we
simulate 5000 samples from each of the three DGPs (with $\gamma = 0$ and $\gamma = 2$) and calculate nominal 95\%
two-sided confidence intervals. We use 999 bootstrap replications ($B_2$) to
calculate the asymptotic distribution of the bias corrected estimator, and each
of those replications uses an additional 500 replications ($B_1$) to estimate
the bias. The bandwidths, $h$ and $b$,
are chosen using the AMSE-optimal rule proposed by CCT and the wild bootstrap
uses the ``HC3'' transformation of \citet{DF08}, defined in Equations~\eqref{eq:14}
and~\eqref{eq:15}. In these simulations, we use Mammen's (1993) \nocite{Mam93}
two-point distribution for $e_i^*$ in the bootstrap:
\begin{equation}
  \label{eq:19}
  e_i^* =
  \begin{cases}
    \frac{1 + \sqrt{5}}{2} & \text{with probability\ } \frac{\sqrt{5} - 1}{2 \sqrt{5}} \\
    \frac{1 - \sqrt{5}}{2} & \text{otherwise}.
  \end{cases}
\end{equation}

We include two other procedures for comparison: CCT's bias-corrected interval and a ``naive'' uncorrected estimator.
 We estimate CCT's bias corrected procedure with AMSE-optimal bandwidth and estimate
the ``naive'' uncorrected interval with the optimal bandwidth $h$ proposed by
\cite{IK}. We use triangular kernel throughout all the three procedures to facilitate direct comparisons.\footnote{%
Simulation results for other kernels and variance estimators produced similar outcomes.} %

\begin{table}[t]
  \centering
  \begin{tabular}{rlrrrrrrr}
    \toprule
    DGP & Method       & Bias    & SD    & RMSE  & CI Coverage (\%) & CI Length \\
    \midrule                                                     \\
    \multicolumn{7}{l}{Homoskedastic data with $\gamma = 0$ (Panel A)}  \\
    \midrule
    1 & Wild bootstrap & 0.011 & 0.067 & 0.068 & 0.926 & 0.247 \\
      & CCT            & 0.011 & 0.067 & 0.068 & 0.914 & 0.239 \\
      & Naive          & 0.037 & 0.041 & 0.055 & 0.823 & 0.152 \\
    2 & Wild bootstrap & 0.008 & 0.086 & 0.086 & 0.938 & 0.334 \\
      & CCT            & 0.008 & 0.086 & 0.086 & 0.932 & 0.346 \\
      & Naive          & 0.145 & 0.065 & 0.159 & 0.311 & 0.224 \\
    3 & Wild bootstrap & 0.005 & 0.065 & 0.065 & 0.945 & 0.252 \\
      & CCT            & 0.005 & 0.065 & 0.065 & 0.934 & 0.244 \\
      & Naive          & --0.034 & 0.051 & 0.061 & 0.851 & 0.185 \\  \\
    \multicolumn{7}{l}{Heteroskedastic data with $\gamma = 2$ (Panel B)}  \\
    \midrule
    1 & Wild bootstrap & 0.010 & 0.073 & 0.074 & 0.935 & 0.276 \\
      & CCT            & 0.010 & 0.073 & 0.074 & 0.923 & 0.265 \\
      & Naive          & 0.035 & 0.046 & 0.058 & 0.838 & 0.165 \\
    2 & Wild bootstrap & 0.007 & 0.084 & 0.084 & 0.922 & 0.312 \\
      & CCT            & 0.007 & 0.084 & 0.084 & 0.929 & 0.340 \\
      & Naive          & 0.144 & 0.064 & 0.158 & 0.277 & 0.211 \\
    3 & Wild bootstrap & 0.002 & 0.081 & 0.081 & 0.940 & 0.311 \\
      & CCT            & 0.002 & 0.081 & 0.081 & 0.926 & 0.297 \\
      & Naive          & --0.033 & 0.059 & 0.068 & 0.835 & 0.194 \\
    \bottomrule
  \end{tabular}
  \caption{%
    Experimental coverage probabilities for each interval estimator based on 5000
    simulations; nominal coverage probabilities are 95\% for each estimator. The column
    ``CI Coverage'' lists the coverage frequency in these simulations and ``CI Length''
    lists the average length of the confidence interval across simulations.}
  \label{tbl:1}
\end{table}

Table~\ref{tbl:1} presents the results of these simulations. It consists of two panels, with results for homoskedastic data in Panel A and heteroskedastic data in Panel B. The row labeled
``Wild bootstrap'' shows results for our proposed bootstrap based interval; ``CCT'' shows
results for the analytically-corrected interval and ``Naive'' shows results for
the analytically-uncorrected interval. The first three columns give the bias,
standard deviation, and Root-MSE of the point
estimators corresponding to each approach.  As expected, both
bias corrected estimators have
bias close to zero, with their standard deviation largely
determining the estimators' RMSE.
As we remark earlier, the bias correction from the wild bootstrap is equivalent
to CCT's bias correction up to simulation error from the bootstrap and this
equivalence is apparent in the Monte Carlo. This equivalence also explains why
the bootstrap and analytically-corrected estimators have essentially the same
standard deviation across all of the DGPs.\footnote{%
  Note that this table reports the estimators' variance across simulations,
  not the \emph{estimated} variance used for the confidence intervals.} %

The column ``CI Coverage'' lists the experimental coverage of each procedure.  In 
Panel A, all the intervals produced by the two robust estimators are reasonably close to 
their nominal coverage, although they slightly under-cover the true treatment effect.
Our proposed bootstrap procedure consistently performs well and it is about a percentage point 
closer to nominal coverage than the analytically-corrected interval. The uncorrected interval performs substantially worse, with coverage as low as 31\% (in the second DGP) and never
higher than 86\%. 

Panel B presents results for the heteroskedastic case. The coverage obtained is similar to that in Panel A and highlight
the robustness of the wild bootstrap procedure proposed to heteroskedasticity. The two robust procedures again perform
similarly and have coverage close to the nominal level. The bootstrap explicitly
accounts for heterosedasticity in the DGP; CCT's procedure implicitly accommodates
smooth heteroskedasticity by estimating the variance in a neighborhood around
the cutoff.

The final column, ``CI Length,'' indicates that intervals from the
two robust procedures have average lengths close to each other in all of the DGPs,
while interval from the naive procedure is much shorter on average.

These are DGPs where the naive confidence intervals are known to perform poorly
and this set of simulation results indicates that our proposed bootstrap approach is 
competitive with analytical methods for producing robust intervals.
Overall, the bootstrap bias-correction procedure proposed in this paper provides a simple alternative to obtain valid robust confidence intervals in RD designs
and performs similarly well compared to the analytical bias correction procedures proposed by CCT.


\section{Application}\label{application}

In this section, we apply the bootstrap procedure to the data used in
\cite{ludwig2007}.\footnote{%
  The data is publicly available from
  \url{http://faculty.econ.ucdavis.edu/faculty/dlmiller/statafiles}.} %
In their paper, the effects of Head Start application assistance on health and
schooling were investigated under a sharp RD design.

In 1965, the Head Start program was established to help poor children aged three
to five and their families. The program elements include parent involvement,
nutrition, social services, mental health services and health services. To
promote this program in the most needing area, the Office of Economic
Opportunity provided grant-writing assistance to the poorest 300 counties in the
United States based on the 1960 poverty rate. So the poverty rate of the 300th
poorest county serves as a sharp cutoff of treatment. It is shown in
\cite{ludwig2007} that the 228 ``treatment'' counties with poverty rates 10
percentage points above this cutoff have average Head Start spending per
four-year-old as twice of that for 349 ``control'' counties with poverty rates
10 percentage points below this cutoff.

\cite{ludwig2007} utilize this fact to estimate the ``intent-to-treat'' effect:
the effect of proposal developing assistance on health and schooling. They use
mortality as their health outcome measure and use data from the National Vital
Statistics System of the National Center for Health Statistics, which provide
information on cause of death and age at death. \cite{ludwig2007} limited the
causes of death to those which could be affected by Head Start health services
and found a large drop in mortality rates of children five to nine years of age
over the period of 1973--1983. They also found some evidence for a positive
effect on schooling from decennial census data.

We reestimate the ATE on health and schooling with robust procedures. To be
specific, we apply both the robust procedure proposed by CCT and the bootstrap
procedure introduced in this paper.  Our bootstrap estimator uses the
AMSE-optimal bandwidths proposed by CCT, the uniform kernel, and the HC3 wild
bootstrap from \citet{DF08} with Mammen's (1993) two-point distribution
on $e^*_i$; see Equations~\eqref{eq:14} and~\eqref{eq:15} of
this paper and their surrounding remarks as well as Equation~(\ref{eq:19}) for
formal definitions. In the bootstrap
procedure, we use 500 bootstraps for bias correction and 999 to calculate the
confidence intervals. The analytical estimator using CCT's bias correction and
variance estimator uses
their AMSE-optimal bandwidths as well.

\begin{table}[t]
  \centering
  \begin{tabular}{lrr@{, }rr@{}rrr}
    \toprule
                      & ATE     & \multicolumn{2}{r}{95\% CI}            &     &  $h$ &   $b$ & p-value \\
    \midrule
    LM (2007)         & --1.895 & (--3.930                    & 0.139)   & 9   &      &       & 0.036 \\
    LM (2007)         & --1.198 & (--2.561                    & 0.165)   & 18  &      &       & 0.081 \\
    LM (2007)         & --1.114 & (--2.138                    & --0.090) & 36  &      &       & 0.027 \\
    CCT               & --3.795 & (--7.040                    & --0.551) & 3   & .888 & 6.807 &       \\ 
    Wild bootstrap    & --3.800 & (--6.781                    & --0.408) & 3   & .888 & 6.807 &       \\
    \bottomrule
  \end{tabular}
  \caption{The effect of Head Start assistance on mortality. The first three
    rows come from Table 3 in \cite{ludwig2007} except ``95\% CI,'' which is
    calculated using the conventional asymptotic interval estimator.
    The last two rows list results from two robust procedures.}
  \label{tbl:2}
\end{table}

For completeness, we also report the
original results in \cite{ludwig2007}.\footnote{%
  One minor issue arose in reproducing Ludwig and Miller's results: their paper
  presents results for the \emph{triangular} kernel, but we were only able to
  recover their ATE estimate using the \emph{uniform} kernel. Results for the
  triangular and uniform kernel were essentially the same, and this discrepancy
  does not affect Ludwig and Miller's conclusions.} %
Since our research focuses on confidence intervals, we calculate and report
the conventional unadjusted RD confidence interval for each of the bandwidths
used by \cite{ludwig2007}. (Ludwig and Miller use a range of bandwidths
for their analysis.) They also use a paired bootstrap algorithm to generate
p-values, and we report those p-values for completeness.

Table \ref{tbl:2} shows the results of the Head Start program on mortality of
children five to nine years of age.\footnote{%
  We, like Ludwig and Miller, focus on the 1973--1983 period.} %
Instead of choosing an optimal bandwidth, \cite{ludwig2007} adopted three
candidate bandwidths 9, 18, and 36. Their estimates indicate that Head Start
assistance lowers the targeted mortality rate by --1.895, --1.198 and --1.114
respectively, which are not very sensitive to the choice of bandwidths in this
range. These ATEs are also significantly different from zero (with p-value
0.036, 0.081 and 0.027) based on Ludwig and Miller's percentile-$t$ bootstrapped
p-value. The statistical inference changes when conventional analytical
confidence interval is used, which includes zero for bandwidth 9 and 18.

Results from the two robust procedures are similar to each other but greatly
differ from the original estimates. Both ATE estimates are significantly different from
zero and indicate a reduction in mortality, although the confidence intervals are
also very wide, which is likely to be
due to the much smaller bandwidths used relative to the other estimators
($h=3.888$, $b=6.807$). The ATE estimate reported are the bias-corrected
point estimates, and their value indicates that the naive estimates are
subject to substantial bias.

Table \ref{tbl:3} presents the effect of the program on schooling for the cohort
aged 18--24 in 1990. The measurement of schooling is the fraction of people with
high school or more in Panel A and the fraction of people with some college or
more in Panel B. A bandwidth of 7 is used in \cite{ludwig2007}. Their estimates
suggest that Head Start assistance increases the fraction of people with high
school or more by 3\% and the fraction of people with some college or more by
3.7\%. Both are significantly different from zero based on both their p-value
and the conventional analytical confidence interval.

The two robust procedures are again very similar to each other and give
statistically significant, positive, estimates. The confidence intervals tend to shift
relative to the original intervals, increasing both the range of effects consistent
with the data as well as the lower bound on plausible effects.
In contrast to the striking differences between the conventional
and bias-corrected procedures for the mortality estimates, the differences
in Table~\ref{tbl:3} are much smaller.

\begin{table}[t]
  \centering
  \begin{tabular}{lrr@{, }rr@{}rrr}
    \toprule
                   & ATE   & \multicolumn{2}{r}{95\% CI} & & $h$     & $b$ & p-value     \\
    \midrule                                                                          \\
    \multicolumn{5}{l}{Fraction ``high school or more'' (Panel A)}                    \\
    \midrule
    LM (2007)      & 0.030 & (0.003                      & 0.057) & 7&     &  & 0.032    \\
    CCT            & 0.055 & (0.014                      & 0.096) & 3&.671 & 8.618       \\
    Wild bootstrap & 0.055 & (0.010                      & 0.097) & 3&.671 & 8.618       \\\\
    \multicolumn{5}{l}{Fraction ``some college or more'' (Panel B)}                   \\
    \midrule
    LM (2007)      & 0.037 & (0.002                      & 0.073) & 7&     &  & 0.032 \T \\
    CCT            & 0.051 & (0.004                      & 0.099) & 5&.076 & 10.251      \\
    Wild bootstrap & 0.052 & (0.006                      & 0.092) & 5&.076 & 10.251      \\
    \bottomrule
  \end{tabular}
  \caption{The effect of Head Start assistance on education for cohort
    18--24 in 1990. Panel A uses the fraction of people with high school
    or more as dependent variable. Panel B uses the fraction of people
    with some college or more as dependent variable. The first row in
    each panel comes from Table 4 in \cite{ludwig2007} except ``95\% CI,''
    which is calculated using the conventional asymptotic interval estimator.
    The last two rows in
    each panel list results from two robust procedures.}
  \label{tbl:3}
\end{table}

To briefly summarize, the bootstrap procedure performs similarly to the robust
estimator proposed by CCT in the above applications. Both provide somewhat
dissimilar answers from the classical point and interval estimators that do
not account for the estimator's bias, but our estimates largely support the
direction and the statistical significance of Ludwig and Miller's (2007)
empirical findings.

\section{Conclusion}\label{conclusion}

This paper proposes a novel bootstrap procedure to obtain robust bias-corrected
confidence intervals in sharp regression discontinuity designs. The approach proposed builds upon the developments and intuition
advanced by CCT and is based on a first-order bias correction. We exploit CCT's
theoretical insight through a new wild bootstrap. In particular, we propose
estimating the local linear model as usual, then estimating a local second order
polynomial and generating bootstrap datasets based on that
polynomial. This bootstrap allows the bias of the linear model to be estimated
and removed, and the bootstrap can be repeated to accurately estimate the
sampling distribution of the bias-corrected estimator. These results demonstrate
that there are bootstrap procedures that can generate asymptotically valid
confidence intervals in the same settings as CCT's.

\appendix
\section{Mathematical appendix}
Let $e_p$ be the selection vector with 1 in element $p+1$ and 0
everywhere else and assume, with some abuse of notation, that the
dimension of $e_p$ adapts to make matrix and vector operations
conformable. Much of the theory in this appendix applies to both
sides of the cutoff symmetrically, so we will use a ``$\bullet$'' as a
placeholder for either $+$ or $-$ in equations; i.e.\ the equation
\[
  \hat\mu_\bullet(h) = \argmin_{\mu} \min_{\beta} \sum_{i=1}^n
  (Y_i - \mu - \beta X_i)^2 K_{\bullet,h}(X_i)
\]
would define both $\hat\mu_+(h)$ and $\hat\mu_-(h)$ as
\begin{align*}
  \hat\mu_+(h) &= \argmin_{\mu} \min_{\beta} \sum_{i=1}^n
  (Y_i - \mu - \beta X_i)^2 K_{+,h}(X_i) \\
  \hat\mu_-(h) &= \argmin_{\mu} \min_{\beta} \sum_{i=1}^n
  (Y_i - \mu - \beta X_i)^2 K_{-,h}(X_i).
\end{align*}
Define the following additional notation:\footnote{%
  As we mention in Section~\ref{background}, we have adopted CCT's
  notation where possible and these terms originate in that paper.} %
$r_p(x) = (1,\ x,\dots,\ x^p)'$, $\1_+(x) = \1\{x \geq 0\}$, $\1_-(x) = \1\{x < 0\}$,
\begin{align*}
  \Gamma_{\bullet,p}(h)
  &= \tfrac{1}{n} \sum_{i=1}^n r_p(X_i/h) r_p(X_i/h)' K_{\bullet,h}(X_i) \\
  \Bf_\bullet(h)
  & = e_0' \big(\Gamma_{\bullet,1}(h)\big)^{-1}
    \tfrac{1}{n} \sum_{i=1}^n (X_i / h)^2 r_1(X_i/h) K_{\bullet,h}(X_i) \\
  \Psi_{\bullet,p,q}(h,b)
  &= \tfrac{1}{n} \sum_{i=1}^n r_p(X_i/h) r_q(X_i/b)'
     K_{\bullet,h}(X_i) K_{\bullet,b}(X_i) \V(Y_i \mid X_i)
\end{align*}
as well as their population counterparts,
\begin{align*}
  \Gammat_{\bullet,p}(h) &= \int_{-\infty}^\infty   r_p(u) r_p(u)' K(u) f(h u) \1_\bullet(u) du \\
  \Gamma_{\bullet,p} &= \int_{-\infty}^\infty   r_p(u) r_p(u)' K(u) \1_\bullet(u) du \\
  \Bft_\bullet(h) & = e_0' \big(\Gammat_{\bullet,1}(h)\big)^{-1}
    \int_{-\infty}^\infty r_1(u) u^2 K(u) f(h u) \1_\bullet(u) du \\
  \Bf_\bullet & = e_0' \Gamma_{\bullet,1}^{-1} \int_{-\infty}^\infty r_1(u) u^2 K(u) \1_\bullet(u) du \\
  \Psit_{\bullet,p,q}(h,b) &= \int_{-\infty}^\infty
     r_p(\tfrac{m}{h} u) r_q(\tfrac{m}{b} u)' K(\tfrac{m}{b} u) K(\tfrac{m}{h} u)
     \V(Y_i \mid X_i = um) f(um) \1_\bullet(u) du \\
  \Psi_{\bullet,p} &= \int_{-\infty}^\infty
     r_p(u) r_p(u)' K(u)^2 \1_\bullet(u) du.
\end{align*}
Remember that $f$ is the pdf of the running variable. Lemmas
SA1 and SA2 of CCT imply that all of these terms have well-defined limits under
Assumptions~\ref{A1} and~\ref{A2} (for some, after normalization), namely that
the following relationships hold:
\begin{align}
  \Gamma_{\bullet,p}(h) & = \Gammat_{\bullet,p}(h) + o_p(1) &
  \Gammat_{\bullet,p}(h) &= f(0) \, \Gamma_{\bullet,p} + o(1) \label{eq:10} \\
  \Bf_{\bullet}(h) &= \Bft_{\bullet}(h) + o_p(1) &
  \Bft_{\bullet}(h) &= \Bf_{\bullet} + o_p(1) \label{eq:11} \\
  \tfrac{hb}{m} \Psi_{\bullet,p,q}(h, b) &= \Psit_{\bullet,p,q}(h,b) + o_p(1) &
  \Psit_{\bullet,p,p}(h,h) &= f(0) \, \sigma^2_{\bullet}(0) \, \Psi_{\bullet,p} + o(1). \label{eq:12}
\end{align}
Implicitly the right column of equations holds as $h \to 0$.

Also let $\hat\beta_{\bullet,p}(h)$ be the coefficient
estimators from the weighted regression of $Y_i$ on $r_p(X_i)$. As in CCT, this
implies that
\begin{align*}
  \hat\beta_{\bullet,p}(h) &= H_p(h) \Gamma_{\bullet,p}(h)^{-1}
  \tfrac{1}{n} \sum_{i=1}^n r_p(X_i/h) Y_i K_{\bullet,h}(X_i)
\end{align*}
with $H_p(h) = \diag(1, h^{-1}, \dots, h^{-p})$.
These coefficients are related to the quantities of interest by
\begin{equation*}
  \hat{\mu}_{\bullet,p}^{(\nu)}(h) = \nu! e_\nu' \hat\beta_{\bullet,p}(h).
\end{equation*}
for $\nu = 0,\dots,p$ and similar arguments to those used above imply that
\begin{equation*}
  H_p(h)^{-1}(\hat\beta_{\bullet,p}(h) - \beta_{\bullet,p}(h))= O_p(1/\sqrt{nh}).
\end{equation*}

Finally, define the variance components
\begin{equation}\label{eq:8}
  V(h)
  = \tfrac{1}{n} e_0' \big(\Gamma_{-,1}(h)^{-1} \Psi_{-,1,1}(h,h)
    \Gamma_{-,1}(h)^{-1}
  + \Gamma_{+,1}(h)^{-1} \Psi_{+,1,1}(h,h) \Gamma_{+,1}(h)^{-1} \big) e_0
\end{equation}
and
\begin{multline}\label{eq:13}
  C(h,b) =
  \tfrac{h^4}{n b^4} \, e_2' \big[
    \Gamma_{+,2}(b)^{-1} \Psi_{+,2,2}(b,b) \Gamma_{+,2}(b)^{-1} \Bf_+(h)^2 +
    \Gamma_{-,2}(b)^{-1} \Psi_{-,2,2}(b,b) \Gamma_{-,2}(b)^{-1} \Bf_-(h)^2
  \big] e_2 \\
  - \tfrac{2 h^2}{n b^{2}}
    e_0' \Big[
    \Gamma_{+,1}(h)^{-1} \Psi_{+,1,2}(h,b) \Gamma_{+,2}(b)^{-1} \Bf_+(h) +
    \Gamma_{-,1}(h)^{-1} \Psi_{-,1,2}(h,b) \Gamma_{-,2}(b)^{-1} \Bf_-(h)
    \Big] e_2.
\end{multline}
See CCT for more on the motivation and derivation of these formulas.

\subsection{Proof of Theorem~\ref{T1}}
We have
\begin{equation*}
  \hat\tau(h) - \Delta^*(h,b) - \tau = (\hat\tau(h) - \E \hat \tau(h)) +
  (\E \hat \tau(h) - \tau) - (\E^* \hat \tau^*(h) - \tau^*).
\end{equation*}
By construction,
\begin{equation*}
  Y_i^* =
  \begin{cases}
    r_2(X_i/b)'H_2(b)^{-1} \beta_{+,2}^* + \varepsilon_i^* & X_i \geq 0 \\
    r_2(X_i/b)'H_2(b)^{-1} \beta_{-,2}^* + \varepsilon_i^* & X_i < 0
  \end{cases}
\end{equation*}
which, along with the definitions at the beginning of the appendix,
ensures that
\begin{align*}
  \E^* \hat\mu_{\bullet,1}^*(h) - \mu_{\bullet}^{*}
  = h^{2} \mu_{\bullet}^{*(2)} \Bf_{\bullet}(h)/2
\end{align*}
almost surely and, consequently,
\begin{equation*}
  \E^* \hat\tau^*(h) - \tau^{*} = h^2\, \mu_+^{*(2)} \Bf_+(h)/2
  - h^2\, \mu_{-}^{*(2)} \Bf_{-}(h)/2.
\end{equation*}
Equation~\eqref{eq:18} holds because
$\mu_\bullet^{*(2)} = \hat\mu_{\bullet,2}^{(2)}(b)$ almost surely.

CCT's Lemma A1 implies that
\begin{equation*}
  \E \hat\tau(h) - \tau = h^2\, \mu_+^{(2)} \Bf_+(h)/2
  - h^2\, \mu_{-}^{(2)} \Bf_{-}(h)/2 + O_p(h^3)
\end{equation*}
as well, giving
\begin{align}
  \hat\tau(h) - & \E \hat\tau(h) + (\E \hat\tau(h) - \tau) - (\E^* \hat\tau^*(h) - \tau^*)\notag \\
  &= \hat\tau(h) - \E\hat\tau(h)
   + h^2 \big((\mu_{-}^{*(2)} - \mu_{-}^{(2)}) \Bf_{-}(h) /2
   - (\mu_+^{*(2)}-\mu_+^{(2)}) \Bf_{+}(h) / 2 \big) + O_p(h^{3}) \notag \\
  \label{eq:6}
  &= \hat\tau(h) - \E\hat\tau(h)
   + h^2 (\hat\mu_{-,2}^{(2)}(b) - \mu_-^{(2)}) \Bf_{-}(h) /2 \\
  \notag
  &\pushright{- h^2 (\hat\mu_{+,2}^{(2)}(b) - \mu_+^{(2)}) \Bf_{+}(h) / 2
   + O_p(h^{3})}
\end{align}
Asymptotic
normality then follows from normality of $\hat\tau(h) - \E\hat\tau(h)$ and
$\hat\mu_{\bullet,2}^{(2)}(b) - \mu_\bullet^{(2)}$ using similar arguments to
CCT's Lemma SA4.D.\qed

\subsection{Proof of Theorem~\ref{T2}}
Repeat the steps from Theorem~\ref{T1}'s proof through~\eqref{eq:6} for
the iterated bootstrap to get
\begin{align*}
  \hat\tau^{*}(h) - &\Delta^{**}(h,b) - \tau^*
  = (\hat\tau^*(h) - \E^* \hat \tau^*(h)) +
  (\E^* \hat \tau^*(h) - \tau^*) - (\E^{**} \hat \tau^{**}(h) - \tau^{**}) \\
  &= \hat\tau^*(h) - \E^*\hat\tau^*(h)
   + h^2 (\hat\mu_{-,2}^{*(2)}(b) - \mu_-^{*(2)}) \Bf_{-}(h) /2
   - h^2 (\hat\mu_{+,2}^{*(2)}(b) - \mu_+^{*(2)}) \Bf_{+}(h) / 2 \\
  &= e_0' \Gamma_{+,1}(h)^{-1} \Big(\tfrac{1}{n}
  \sum_{i=1}^n r_1(X_i/h) K_{+,h}(X_i) \varepsilon_i^*\Big)
   - e_0' \Gamma_{-,1}(h)^{-1} \Big(\tfrac{1}{n}
  \sum_{i=1}^n r_1(X_i/h) K_{-,h}(X_i) \varepsilon_i^*\Big) \\
  &\quad + (h/b)^2 \cdot e_2' \Gamma_{-,2}(b)^{-1}
    \Big(\tfrac{1}{n} \sum_{i=1}^n r_2(X_i/b) K_{-,b}(X_i) \varepsilon_i^* \Big) \Bf_-(h) \\
  &\quad - (h/b)^2 \cdot e_2' \Gamma_{+,2}(b)^{-1}
    \Big(\tfrac{1}{n} \sum_{i=1}^n r_2(X_i/b) K_{+,b}(X_i) \varepsilon_i^*\Big) \Bf_+(h)
\end{align*}
with the last equality holding from the relationship between the coefficient
estimates discussed at the end of the introduction to the appendix.
We will prove convergence of the variance first, then prove asymptotic
normality.

To prove convergence in probability of the bootstrap variance, note that the
wild bootstrap ensures
\begin{equation*}
  \V^*\Big(\sum_{i=1}^n R(X_i) \varepsilon_i^*\Big)
  = \sum_{i=1}^n R(X_i)^2 \hat\varepsilon_i^2.
\end{equation*}
for any measurable function $R(\cdot)$.
Given this expression for the iterated bootstrap and the formula for the
variance conditional on $X$ presented at the end of the introduction to the
appendix, it suffices to show that
\begin{gather}
  \tfrac{1}{n} \sum_{i=1}^n r_1(X_i/h) r_1(X_i/h)' K_{\bullet,h}(X_i)^2
  \hat\varepsilon_i^2 = \tfrac{1}{h} \Psit_{\bullet,1,1}(h,h) + o_p(\tfrac{1}{h})
  \label{eq:3} \\
  \tfrac{1}{n} \sum_{i=1}^n r_1(X_i/h) r_2(X_i/b)' K_{\bullet,h}(X_i)
  K_{\bullet,b}(X_i) \hat\varepsilon_i^2 = \tfrac{m}{hb} \Psit_{\bullet,1,2}(h,b)
  + o_p(\tfrac{m}{hb})
  \label{eq:5} \\
  \tfrac{1}{n} \sum_{i=1}^n r_2(X_i/b) r_2(X_i/b)' K_{\bullet,b}(X_i)^2
  \hat\varepsilon_i^2 = \tfrac{1}{b} \Psit_{\bullet,2,2}(b,b) + o_p(\tfrac{1}{b}).
  \label{eq:7}
\end{gather}
But these convergence results follow from CCT's results on
$\hat\mu_{\bullet,p}^{(\nu)}$, Lemmas SA3 and SA4 in particular,
and standard arguments on the convergence of residuals to
the population error term.

Normality also follows standard arguments for the wild
bootstrap. Conditional on the regressors and residuals,
$\{R(X_i) \hat\varepsilon_i e_i^* \}$ is a sequence of independent and
mean zero random variables for any measurable function $R$, each with
$q$th absolute moment
$\lvert R(X_i) \hat\varepsilon_i \rvert^q \E \lvert e_i^*
\rvert^{q}$.
The result then holds as a consequence of the Lindeberg-Feller CLT if
we establish that Liapunov's condition holds in probability, specifically that
\begin{equation}\label{eq:17}
  \big(\tfrac{1}{n}\big)^{1+\delta/2} \sum_{i} \lvert (R_+(X_i) - R_-(X_i))
  \hat\varepsilon_i \rvert^{2+\delta} \to^{p} 0.
\end{equation}
with
\begin{equation*}
  R_\bullet(X_i) = h^{1/2} \cdot \gamma_1' r_1(X_i/h) K_{\bullet,h}(X_i)
  + b^{1/2} \cdot \gamma_2' r_2(X_i/b) K_{\bullet,b}(X_i)
\end{equation*}
for arbitrary nonzero vectors $\gamma_1$ and $\gamma_2$.
\citep[See, for example,][Proposition 2.27 and Theorem 23.4.]{VDV00}
But similar arguments to those used for the variance establish
that
\begin{equation}\label{eq:16}
  \tfrac{1}{n} \sum_{i} \lvert (R_+(X_i) - R_-(X_i))
  \hat\varepsilon_i \rvert^{2+\delta}
  = O_p(h^{-\delta/2}) + O_p(b^{-\delta/2}).
\end{equation}
Since $nb \to \infty$ and $n h \to \infty$ by
assumption,~\eqref{eq:16} implies that~\eqref{eq:17} holds and
completes the proof. \qed

\clearpage
\bibliographystyle{jpe}
\bibliography{tex/references}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:

%  LocalWords:  Calonico Cattaneo Titiunik's Econometrica CCT AMSE CCT's DGP
%  LocalWords:  datasets frolich calonico HTV CIs analytical rdrobust
%  LocalWords:  undersmoothing doParallel foreach doparallel doRNG eq
%  LocalWords:  dorng nonparametrically RMSE ATEs resample
