\documentclass[12pt,fleqn]{article}

% Add commit information from Git to the pdf. It is automatically
% generated with the R script 'R/version.R' and can be run from
% the command line in the project's top directory via:
%
% $ Rscript R/version.R
%
% If VERSION.tex does not exist we can't add information from
% Git, so we'll use today's date as a fallback.

\IfFileExists{./VERSION.tex}{\input{VERSION}}{%
\providecommand\VERSION{\today}}

\input{tex/setup}
\input{tex/macros}

\title{Bootstrap Confidence Intervals for Sharp Regression Discontinuity Designs
  with Uniform Kernels}

\author{Ot\'avio Bartalotti \and Gray Calhoun \and Yang He\thanks{All authors: 
Department of Economics, Iowa State University. 260 Heady Hall, Ames, IA  50011.
Bartalotti: Email: bartalot@iastate.edu. Calhoun: gcalhoun@iastate.edu.
He: yanghe@iastate.edu.}}

\begin{document}
\maketitle

\begin{abstract}\noindent
  This paper develops a novel bootstrap procedure to obtain robust
  bias-corrected confidence intervals in regression discontinuity designs that
  use the uniform kernel. The procedure uses a residual bootstrap to estimate
  the bias of the RD estimator, which is then removed from the original estimator.
 The bias-corrected estimator is then bootstrapped itself to
  generate valid confidence intervals. This procedure is valid for empirically
  relevant bandwidths when the residual bootstraps use a polynomial of higher
  order than used to estimate the treatment effect and can be applied under the
  same conditions as Calonico, Cattaneo and Titiunik's (2014,
  \textit{Econometrica}) analytical correction.
\end{abstract}

\paragraph{Important message from the authors:} This version of the paper is
very preliminary and incomplete. We have sent out a few early copies to get
feedback, but there are still serious errors that we have not addressed. Please
do not cite this paper or circulate it in its current form.

\newpage
\section{Introduction}
Regression Discontinuity (RD) designs have emerged in the last decade as an
important and popular research design strategy for analyzing the causal impact
of policies and interventions in several fields of the social sciences,
including economics, political science, public policy, and sociology, among
others. This strategy exploits the fact that many programs use somewhat
arbitrary thresholds to determine whether or not to provide a ``treatment,''
usually relying on some type of score, which this literature calls a ``running
variable.'' In its basic version, individuals or groups with score above a
certain threshold receive treatment while the ones below that cutoff are left
untreated.

The identification of the treatment effect at the threshold is then based on
comparing treated and untreated units at the cutoff. Since the observed
characteristics of these subjects are very similar, similarity of their
unobserved characteristics is often credible as well, and differences in the
outcomes between the treated and untreated can be plausibly attributed to the
treatment alone. As a practical matter this involves comparing units within a
bandwidth just above and just below the threshold.

The RD design strategy was introduced by \cite{thistlethwaite1960} to study
educational outcomes, and many of its first recent applications in economics
were to estimate the effects of other educational policies: evaluating the
impact of investments in school facilities, class sizes, remedial education,
early childhood education, and financial aid effects on student achievement and
later outcomes, for example.\footnote{%
  See, for example, \cite{vdk2002}, \cite{jacoblefgren2004}, \cite{ludwig2007},
  \cite{urquiola2009}, \cite{cellini2010}} %
But the underlying identification strategy has proven to apply much more widely,
and RD has been used in health economics\footnote{%
  \cite{Card2009,barreca2011saving}}; %
political science, providing evidence regarding incumbency advantage in
elections, electoral enfranchising and policy changes associated with the
introduction of electronic voting, strategic voting behavior, and local media
effects and ad expenditure in presidential elections in the U.S.\footnote{%
  \cite{lee2008}, \cite{Caughey2011}, \cite{keele2014geographic},
  \cite{erikson2015}, \cite{Fujiwara2011,Fujiwara2015}}, %
among other fields. \cite{imbens2008} and \cite{lee2010} provide recent
overviews of this literature with many more examples.

In these studies, identification typically occurs exactly at the cutoff, so the
treatment effect is typically estimated by fitting separate local linear models
above and below the cutoff, then extrapolating the model to the exact point of
discontinuity. The difference between the estimated effects at that point is
taken to be an estimate of the treatment effect. As a practical matter, the key
econometric issue is determining the bandwidth for the local linear models.  One
very popular choice is the bandwidth estimator proposed by \cite{IK} and
extended by \cite{calonico2014}, which minimizes the Asymptotic Mean Squared
Error of difference in the models' point estimators at the cutoff.

But, as \cite{calonico2014}\footnote{%
  We will refer to \cite{calonico2014} frequently, so we will reference it as
  ``CCT'' for the rest of the paper.} %
point out, the \cite{IK} bandwidth estimator has the serious drawback that,
although it is AMSE-optimal for point estimation, it produces invalid
\emph{confidence intervals} and hypothesis tests. The AMSE-optimal bandwidth
converges to zero slowly, so the remaining bias term is large enough to affect
the asymptotic distribution of the estimator. In that case the usual ``naive''
confidence intervals for the RD treatment effects may be biased and have
coverage well below their nominal level. CCT then make two contributions: first,
they propose an estimator of the bias resulting from undersmoothing and provide
a treatment effect estimator that remains asymptotically unbiased even when the
bandwidth converges to zero slowly, at the AMSE-optimal rate.  Second, since
this bias-correction term contributes to the asymptotic variance of the
resulting treatment effect estimator, CCT also provide a new formula for its
asymptotic variance. The resulting confidence intervals have accurate coverage
even when the naive RD interval does not.

In this paper, we propose a bootstrap alternative to CCT's analytical
corrections. CCT justify their estimator by showing that the bias and variance
components for a local linear model can be accounted for by estimating a local
second order polynomial with bandwidth of the same order.\footnote{%
  More generally, they show that the bias and variance of a local polynomial of
  order $p$ can be accounted for by estimating the $p+1$ local polynomial. We
  will restrict our analysis to the case with $p = 1$ in this paper because
  of its widespread use.} %
They use a Taylor expansion around the cutoff to show that the bias associated
with the second order polynomial converges to zero at a faster rate, fast
enough that the bias of the local linear model can be estimated and removed
using the second order polynomial. The second order polynomial also provides
fast enough convergence that it can be used to estimate the correct variance
correction as well.

Our approach exploits CCT's theoretical insight through a new residual
bootstrap. In particular, we propose, after estimating the local linear model as
usual, estimating a local second order polynomial and generating bootstrap
datasets by resampling the residuals of that polynomial. Since the second order
polynomial is the true Data Generating Process (DGP) in the bootstrapped data, its estimate of the
treatment effect is the true value of the treatment effect under the
distribution induced by this bootstrap. The bias of the linear model is known
under this distribution and can be calculated by averaging the error of the
linear model's estimates across many bootstrap replications. This approach is
described in detail by our Algorithm~\ref{Alg1} and the resulting bias corrected
estimator is shown to be asymptotically normal with mean zero in our
Theorem~\ref{T1} under AMSE-optimal bandwidth rates.

Just as in CCT, this bias correction introduces additional variability. However,
the second order polynomial again adequately estimates the features of the true
DGP that are necessary for estimating that additional variability. So we propose
an iterated bootstrap procedure \citep{hall1988}: use the second order
polynomial residual bootstrap to produce many bootstrap replications of the bias
corrected estimator, and then use the resulting bootstrap distribution to
produce confidence intervals. This procedure, which requires bootstrapping the
datasets produced by an initial bootstrap procedure, is described in Algorithm~\ref{Alg2}, and the resulting confidence intervals are shown to be asymptotically valid in Theorem~\ref{T2}.

This bootstrap procedure can offer some advantages over analytical methods in
this setting. In particular, both this paper and CCT assume that the
observations are generated independently of each other; extending these bias
correction methods to other forms of dependence is relatively straightforward
for the bootstrap but can be substantially more complicated for analytical
corrections, which have to be explicitly derived by the researcher. Similarly,
although we only provide results for the local linear model in this paper, it is
trivial to implement this procedure for higher order polynomials, for
covariate-adjusted estimators (\citealp{frolich2007}, \citealp{calonico2015}),
or for other local smoothers. \citep{loader2006} However, in this paper we focus
on the baseline case of sharp RD with a local linear model and uniform
kernel. Extensions to fuzzy RD designs\footnote{%
  ``Fuzzy'' regression discontinuity design \citep{HTV2001}, as opposed to
  ``sharp'' RD, describes situations where the probability of treatment changes
  discontinuously at a known threshold, but by less than 100\%. Then there are
  treated and untreated subjects above and below the discontinuity but the
  treatment effect remains identified.} %
and nonuniform kernels, which require nontrivial changes to the underlying
bootstrap algorithm, as well as developments to address cross sectional
dependence, are the subject of ongoing research.

The paper is organized as follows. Section~\ref{background} describes the basic
RD approach, its usual implementation, and the explicit analytical bias
correction approach in the literature. Section~\ref{boot} presents our proposed
bootstrap bias corrected RD algorithm and discusses its asymptotic
properties. Simulation evidence that the bootstrap procedure provides valid CIs
and its relative performance to the analytical bias correction are presented in
Section~\ref{sim}, and Section~\ref{application} demonstrates the estimator's
usage by applying it to the Head Start dataset used by~\cite{ludwig2007}.
Finally, Section~\ref{conclusion} concludes.

\section{Background}\label{background}

This section provides additional details of RD estimators in general and of
CCT's proposed bias correction. It also defines some of the notation and
presents the assumptions that will be used for our theoretical analysis in
Section~\ref{boot}.

In the typical sharp RD setting, a researcher wishes to estimate the local
causal effect of treatment at a given threshold. A running variable, $X_{i}$,
determines treatment assignment.  Given a known threshold, $\bar{x}$, which can
be set to zero without loss of generality, the $i$th subject receives the
treatment of interest if $X_{i} \geq 0$ and does not receive treatment if
$X_{i} < 0$.

Subject $i$'s potential outcomes are denoted by the variable $Y_i(\cdot)$,
$Y_i(1)$ indicates that the subject has received treatment and $Y_i(0)$
indicates that the subject has not received treatment. Since only one of the two
outcomes is observed, the sample is comprised of the running variable, $X_{i}$,
and the observed outcome $Y_i$, where
\begin{equation*}
  Y_{i}=Y_{i}(0) \1\{X_{i}<0\}+Y_{i}(1) \1\{X_{i} \geq 0\}
\end{equation*}
and $\1\{ \cdot\}$ denotes the indicator function.

In most cases the population parameter of interest is the Average Treatment
Effect (ATE) at the cutoff, which we will denote $\tau$. This parameter is the
difference in expected potential outcomes given $X_i = 0$; formally,
\begin{equation*}
  \tau=\E(Y(1)-Y(0) \mid X=0).
\end{equation*}
\cite{HTV2001} show that the effect $\tau$ is identified under continuity and
smoothness conditions on the joint distribution of $X_i$, $Y_i(0)$, and $Y_i(1)$
around the cutoff $X_i = 0$. When these conditions hold, which are made precise
in Assumption~\ref{A1}, $\tau$ is equal to
\begin{equation*}
  \tau = \lim_{x \to 0+} \mu(x) - \lim_{x \to 0-}(x)
\end{equation*}
where
\begin{equation*}
  \mu(x)= \E(Y_{i} \mid X_{i}=x).
\end{equation*}
For convenience, also define
\begin{equation*}
  \mu^{(\eta)}(x)=\frac{d^{\eta}\mu(x)}{dx^{\eta}}
\end{equation*}
and let
\begin{align*}
  \mu_{+}(x)
  &= \E( Y_{i}(1) \mid X_{i}=x )
  &\mu_{-}(x)
  &= \E( Y_{i}(0) \mid X_{i}=x ) \\
  \sigma^{2}_{+}(x) &= \V( Y_{i}(1) \mid X_{i}=x )
  &\sigma^{2}_{-}(x)&=\V( Y_{i}(0) \mid X_{i}=x ) \\
\intertext{and}
  \mu^{(\eta)}_{+}
  &= \lim_{x \rightarrow 0^{+}}\mu^{(\eta)}(x),
  &\mu^{(\eta)}_{-}
  &= \lim_{x \rightarrow 0^{-}}\mu^{(\eta)}(x),
\end{align*}
where the symbol $\V(\cdot)$ represents the variance.  The effect $\tau$ is
nonparametrically identified because both $\mu_-$ and $\mu_+$ can be estimated
consistently under the conditions spelled out in Assumption~\ref{A1}.

\begin{assumption}[Behavior of the DGP near the cutoff]\label{A1}
  The random variables $Y_i, X_i$ form a random sample of size $N$.
  There exists a positive number $\kappa_0$ such that the following
  conditions hold for all $x$ in the neighborhood $(-\kappa_{0},\kappa_{0})$
  around zero:
  \begin{enumerate}
  \item The density of each $X_i$ is continuous and bounded away from zero.
  \item $\E(Y_{i}^{4} \mid X_{i}=x)$ is bounded.
  \item $\mu_+(x)$ and $\mu_-(x)$ are both 3 times continuously differentiable.
  \item $\sigma_+^2(x)$ and $\sigma_-^2(x)$ are both continuous and bounded away
    from zero.
 \end{enumerate}
\end{assumption}

These conditions are the usual smoothness and existence conditions in which the
RD literature relies. (See, in particular, \citealp{HTV2001},
\citealp{Porter03}, and CCT). The third part of the assumption is especially
important for the characterization of the leading bias term drives CCT's bias
correction and our bootstrap procedure.

Since the conditions for identification only need to hold in a neighborhood
around the cutoff, $\mu_+$ and $\mu_-$ can be estimated by extrapolating from a
local polynomial regression.  We will focus here on local linear
regression.\footnote{%
  See \cite{HTV2001}, \cite{Porter03} or \cite{FanGijbels92} for discussions of
  the properties of local polynomial regressions for boundary problems. The
  bootstrap algorithm proposed in this paper can be extended to accommodate
  higher order polynomials discontinuities in the derivatives of the conditional
  expectation, like ``Kink RD'' design \citep{card2009b}. We focus on the linear
  case for ease in exposition.} %
For this model, if $h$ represents a bandwidth parameter, the estimator of
$\tau$, $\hat\tau(h)$, is
\begin{equation}
  \label{eq:13}
  \hat{\tau}(h) = \hat {\mu}_{+}(h) -\hat{\mu}_{-}(h)
\end{equation}
with
\begin{align*}
  (\hat {\mu}_{+}(h),\ \hat {\mu}^{(1)}_{+}(h))'
  &= \argmin_{\beta} \sum_{i=1}^{N}
  \1\{h > X_{i} \geq 0\} (Y_{i} - \beta_0 - X_{i} \beta_1)^{2}
\intertext{and}
  (\hat {\mu}_{-}(h),\hat {\mu}^{(1)}_{-}(h))'
  &= \argmin_{\beta} \sum_{i=1}^{N}
  \1\{0 > X_{i} > -h_n \} (Y_{i} - \beta_{0} - X_{i} \beta_{1})^{2}.
\end{align*}

As shown by \cite{HTV2001}, \cite{Porter03} and expanded by CCT identification and inference procedures can be developed based on the following assumptions.\footnote{The assumptions below are the same as presented by CCT.}


\begin{assumption}[Bandwidth]\label{A2}
  Let $h_{n}$ be the bandwidth used to estimate the local linear model and let
  $b_{n}$ be the bandwidth used to estimate a local quadratic model. Then
  $n h_{n} \to \infty$, $n b_{n} \to \infty$, $n h_{n}^{5} b_{n}^{2} \to 0$, and
  $n b_{n}^{5} h_{n}^{2} \to 0$ as $n \to \infty$.\footnote{%
    Unless otherwise stated, all limits in this paper are assumed to hold as
    $n \to \infty$.} %
  The relationship $h_{n} \leq b_{n}$ also holds for all $n$.
\end{assumption}

CCT point out that the conventional approaches to construct confidence intervals for $\tau$ using the local linear estimator rely on a large-sample approximation for the standardized $t$-statistic that is valid only if the bandwidth shrink fast enough to eliminate the leading bias term contribution to the approximation. Hence, if $nh_{n}^{5}\rightarrow0$ and $nh_{n}\rightarrow \infty$, then
 \begin{align}
  T(h_{n}) &=\frac{\hat{\tau}(h_{n})-\tau}{\sqrt{V(h_{n})}}\rightarrow_{d}N(0,1), &
  V(h_{n}) &=\V(\tau(h_{n}) \mid X_{1},\dots,X_{n})
 \end{align}
 The conditional variance $V(h_{n})$ depends on the $\sigma^{2}_{+}=lim_{x \rightarrow 0^{+}}\sigma^{2}_{+}(x)$ and $\sigma^{2}_{-}=lim_{x \rightarrow 0^{-}}\sigma^{2}_{-}(x)$ as well as $h_{n}$ and known quantities that depend on the kernel and order of polynomial used in the estimation. Under these conditions we could use a conventional confidence interval for $\tau$ given by
 \begin{align}
  I(h_{n})=\left[\hat{\tau}(h_{n})\pm \Phi^{-1}_{1-\frac{\alpha}{2}}\sqrt{V(h_{n})} \right]
 \end{align}
with $\Phi^{-1}_{1-\frac{\alpha}{2}}$ being the $\alpha$-quantile of the standard normal distribution. However, most approaches to select the bandwidth, $h_{n}$ in the literature, including the widely used optimal-AMSE bandwidth selector proposed by \cite{IK} lead to bandwidth choices that are ``too large'' since they do not satisfy the condition $nh_{n}^{5}\rightarrow0$, leading to a first-order bias in the distributional approximation used to construct the confidence intervals.

CCT solve the problem by deriving the analytical form of the first-order bias and correcting it directly. To obtain correct coverage the confidence interval needs not only to be re-centered to correct the bias, but also rescaled to allow for the additional variability introduced by the bias correction. Their approach relies on obtaining a bias-corrected  estimator, $\hat{\tau}^{bc}$ and its adjusted variance. It is important first to note that the first-order bias term itself will depend on the bandwidth $h_{n}$. The approximate bias term can be described as:
\begin{align*}
     \E[\hat{\tau}(h_{n}) \mid \chi_{n}]-\tau=& h_{n}^{2}\Bs(h_{n})\{1+o_{p}(1)\}\\
     \Bs(h_{n})=&\frac{\mu_{+}^{(2)}}{2!}\Bf_{+}(h_{n})-\frac{\mu_{-}^{(2)}}{2!}\Bf_{-}(h_{n})
   \end{align*}
where $\Bf_{+}(h_{n})$ and $\Bf_{-}(h_{n})$ are observed quantities that depend on the data, kernel and $h_{n}$. The plug-in bias-corrected estimator then requires estimates for the second derivatives of the conditional mean from above and below the cutoff, $\mu_{+}^{(2)}$ and $\mu_{-}^{(2)}$. CCT propose using a conventional local quadratic estimator, i.e. one order higher than the polynomial used to obtain $\hat{\tau}$,  using a (potentially) different pilot bandwidth $b_{n}$.
\begin{align*}
     \hat{\tau}^{bc}(h_{n}, b_{n})=& \hat{\tau}-h_{n}^{2}\hat{\Bs}(h_{n},b_{n})\\
     \hat{\Bs}(h_{n})=&\frac{\hat{\mu}_{+}^{(2)}(b_{n})}{2!}\Bf_{+}(h_{n})-\frac{\hat{\mu}_{-}^{(2)}(b_{n})}{2!}\Bf_{-}(h_{n})
   \end{align*}
By incorporating the contribution of both $\hat{\tau}(h_{n})$ and $\hat{\Bs}(h_{n},b_{n})$ to the asymptotic distribution of the estimator, CCT obtain a robust confidence interval with a different asymptotic variance in general. So under Assumptions \ref{A1} and \ref{A2}, if $n\min\{h_{n}^{5}, b_{n}^{5}\}max\{h_{n}^{2}, b_{n}^{2}\}\rightarrow 0$ and $n\min\{h_{n}, b_{n}\}\rightarrow \infty$ and $\kappa \max\{h_{n},b_{n}\}< \kappa_{0}$, they show that
\begin{align}
  T^{rbc}(h_{n}, b_{n})=\frac{\hat{\tau}^{bc}(h_{n}, b_{n})-\tau}{\sqrt{V^{bc}(h_{n}, b_{n})}}\rightarrow_{d}N(0,1), \text{        }V^{bc}(h_{n}, b_{n})=V(h_{n})+C^{bc}(h_{n}, b_{n})\\
 \end{align}
where the additional term $C^{bc}(h_{n}, b_{n})$ depends on the (asymptotic) variability of the bias estimate used for correction as well as its correlation with the original RD estimator $\hat{\tau}(h_{n})$.\footnote{The details of the formulas are currently ommitted from this paper since their form is not relevant to the bootstrap procedure innovation proposed. The details can be found at CCT Appendix.}  Under these conditions we can construct a valid confidence interval for $\tau$ given by
 \begin{align}
  I^{rbc}(h_{n}, b_{n})=\left[\left(\hat{\tau}(h_{n})-h_{n}^{2}\hat{\Bs}(h_{n},b_{n})\right)\pm \Phi^{-1}_{1-\frac{\alpha}{2}}\sqrt{V(h_{n})+C^{bc}(h_{n}, b_{n})} \right]
 \end{align}

This approach is shown by the authors to significantly improve coverage of the CIs constructed and provides practitioners with a new tool set to perform inference that is more robust to the choice of bandwidth. We build upon the insight provided by CCT bias-corrected estimator and propose a simple bootstrap procedure that can directly construct the robust CIs without requiring the derivation of analytical formulas and direct estimators for the bias, variance and covariance terms, while relying on the same first-order bias correction approximation.

\section{Bootstrap Bias Correction}\label{boot}

Building upon the results and intuition developed by CCT we propose to implement a first-order bias correction and obtain valid CIs through a bootstrap procedure.
Intuitively, for the local-linear RD estimator's case, we will rely on a local quadratic polynomial approximation for the conditional outcome around the cutoff to capture the potential bias present in the original RD estimate. This insight comes directly from CCT's results using the same local quadratic approximation to estimate bias term as presented in Section \ref{background}.

The first algorithm describes how to obtain a point estimate and perform the bias correction. The second algorithm obtains its confidence interval with correct coverage. For simplicity, we adopt the same optimal-AMSE bandwidth selector proposed by CCT.\footnote{An interesting question is whether one could use a bootstrap based procedure to select a bandwidth that would achieve similar (or improved) results. This is beyond the scope of this particular paper but is part of ongoing research effort.} 

Essentially, we apply CCT's insight that a polynomial one order higher can capture the first-order bias behavior and create a bootstrap procedure that can correctly mimic the behavior of the bias present in the original estimator. For such, for a chosen bandwidth, we estimate a local quadratic regression as suggested by CCT, but then bootstrap data by imposing the local quadratic functional form as the DGP. Then, in the bootstrap data the DGP is known. Let the ``true'' treatment effect under the bootstrap DGP be denoted by $\tau^{*}$. We then estimate the treatment effect using the (misspecified) local linear estimator, $\hat{\tau}^{*}$ to the generated data. Hence, the bias for a specific bootstrap data is given by $\tau^{*}-\hat{\tau}^{*}$. More importantly, by generating new bootstrapped data based on the same quadratic DGP and its residual's distribution, we can obtain an estimate of the first-order bias (the part that is captured by $\hat{\Bs}(h_{n},b_{n})$ in CCT's formulas on Section \ref{background}) in the original estimate by averaging across the bias in all $B$ bootstrapped data estimates, i.e., $\tau^{*} - \sum_{i = 1}^{B} \hat{\tau}_{ i}^{*}$. Algorithm~\ref{Alg1} describes this procedure step-by-step for clarity.

\begin{algorithm}[Bias estimation]\label{Alg1}
  Assume $h_{n}$ and $b_{n}$ are bandwidths as described by Assumption~\ref{A2} and define
  \begin{align}
    I^{-}(h) &= \{i : h < X_{i} < 0\}, &
    I^{+}(h) &= \{i : 0 < X_{i} < h\}.
  \end{align}
  Also define $M^{-}(h_{n})$ and $M^{+}(h_{n})$ to be the number of elements in
  $I^{-}(h_{n})$ and $I^{+}(h_{n})$ respectively.
  \begin{enumerate}
  \item Estimate the local polynomials $g_{p+1}^{+}$ and $g_{p+1}^{-}$ as defined by
    Equation --- and define the residuals
    \begin{equation}
      \label{eq:1}
      \hat\varepsilon_{i} =
      \begin{cases}
        Y_{i} - \hat g_{p+1}^-(X_{i}) & \textif\ X_{i} < 0 \\
        Y_{i} - \hat g_{p+1}^+(X_{i}) & \otherwise
      \end{cases}
    \end{equation}
  \item Repeat the following steps $B_1$ times to produce a sequence of bootstrap
    estimates $\hat\tau_1^*,\dots,\hat\tau_{B_1}^*$. For the $k$th step:
    \begin{enumerate}
    \item Draw an i.i.d. sample of size $M^-(b_n)$ from
      $\{\hat\varepsilon_i : i \in I^-(b_n)\}$ and one of size $M^+(b_n)$ from
      $\{\hat\varepsilon_i : i \in I^+(h_n)\}$. Let $\varepsilon_i^{*-}$ and
      $\varepsilon_i^{*+}$ denote the $i$th element of each sample and construct
      \begin{align}
        Y_i^{*-} &= \hat g_{p+1}^-(X_i^{*-}) + \varepsilon_i^{*-} &
        Y_i^{*+} &= \hat g_{p+1}^+(X_i^{*+}) + \varepsilon_i^{*+}
      \end{align}
    \item Calculate $\hat\mu_+^*$ and $\hat\mu_-^*$ as
      \begin{align}
        \hat\mu_-^*
        &= \argmin_{\beta_0} \min_{\beta_1,\dots,\beta_p}
          \sum_{i = 1}^{M^-(h_n)} (Y_i^{*-} -
          (\beta_0 + \beta_1 X_i^{*-} + \cdots + \beta_p (X_i^{*-})^p))^2 \\
        \hat\mu_+^*
        &= \argmin_{\beta_0} \min_{\beta_1,\dots,\beta_p}
          \sum_{i = 1}^{M^+(h_n)} (Y_i^{*+} -
          (\beta_0 + \beta_1 X_i^{*+} + \cdots + \beta_p (X_i^{*+})^p))^2
      \end{align}
    \item Save $\hat\mu_+^* - \hat\mu_-^*$ as $\hat\tau^*_k$.
    \end{enumerate}
  \item Estimate the bias as
    \begin{equation}
      \label{eq:2}
      \Delta^* = \tfrac{1}{B_1} \sum_{k=1}^{B_1} \hat\tau^*_k -
      \big[\hat g_{p+1}^+(0) - \hat g_{p+1}^-(0)\big]
    \end{equation}
  \end{enumerate}
\end{algorithm}
The distribution of $\Delta^{*}$ depends on $B_{1}$ and we could
incorporate that dependency into our notation by writing $\Delta^*_{B_1}$. However, we will follow standard practice in the bootstrap
literature and assume that $B_{1}$ is large enough that it can be ignored and define $\Delta^* = \plim_{B_{1} \to \infty} \Delta^*_{B_{1}}$

Under Assumptions~\ref{A1} and~\ref{A2} the procedure described by
Algorithm~\ref{Alg1} provides a consistent estimator of the bias component that
converges fast enough in probability that it can be be used as a correction. As is standard in the bootstrap literature, we will assume
that the number of bootstrap replications, $B_{1}$, is large enough that the simulation error can be ignored.

\begin{theorem}\label{T1}
  Under Assumptions~\ref{A1} and~\ref{A2},
\begin{align}
  \frac{(\hat\tau(h_{n},b_{n}) - \Delta^{*}(h_{n},b_{n}) - \tau)}{ V^{1/2}(h_n, b_n)} \to^{d} N(0,1)
\end{align}
\end{theorem}

With Algorithm~\ref{Alg1} at hand, we can move to generate the bias-corrected confidence intervals to perform inference about $\tau$. Here the idea is similar, but now we actually calculate a bias corrected estimate using Algorithm~\ref{Alg1} for every bootstrapped dataset generated, a bootstrap within the bootstrap. Each one of the bias corrected estimates for the generated data is denoted $\hat{\tau}^{bc*}$. These estimator's empirical distribution capture the variability induced by the first order bias correction that lead CCT to propose the rescaling of their confidence intervals by introducing $C^{bc}(h_{n},b_{n})$ in the asymptotic variance formula on Section \ref{background}.

\begin{algorithm}[Confidence intervals]\label{Alg2}
  Define $I^{-}$, $I^{+}$, $M^{-}$, and $M^{+}$ as in Algorithm~\ref{Alg1}.
  \begin{enumerate}
  \item Estimate the local polynomials $g_{p+1}^{+}$ and $g_{p+1}^{-}$ as defined by
    Equation --- and define the residuals
    \begin{equation}
      \label{eq:3}
      \hat\varepsilon_{i} =
      \begin{cases}
        Y_i - \hat g_{p+1}^-(X_{i}) & \textif\ X_{i} < 0 \\
        Y_i - \hat g_{p+1}^+(X_{i}) & \otherwise
      \end{cases}
    \end{equation}
  \item Repeat the following steps $B_{2}$ times to produce a sequence of bootstrap
    estimates $\hat\tau_1^{\prime*},\dots,\hat\tau_{B_{2}}^{\prime*}$. For the $k$th step:
    \begin{enumerate}
    \item Draw an i.i.d. sample of size $M^{-}(b_{n})$ from
      $\{\hat\varepsilon_{i} : i \in I^{-}(b_{n})\}$ and one of size $M^{+}(b_{n})$ from
      $\{\hat\varepsilon_i : i \in I^{+}(h_{n})\}$. Let $\varepsilon_{i}^{*-}$ and
      $\varepsilon_i^{*+}$ denote the $i$th element of each sample and construct
      \begin{align}
        Y_i^{*-} &= \hat g_{p+1}^-(X_i^{*-}) + \varepsilon_i^{*-} &
        Y_i^{*+} &= \hat g_{p+1}^+(X_i^{*+}) + \varepsilon_i^{*+}
      \end{align}
    \item Calculate $\hat\mu_+^*$ and $\hat\mu_-^*$ as
      \begin{align}
        \hat\mu_-^*
        &= \argmin_{\beta_0} \min_{\beta_1,\dots,\beta_p}
          \sum_{i = 1}^{M^-(h_n)} (Y_i^{*-} -
          (\beta_0 + \beta_1 X_i^{*-} + \cdots + \beta_p (X_i^{*-})^p))^2 \\
        \hat\mu_+^*
        &= \argmin_{\beta_0} \min_{\beta_1,\dots,\beta_p}
          \sum_{i = 1}^{M^+(h_n)} (Y_i^{*+} -
          (\beta_0 + \beta_1 X_i^{*+} + \cdots + \beta_p (X_i^{*+})^p))^2
      \end{align}
    \item Apply Algorithm~\ref{Alg1} to the bootstrapped data set,
      \begin{equation*}
        (Y_1^{*-}, X_1^{*-}),\dots,(Y_{M^-(b_n)}^{*-},X_{M^-(b_n)}^{*-}),
        (Y_1^{*+}, X_1^{*+}),\dots,(Y_{M^+(b_n)}^{*+},X_{M^+(b_n)}^{*+}),
      \end{equation*}
      using the same bandwidths $h_n$ and $b_n$ that are used in the rest of
      this algorithm but reestimating all of the local polynomials on the
      bootstrap data. Let $\Delta^{**}$ represent the bias estimator returned
      by Algorithm~\ref{Alg1}.
    \item Save the bias-corrected estimator
      $\hat\tau_k^{\prime*} = \hat\mu_+^* - \hat\mu_i^* - \Delta^{**}$.
    \end{enumerate}
  \item Define the confidence interval endpoints $\hat\ell(\alpha)$ and
    $\hat u(\alpha)$ as the $\alpha/2$ and $1-\alpha/2$ quantiles of the
    empirical CDF of $\hat\tau_1^{\prime*},\dots,\hat\tau_{B_2}^{\prime*}$.
  \end{enumerate}
\end{algorithm}

As earlier, we assume that $B_1$ and $B_2$ are large enough that simulation
error can be ignored. Define the limiting values
\begin{align*}
\Delta^{**} &= \plim_{B_1,B_2 \to \infty} \Delta^{**}_{B_1,B_2} \\
\hat\ell^*(\alpha) &= \plim_{B_1,B_2 \to \infty} \hat\ell^*_{B_1,B_2}(\alpha) \\
\hat u^*(\alpha) &= \plim_{B_1,B_2 \to \infty} \hat u^*_{B_1,B_2}(\alpha).
\end{align*}

Theorem~\ref{T2} establishes that this ``bootstrap-within-bootstrap''
approximates the asymptotic distribution of the bias-corrected
statistic proposed by Algorithm~\ref{Alg1} and justifies this second
algorithm.

\begin{theorem}\label{T2}
  Under Assumptions~\ref{A1} and~\ref{A2},
  \begin{gather}
    \label{eq:4}
    \V^*(\hat\tau^{*} - \Delta^{**})/V(h_n,b_n) \to^p 1
  \intertext{and}
  \label{eq:5}
    \sup_{x}
    \Big\rvert \Pr^*[\hat\tau^{*} - \Delta^{**} - \tau^* \leq x ]
    - \Pr[\hat\tau - \Delta^* - \tau \leq x] \Big\lvert \to^p 0.
  \end{gather}
\end{theorem}

Evidence of the usefulness of the procedures proposed above and their relative performance to the analytical bias correction proposed in CCT are presented in a series of Monte Carlo simulations in Section \ref{sim}.

\section{Simulation Evidence}\label{sim}

This section presents evidence from Monte Carlo simulations that the bootstrap bias correction procedures proposed in Section \ref{boot} produces valid, robust confidence intervals similar to the ones obtained by the analytical procedures established in CCT. The bootstrap CIs obtained compare favorably to the analytical alternative, with coverage closer to the nominal size of the desired test and shorter length of the intervals in the specifications implemented.

We adopt the same three data generating processes (DGP) used by CCT to make it easy to compare our results. Also, these DGPs are widely used in the RD literature to evaluate the performance of inference procedures and provide a good testing ground for bias correction due to their exacerbated curvature.

For all Monte Carlo experiments, the pseudo data are generated as 500 i.i.d. draws from:
\begin{align*}
& Y_{i} = \mu_{j}(X_{i}) + \epsilon_{i}, \;\;\; X_{i} \sim  (2 B (2,4) - 1)), \\
& \epsilon_{i} \sim N(0, \sigma^{2}), \;\;\; \sigma = 0.1295, \;\;\; j = 1,2,3.
\end{align*}
Regarding the functional form for $\mu(x)$ in each DGP, the first one is based on a fifth order polynomial fitted on the data observed by \cite{lee2008rand} when estimating the incumbency advantage in electoral races to the U.S. House of Representatives. Hence, $\mu_{1}(x)$ is given by,
\begin{align}
\mu_{1}(x) & = 
\begin{cases}
0.48 + 1.27x + 7.18x^{2} + 20.21x^{3} + 21.54x^{4} + 7.33x^{5}, \;\;\;\; & \text{if} \;\; x < 0, \\
0.52 + 0.84x - 3.00x^{2} + 7.99x^3 - 9.01x^4 + 3.56x^{5},  & \text{if} \;\; x \ge 0.
\end{cases}
\end{align}

The functional form for $\mu_{2}(x)$, in the second DGP follows the data observed by \cite{ludwig2007}, which we examine in more detail on Section \ref{application}, and follows
\begin{align}
\mu_{2}(x) & = 
\begin{cases}
3.71 + 2.30x + 3.28x^2 + 1.45x^3 + 0.23x^4 + 0.03x^5, \;\; & \text{if} \;\; x < 0, \\
0.26 + 18.49x - 54.81x^2 + 74.30x^3 - 45.02x^4 + 9.83x^5,  & \text{if} \;\; x \ge 0.
\end{cases}
\end{align}

Finally, the third DGP used in the simulations is given by a modified version of  \cite{lee2008rand} designed to exhibit more curvature and, hence, be more sensitive to the choice of bandwidth in terms of bias. This functional form is the same as used in CCT and serves as an useful example of the effectiveness of the bias correction procedures implemented.
\begin{align}
\mu_{3}(x) & =
\begin{cases}
0.48 + 1.27x + 0.5 \times 7.18x^{2}+ \\
+ 0.7 \times 20.21x^3 + 1.1 \times 21.54x^4 + 1.5 \times 7.33x^5, \;\;\;\;\;\;\;\;\;\;\;\; & \text{if} \;\; x < 0, \\
0.52 + 0.84x - 0.1 \times 3.00x^{2}+ \\
+ 0.3 \times 7.99x^3 - 0.1 \times 9.01x^4 + 3.56x^5, & \text{if} \;\; x \ge 0.
\end{cases}
\end{align}

For each DGP, we used four methods to obtain point estimate and confidence intervals.\footnote{Note that the true value of $\tau$ is $0.04$ for DGPs 1 and 3 and $-3.45$ for DGP 2} For each method, a total 1500 replications were performed. The first two methods follow CCT analytical bias-correction procedure with triangular ($CCT_{tri}$) and uniform ($CCT_{uni}$) kernel, respectively. The third and fourth methods follow the bootstrap procedure proposed in Section \ref{boot} with uniform kernel with ($bootstrap_{uni}$) and without ($bootstrap_{uni}^{naive}$) bias-correction, respectively.

The ``naive'' bootstrap estimator, which bootstraps the confidence intervals for the usual RD estimator without any bias-correction serves as a baseline comparison, making clear the need of the corrections proposed, especially for DGPs with higher (potential) bias.

In the bootstrap procedure, we used 500 bootstraps for bias correction, 999 bootstraps to calculate confidence interval. Table \ref{Tb: simulation 1} presents the main results in which a local-linear RD estimator ($p=1$) is employed with the bias-correction based on a local-quadratic approximation ($q=2$).

\begin{table}[t]
\centering
\begin{tabular}{rrrrrrrrr}
  \toprule
	DGP & Method    & Bias    & SD    & MSE   & CI Coverage & CI Length \\
  \midrule
	1 & $CCT_{tri}$ & -0.010 & 0.068 & 0.068 & 0.911 & 0.241 \\ 
	  & $CCT_{uni}$ & -0.016 & 0.067 & 0.069 & 0.924 & 0.246 \\ 
	  & $bootstrap_{uni}$ & -0.018 & 0.066 & 0.069 & 0.931 & 0.240 \\ 
	  & $bootstrap_{uni}^{naive}$ & -0.020 & 0.060 & 0.064 & 0.916 & 0.208 \\
	\midrule
	2 & $CCT_{tri}$ & -0.011 & 0.085 & 0.086 & 0.935 & 0.343 \\ 
	  & $CCT_{uni}$ & -0.010 & 0.087 & 0.088 & 0.938 & 0.350 \\ 
	  & $bootstrap_{uni}$       & -0.012 & 0.088 & 0.089 & 0.953 & 0.324 \\ 
	  & $bootstrap_{uni}^{naive}$ & -0.051 & 0.086 & 0.100 & 0.869 & 0.300 \\ 
	\midrule
	3 & $CCT_{tri}$ & -0.008 & 0.067 & 0.067 & 0.919 & 0.246 \\ 
	  & $CCT_{uni}$ & -0.004 & 0.065 & 0.065 & 0.943 & 0.250 \\ 
	  & $bootstrap_{uni}$       & -0.004 & 0.064 & 0.064 & 0.960 & 0.246 \\ 
	  & $bootstrap_{uni}^{naive}$ & 0.016 & 0.062 & 0.064 & 0.931 & 0.218 \\ 
  \bottomrule
\end{tabular}
\caption{Summary of estimates ($p = 1, q = 2$)}
\label{Tb: simulation 1}
\end{table}

Table \ref{Tb: simulation 1} presents the results for the local linear model, with three panels reporting on each DGP described above. For each panel, we report bias, standard deviation, MSE, confidence interval empirical coverage and length for the four methods. For the three DGPs coverage for
    both analytical and bootstrap based bias correction methods is near nominal size but with slight undercoverage for CCT's analytical correction. For DGP 1, our robust bootstrap method improves coverage slightly, obtaining 93.1\% coverage against 92.4\% from CCT's analytical correction using the uniform kernel. Also, the CI length is slightly shortened on average. Both robust approaches improve over the naive estimator, even in this case for which bias is relatively small.

For the second DGP, based on \cite{ludwig2007}, the importance of correcting for the leading bias term presence becomes clear, with the naive bootstrap CI displaying an empirical coverage of only 86.9\%. Once again, the CIs based on both analytical and bootstrap robust methods improves the coverage meaningfully, with moderate gains in terms of coverage and length from $0.324$ to $0.350$ when using the bootstrap estimator relative to CCT's with uniform kernel.

In the case of the modified \cite{lee2008rand} DGP that exacerbates the importance of bias, $\mu_{3}(x)$, once more the robust bootstrap procedure provides similar empirical coverage and interval length relative to its analytical alternatives. The naive method once again suffers in terms of coverage due to the presence of uncorrected first order bias.

 Overall, the bootstrap bias-correction procedure proposed in this paper provides a simple alternative to obtain valid robust confidence intervals in RD designs and performs well compared to the analytical
bias correction procedures proposed by CCT. The results also corroborates CCT in the importance of bias correction when high curvature is present at the cutoff. 

\section{Application}\label{application}

In this section, we apply the bootstrap procedure to the data\footnote{The data is publicly available from http://faculty.econ.ucdavis.edu/faculty/dlmiller/statafiles/.} used in \cite{ludwig2007}. In their paper, the effects of Head Start application assistance on federal spending, mortality and education attainment were investigated under standard RD design. They utilized the fact that grant-writing assistance were offered only to the poorest 300 counties to identify the average treatment effect (ATE) at the cutoff.

We estimate the ATE with alternative procedures and compare the new estimates with results in \cite{ludwig2007}. Bias-corrected point estimates and robust confidence intervals are reported. For completeness, we also include results following the procedure proposed by CCT. In \cite{ludwig2007}, analytical standard error and percentile-T bootstrapped p-value were reported\footnote{However, we were unable to reproduce their point estimates for the ATE under the nonparametric setting using triangular kernel, which was documented in their paper. Instead, we used uniform kernel and obtained exactly the same results as theirs.}. The procedure proposed by CCT provides analytical standard error and confidence interval. Our bootstrap procedure provides standard error and confidence interval from the bootstrap distribution. In both CCT and bootstrap procedures, we set $p = 1$, $q = 2$ and used uniform kernel. In the bootstrap procedure, we used 500 bootstraps for bias correction and 999 for confidence interval.

These results are shown in Table \ref{tab: head start spending}-\ref{tab: education 2000 ages 25-34}. We find that (1) the choice of bandwidth may significantly affect estimated ATE and its significance level; (2) the optimal bandwidths following CCT procedure are much smaller than actually being used in \cite{ludwig2007} and (3) our bootstrap procedure produces similar results to CCT.

\begin{table}[ht]
	\centering
	\begin{tabular}{cccccc}
		\toprule
		& \multicolumn{3}{c}{LM 2007} & CCT 2014 & Bootstrap \\
		\midrule
		& \multicolumn{5}{c}{Panel A: 1968 Head Start spending per child} \\ 
		Bandwidth & 9 & 18 & 36 & 5.414, 10.175 & 5.414, 10.175 \\
		Effect    & 137.251 & 114.711 & 134.491 & 143.811 & 148.990 \\
		SE        & 128.968 & 91.267  & 62.593 & 113.404 & 182.729 \\
		p-value   & 0.157   & 0.138   & 0.045  & & \\
		95\% CI   & & & & (-78.457 366.078) & (-179.120  547.755) \\
		
		&&&&& \\
		& \multicolumn{5}{c}{Panel B: 1972 Head Start spending per child} \\ 
		Bandwidth & 9 & 18 & 36 & 5.034, 12.020 & 5.034, 12.020 \\
		Effect    & 182.119 & 88.959 & 130.153 & 94.260 & 100.404 \\
		SE        & 148.321 & 101.697 & 67.613 & 122.224 & 196.008 \\
		p-value   & 0.085   & 0.352   & 0.090  & & \\
		95\% CI   & & & & (-145.294 333.814) & (-165.816 622.976) \\
		
		&&&&& \\
		& \multicolumn{5}{c}{Panel C: 1972 other social spending per capita} \\ 
		Bandwidth & 9 & 18 & 36 & 4.612, 8.461 & 4.612, 8.461 \\
		Effect    & 14.474 & 19.590 & 14.506 & 15.418 & 15.857 \\
		SE        & 28.356 & 19.612 & 14.929 & 29.041 & 39.902 \\
		p-value   & 0.459   & 0.222   & 0.478  & & \\
		95\% CI   & & & & (-41.501 72.337) & (-64.153 91.999) \\
		\bottomrule
	\end{tabular}
	\caption{The effect of Head Start assistance on Head Start spending. The first three columns come from Table 2 in \cite{ludwig2007}.}
	\label{tab: head start spending}
\end{table}

\begin{table}[ht]
	\centering
	\begin{tabular}{cccccc}
		\toprule
		& \multicolumn{3}{c}{LM 2007} & CCT 2014 & Bootstrap \\
		\midrule
		Bandwidth & 9 & 18 & 36 & 3.888, 6.807 & 3.888, 6.807 \\
		Effect    & -1.895 & -1.198 & -1.114 & -3.795 & -3.852 \\
		SE        & 0.980 & 0.796  & 0.544 & 1.654 & 1.526 \\
		p-value   & 0.036   & 0.081   & 0.027  & & \\
		95\% CI   & & & & (-7.037 -0.554) & (-6.565 -0.578) \\
		\bottomrule
	\end{tabular}
	\caption{The effect of Head Start assistance on mortality. The first three columns come from Table 3 in \cite{ludwig2007}.}
	\label{tab: mortality}
\end{table}


\begin{table}[ht]
	\centering
	\begin{tabular}{cccc}
		\toprule
		& LM 2007 & CCT 2014 & Bootstrap \\
		\midrule
		& \multicolumn{3}{c}{Panel A: Fraction high school or more} \\
		Bandwidth & 7 & 3.671, 8.618 & 3.671, 8.618 \\
		Effect    & 0.030 & 0.055 & 0.054 \\
		SE        & 0.016 & 0.021 & 0.021 \\
		p-value   & 0.032 & & \\
		95\% CI        & & (0.014 0.096) & (0.010 0.094) \\
		
		&&& \\
		& \multicolumn{3}{c}{Panel B: Fraction some college or more} \\
		Bandwidth & 7 & 5.076, 10.251 & 5.076, 10.251 \\
		Effect    & 0.037 & 0.051 & 0.051 \\
		SE        & 0.020 & 0.024 & 0.024 \\
		p-value   & 0.031 & & \\
		95\% CI        & & (0.004 0.099) & (0.001 0.092) \\
		\bottomrule
	\end{tabular}
	\caption{The effect of Head Start assistance on education (1990, ages 18-24). The first column is result for directly treated cohorts (1990, ages 18-24) in Table 4 in \cite{ludwig2007}.}
	\label{tab: education 1990 ages 18-24}
\end{table}

\begin{table}[ht]
	\centering
	\begin{tabular}{cccc}
		\toprule
		& LM 2007 & CCT 2014 & Bootstrap \\
		\midrule
		& \multicolumn{3}{c}{Panel A: Fraction high school or more} \\ 
		Bandwidth & 7 & 4.837, 9.85 & 4.837, 9.85 \\
		Effect    & 0.000 & 0.014 & 0.013 \\
		SE        & 0.016 & 0.019 & 0.019 \\
		p-value   & 0.974 & & \\
		CI        & & (-0.024 0.051) & (-0.026 0.051) \\
		
		&&& \\
		& \multicolumn{3}{c}{Panel B: Fraction some college or more} \\
		Bandwidth & 7 & 5.382, 10.621 & 5.382, 10.621 \\
		Effect    & 0.028 & 0.033 & 0.032 \\
		SE        & 0.019 & 0.023 & 0.022 \\
		p-value   & 0.017 & & \\
		CI        & & (-0.014 0.079) & (-0.009 0.077)\\
		\bottomrule
	\end{tabular}
	\caption{The effect of Head Start assistance on education (2000, ages 18-24). The first column is result for directly treated cohorts (2000, ages 18-24) in Table 4 in \cite{ludwig2007}.}
	\label{tab: education 2000 ages 18-24}
\end{table}

\begin{table}[ht]
	\centering
	\begin{tabular}{cccc}
		\toprule
		& LM 2007 & CCT 2014 & Bootstrap \\
		\midrule
		& \multicolumn{3}{c}{Panel A: Fraction high school or more} \\ 
		Bandwidth & 7 & 6.074, 11.477 & 6.074, 11.477 \\
		Effect    & 0.006 & 0.014 & 0.014 \\
		SE        & 0.014 & 0.015 & 0.015 \\
		p-value   & 0.666 & & \\
		CI        & & (-0.014 0.043) & (-0.023 0.036) \\
		
		&&& \\
		& \multicolumn{3}{c}{Panel B: Fraction some college or more} \\
		Bandwidth & 7 & 5.509, 11.463 & 5.509, 11.463 \\
		Effect    & 0.040 & 0.049 & 0.049 \\
		SE        & 0.017 & 0.020 & 0.021 \\ 
		p-value   & 0.009 & & \\
		CI        & & (0.010 0.089) & (0.003 0.083) \\
		\bottomrule
	\end{tabular}
	\caption{The effect of Head Start assistance on education (2000, ages 25-34). The first column is result for directly treated cohorts (2000, ages 25-34) in Table 4 in \cite{ludwig2007}.}
	\label{tab: education 2000 ages 25-34}
\end{table}

\section{Conclusion}\label{conclusion}
This paper proposes a novel bootstrap procedure to obtain robust bias-corrected
confidence intervals in regression discontinuity designs. 
The approach proposed builds upon the developments and intuition advanced by CCT and is based on a first-order bias correction.
Our procedure provides an alternative to the plug-in analytical methods in the literature and is simple to implement generating robust confidence intervals.
Simulation evidence is presented that the proposed bootstrap bias correction and confidence intervals have improved coverage and shorter length relative to the analytical alternatives proposed by CCT.

\section{Proofs of mathematical results}
\subsection{Proof of Theorem~\ref{T1}}
We have
\begin{equation*}
  \hat\tau_h - \Delta^*_h - \tau = (\hat\tau_h - \E \hat \tau_h) +
  (\E \hat \tau_h - \tau) - (\E^* \hat \tau^*_h - \tau^*)
\end{equation*}
The design of the bootstrap ensures that
\begin{align*}
  \label{eq:6}
  \E^* \hat\mu_{+}^*(h_n) - \mu_{+}^{*}
  &= h_n^{2} \mu_+^{*(2)} \Bf_{+}(h_n)/2,
  &\E^* \hat\mu_{-}^*(h_n) - \mu_{-}^{*}
  &= h_n^{2} \mu_-^{*(2)} \Bf_{-}(h_n)/2,
\end{align*}
almost surely, implying that
\begin{equation*}
  \label{eq:7}
    \E^* \hat\tau^{*}_h - \tau^{*} = h_n^2\, \mu_+^{*(2)} \Bf(h_n)/2
      - h_n^2\, \mu_{-}^{*(2)} \Bf_{-}(h_n)/2.
\end{equation*}
Combining this result with CCT's Lemma A1, gives
\begin{align}
  \hat\tau_h - & \E \hat\tau_h + (\E \hat\tau_h - \tau) - (\E^* \hat\tau^*_h - \tau^*)\notag \\
  &= \hat\tau_h - \E\hat\tau_h
   + h_n^2 \big((\mu_{-}^{*(2)} - \mu_{-}^{(2)}) \Bf_{-}(h_{n}) /2
   - (\mu_+^{*(2)}-\mu_+^{(2)}) \Bf_{+}(h_{n}) / 2 \big) + O_p(h_n^{3}) \\
  &= \hat\tau_h - \E\hat\tau_h
   + h_n^2 (\hat\mu_-^{(2)}(b_n) - \mu_-^{(2)}) \Bf_{-}(h_{n}) /2 \notag \\
  \label{eq:8}
  &\pushright{- h_n^2 (\hat\mu_+^{(2)}(b_n) - \mu_+^{(2)}) \Bf_{+}(h_{n}) / 2 + O_p(h_n^{3})}
\end{align}
The second equality holds because
$\mu_+^{*(2)} = \hat\mu_{+}^{(2)}(b_n)$ and
$\mu_-^{*(2)} = \hat\mu_{-}^{(2)}(b_n)$ almost surely. Asymptotic
normality then follows from normality of $\hat\tau_h - \E\hat\tau_h$,
$\hat\mu_+^{(2)}(b_n) - \mu_+^{(2)}$, and
$\hat\mu_-^{(2)}(b_n) - \mu_-^{(2)}$ using similar arguments to
CCT's Lemma SA4.D.\qed

\subsection{Proof of Theorem~\ref{T2}}
Repeat the steps from Theorem~\ref{T1}'s proof through~\eqref{eq:8} to get
\begin{align*}
  \hat\tau_h^* - &\Delta_h^{**} - \tau^*
  = (\hat\tau^*_h - \E^* \hat \tau^*_h) +
  (\E^* \hat \tau^*_h - \tau^*) - (\E^{**} \hat \tau^{**}_h - \tau^{**}) \\
  &= \hat\tau^*_h - \E^*\hat\tau^*_h
   + h_n^2 (\hat\mu_-^{*(2)}(b_n) - \mu_-^{*(2)}) \Bf_{-}(h_{n}) /2
   - h_n^2 (\hat\mu_+^{*(2)}(b_n) - \mu_+^{*(2)}) \Bf_{+}(h_{n}) / 2 \\
  &= \Omega^{+}(b_n)' \epb_{+}^{*} - \Omega^{-}(b_n)' \epb_{-}^{*},
\end{align*}
where
$\epb_{+}^{*} =
(\varepsilon_{+,1}^{*},\dots,\varepsilon_{+,M_b^{+}}^{*})'$ and
$\Omega^{+}_b$ is an $M_b^{+}$-dimensional vector with $i$th element
$\Omega_{1i}^{+}(b_n) - \Omega_{2i}^{+}(b_n)$, which are defined as
\begin{align*}
  \Omega_{1i}^{+}(b) &=
  (1\quad 0)
  \Big(\ssum{j}{h_n} r_1(X_j / h_n) r_1(X_j / h_n)' \Big)^{-1}
    r_1(X_{m^+_i} / h_n) \1\{h_n > X_{m_i} \geq 0\}
\intertext{and}
  \Omega_{2i}^{+}(b) &=   (0\quad 0 \quad h_n^2) 
  \Big(\ssum{i}{b} r_2(X_j / b) r_2(X_j / b)' \Big)^{-1}
    r_2(X_{m^+_i} / b).
\end{align*}
Here $r_p(x) = (1, x, x^2,\dots,x^{p})'$ and $m^+_i$ is the subsequence of
$1,2,\dots$ such that $0 \leq X_{m_i} < b$. The definitions of
$\epb_{-}^{*}$ and $\Omega^-(b_n)$ have the same definitions as
$\epb_{+}^{*}$ and $\Omega^+(b_n)$ after making the obvious
substitutions of ``$-$'' for ``$+$.''

Given~\eqref{eq:8}, it suffices to prove that
\begin{gather}
  \label{eq:9}
  \rho(V^{-1/2}(h_n) \Omega^{+}(b_n)'\epb_{+}^{*},
    V^{-1/2}(h_n)\Omega^{+}(b_n)'\epb_{+}) \to^p 0
  \intertext{and}
  \label{eq:10}
  \rho(V^{-1/2}(h_n)\Omega^{-}(b_n)'\epb_{-}^{*},
    V^{-1/2}(h_n)\Omega^{-}(b_n)'\epb_{-}) \to^p 0
\end{gather}
with $\epb_+$ an i.i.d. random vector of length $M_b^+$, the $i$th
element of which is distributed as
\[
  \varepsilon_{i,+} =^d
  \begin{cases}
    Y_{m_1^+} - \E(Y_{m_1^+} \mid X_{m_1^+})
    & \text{with probability\ } 1/M_{b}^+ \\
    \vdots \\
    Y_{m_{M_b^+}^+} - \E(Y_{m_{M_b^+}^+} \mid X_{m_{M_b^+}^+})
    & \text{with probability\ } 1/M_{b}^+,
  \end{cases}
\]
$\epb_-$ the corresponding quantity after replacing ``+'' with ``--,'' and
$\rho$ the Mallows metric \citep{bickel1981}.\footnote{%
  For two random vectors $u$ and $v$ with finite 2nd moments, $\rho(u, v)$
  is defined as
  \begin{equation}
    \label{eq:11}
    \rho(u, v) = \inf\nolimits_{(U, V)\ s.t.\ U =^d u,\ V =^d v}
    \big(\E \lVert U - V \rVert^2\big)^{1/2}.
  \end{equation}}

We will only prove~\eqref{eq:9} since the proof of~\eqref{eq:10} is
identical. By Theorem 2.1 of \cite{freedman1981}, we have
\begin{align*}
  \rho(V^{-1/2}(h_n) \Omega^{+}(b_n)'\epb_{+}^{*}, V^{-1/2}(h_n)&\Omega^{+}(b_n)'\epb_{+}) \\
  &\leq \rho(\varepsilon_{+,1}^{*}, \varepsilon_{+,1}) \times
    \tr(V^{-1/2}(h_n) \Omega^{+}(b_n)' \Omega^{+}(b_n)V^{-1/2}(h_n)) \\
  &= \rho(\varepsilon_{+,1}^{*}, \varepsilon_{+,1}) \times O_{p^{*}}(1),
\end{align*}
the second line holding as a consequence of CCT's Lemma SA1. So it suffices
to show that $\rho(\varepsilon_{+,1}^{*}, \varepsilon_{+,1}) \to^{p^{*}} 0$. Let
$\dot{\varepsilon}_i$ be an i.i.d. sequence randomly drawn from $\epb_+$
with replacement and let
$\bar{\varepsilon} = (1/M_b^{+}) \osum{i}{b_n} \dot{\varepsilon}_i$. Then,
using the same arguments as \cite{freedman1981}, we have the upper bound
\begin{equation}
  \label{eq:12}
  \rho(\varepsilon_{+,1}^*, \varepsilon_{+,1})
  \leq
  \bar\varepsilon^2 +
  (2/M_{b_n}^+) \osum[+]{i}{b} (\hat\varepsilon_i - \varepsilon_i)^2
  + \rho(\dot\varepsilon_1, \varepsilon_{+,1}).
\end{equation}
But $\bar\varepsilon^2 \to^p 0$ by the LLN,
\[
  (1/M_{b_n}^+) \osum[+]{i}{b}
  \E((\hat\varepsilon_i - \varepsilon_i)^2 \mid X_i) \to 0
\]
by CCT's Lemma SA1, and
$\rho(\dot\varepsilon_1, \varepsilon_{+,1}) \to^p 0$ by Lemma~8.4 of
\cite{bickel1981}.\qed

\clearpage
\bibliographystyle{jpe}
\bibliography{tex/references}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:

%  LocalWords:  Calonico Cattaneo Titiunik's Econometrica CCT AMSE CCT's DGP
%  LocalWords:  datasets frolich calonico HTV CIs
