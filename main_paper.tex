\documentclass[12pt,fleqn]{article}

% Add commit information from Git to the pdf. It is automatically
% generated with the R script 'R/version.R' and can be run from
% the command line in the project's top directory via:
%
% $ Rscript R/version.R
%
% If VERSION.tex does not exist we can't add information from
% Git, so we'll use today's date as a fallback.

\IfFileExists{./VERSION.tex}{\input{VERSION}}{%
\providecommand\VERSION{\today}}

\input{tex/setup}
\input{tex/macros}

\title{Bootstrap Confidence Intervals for Regression Discontinuity Designs with Uniform Kernels}
\author{Ot\'avio Bartalotti \and Gray Calhoun \and Yang He\thanks{All authors: 
Department of Economics, Iowa State University. 260 Heady Hall, Ames, IA  50011.
Bartalotti: Email: bartalot@iastate.edu. Calhoun: gcalhoun@iastate.edu.
He: yanghe@iastate.edu.} \\ PRELIMINARY AND INCOMPLETE \\PLEASE DO NOT CITE OR CIRCULATE WITHOUT PERMISSION}

\begin{document}
\maketitle

\begin{abstract}\noindent
  This paper develops a novel bootstrap procedure to obtain robust
  bias-corrected confidence intervals in regression discontinuity designs that
  use the uniform kernel. The procedure uses a residual bootstrap to estimate
  the bias of the RD estimator, which is then removed from the original estimator.
 The bias-corrected estimator is then bootstrapped itself to
  generate valid confidence intervals. This procedure is valid for empirically
  relevant bandwidths when the residual bootstraps use a polynomial of higher
  order than used to estimate the treatment effect and can be applied under the
  same conditions as Calonico, Cattaneo and Titiunik's (2014,
  \textit{Econometrica}) analytical correction.
\end{abstract}

\section{Introduction}
Regression Discontinuity (RD) designs have emerged in the last decade as an 
important and popular identification strategy to analyze the impact of policies
and interventions in several fields of the social sciences, including economics,
political science, public policy, sociology among others.

The strategy exploits arbitrary rules used to assign treatment to units, usually
relying on some type of ``score,'' the so called running variable. In its basic 
version, units with score above a certain threshold receive treatment while the 
ones below that cutoff are left untreated.

The identification of the treatment effect at the threshold is then based on 
comparing treated and untreated units at the cutoff. As a practical matter this 
involves comparing units within a bandwidth just above and just below the 
threshold. That comparison is most commonly done by implementing a local linear
estimator above and below the cutoff within bandwidths chosen to minimize the 
asymptotic mean squared error (AMSE) as proposed by \cite{IK}
and extended by \cite{calonico2014} (CCT, henceforth).

RD designs have been used as a strategy to estimate effects of interest in a large range of topics in several areas. For just a few examples, RD designs are common in education, evaluating the impact of investments in school facilities, class sizes, remedial education, early childhood education, and financial aid effects on student achievement and later outcomes \citep{cellini2010,urquiola2009,jacoblefgren2004,ludwig2007,vdk2002}; in health by analyzing the impact of healthcare provision through Medicare, and neo-natal care on low birth weight children mortality \citep{Card2009,barreca2011saving}; in political science, providing evidence regarding incumbency advantage in elections, electoral enfranchising and policy changes associated with the introduction of electronic voting, strategic voting behavior, and local media effects and ad expenditure in presidential elections in the U.S. \citep{lee2008,Caughey2011,erikson2015,Fujiwara2015,Fujiwara2011,keele2014geographic}. Several more examples of the breadth of RD designs are listed by \cite{lee2010}.


Recently, CCT brought attention to the fact that the popular implementation of RD designs coupling local polynomial estimation and optimal-AMSE bandwidths (as proposed by \cite{IK})
provides confidence intervals (CIs) with incorrect coverage. They argue that the bandwidth selectors typically produce bandwidths that are too ``large,'' leading to a significant bias in the asymptotic distribution of the estimator. In that case the usual``naive'' confidence intervals for the RD treatment effects may be biased and will have provide coverage potentially well below their nominal level.

Then, CCT proposes a bias-correction procedure based on estimating the leading term of the bias to account for the effect of the ``large'' bandwidth and then obtain a re-centered t-statistic.    This conventional bias-correction has poor finite-sample performance that arises from the estimation of the bias leading term, so they provide a new standard error formula for the standard error to  account for the additional variability introduced by the estimated bias. The resulting statistic that combines the higher-order($p+1$) leading term bias correction and rescaling the of the t-statistic remains asymptotically normal and supplies valid confidence intervals.

As pointed out by \cite{ccf2016}, ``(...)valid inference
requires the delicate balancing act of selecting a bandwidth small enough
to remove smoothing bias, yet large enough to ensure adequate precision.''
The authors compare the strategies of undersmoothing, and explicit bias 
correction concluding that direct bias-correction with the corresponding 
rescaling of the CI's is a superior approach.

This paper contributes to the bias-corrected RD literature by proposing a simple
bootstrap procedure to correct the first order bias term and directly obtain 
valid CIs that provide the correct coverage.

The algorithm proposed draws on the main intuition put forward by CCT, that is, that correcting for the leading bias term (of order $p+1$) addresses the recentering needed for the confidence intervals. Hence, we propose that, after obtaining the usual RD estimates using a local polynomial of order $p$, obtain a estimates for a polynomial of order $p+1$ and generate our bootstrap datasets by bootstrapping the residuals of that polynomial. Since in the bootstrap datasets the bias of the estimates obtained using a lower order polynomial is known, we can estimate the leading term of the bias by averaging across bootstraps and finally subtract it from the original RD estimator.

Just as in CCT, this bias correction introduces additional variability due to the estimated bias, so we approximate the distribution of the bias-corrected estimator with a second residual bootstrap in which both the usual RD estimate and the full bias correction procedure are bootstrapped.

The novel bootstrap procedure is easy to implement and extends to RD designs 
using higher order polynomials, producing valid CIs which are robust to 
bandwidth choice and have similar properties to the analytical procedure in 
CCT while sidestepping the need to derive analytical formulas in each case of interest.

Note that the procedure as presented here only coversr the uniform kernel, sharp RD designs, and i.i.d. data. Extensions to other kernels, higher order discontinuities, cross-sectional dependence, and the fuzzy RD design are subject of ongoing research.

The paper is organized as follows. Section \ref{background} describes the basic
RD approach, its usual implementation, and the explicit analytical bias 
correction approach in the literature. Section \ref{boot} presents the proposed 
bootstrap bias corrected RD algorithm and discusses its properties. Simulation
evidence that the bootstrap procedure provides valid CIs and its relative 
performance to the analytical bias correction are presented in Section \ref{sim}.
Finally, Section \ref{conclusion} concludes.

\section{Background}\label{background}

In the typical sharp RD setting, a researcher wishes to estimate the local 
causal effect of treatment at a given threshold. The running variable, $X_{i}$, 
determines treatment assignment.  Given a known threshold, $\bar{x}$, set to 
zero without loss of generality, a unit receives treatment if $X_{i} \geq 0$ or 
does not receive treatment if $X_{i} < 0$. Let $Y_{i}(1)$ and $Y_{i}(0)$ denote 
the potential outcomes for unit $i$ given it receives treatment and
 in the absence of treatment, respectively. Hence, the observed sample is 
 comprised of the running variable, $X_{i}$, and
 \begin{align}
  Y_{i}=Y_{i}(0) \1\{X_{i}<0\}+Y_{i}(1) \1\{X_{i} \geq 0\}
 \end{align}
where $\1\{ \cdot\}$ denotes the indicator function. For convenience,
define
 \begin{align}
  \mu(x)= \E[Y_{i} \mid X_{i}=x]
 \end{align}
Also, let $\mu^{(\eta)}(x)=\frac{d^{\eta}\mu(x)}{dx^{\eta}}$ be the $\eta^{th}$ 
derivative of the unknown regression function and define 
$\mu^{(\eta)}_{+}=lim_{x \rightarrow 0^{+}}\mu^{(\eta)}(x)$ and 
$\mu^{(\eta)}_{-}=lim_{x \rightarrow 0^{-}}\mu^{(\eta)}(x)$.
In most cases the population parameter of interest is 
$\tau=\E[Y(1)-Y(0) \mid X=\bar{x}]$ (i.e., the average treatment effect
at the threshold). Under continuity and smoothness conditions on both the 
conditional distribution of $X_i$ and the first moments of $Y(0)$ and $Y(1)$ at
the cutoff, $\tau$ is nonparametrically identifiable by
$\tau = \mu_{+}- \mu_{-}$
where $\mu_{+}=lim_{x \rightarrow 0^{+}}\mu(x)$ and
$\mu_{-}=lim_{x \rightarrow 0^{-}}\mu(x)$
(Hahn, Todd and Van der Klaauw, 2001)

Naturally, the estimation of $\tau$ in RD designs focuses on the 
problem of approximating $\E[Y(1) \mid X=x]$ and $\E[Y(0) \mid X=x]$
near the cutoff. Due to its desirable properties when estimating regression 
functions at boundary points, we consider the popular approach of fitting 
local linear regressions in neighborhoods on both
sides of the threshold.\footnote{See \cite{HTV2001}, \cite{Porter03} or
\cite{FanGijbels92} for discussions of the properties of local polynomial 
regressions for boundary problems.The results presented here are valid for
higher order polynomials and discontinuities at derivatives, like ``Kink RDD''. We focus on the linear case for ease in exposition.}
For the local linear model,
\begin{align*}
 \hat{\tau}(h_{n})&=\hat {\mu}_{+}(h_{n}) -\hat{\mu}_{-}(h_{n})\\
(\hat {\mu}_{+}(h_{n}),\hat {\mu}^{(1)}_{+}(h_{n}))'&= \argmin_{b_{0},b_{1}}
\sum_{i=1}^{N}\1\{h_n > X_{i} \geq 0\}(Y_{i}-b_{0}-X_{i}b_{1})^{2} \\
(\hat {\mu}_{-}(h_{n}),\hat {\mu}^{(1)}_{-}(h_{n}))' &= \argmin_{b_{0},b_{1}}
\sum_{i=1}^{N}\1\{0 > X_{i} > -h_n \}(Y_{i}-b_{0}-X_{i}b_{1})^{2}
\end{align*}

As shown by \cite{HTV2001}, \cite{Porter03} and expanded by CCT identification and inference procedures can be developed based on the following assumptions.\footnote{The assumptions below are the same as presented by CCT.}

\begin{assumption}[Behavior of the DGP near the cutoff]\label{A1}
  The random variables $Y_{i}, X_{i}$ are i.i.d.\ and
  there exists a positive number $\kappa_0$ such that the following
  conditions hold for all $x$ in the neighborhood
  $(-\kappa_{0},\kappa_{0})$ around zero:
  \begin{enumerate}
  \item $\E[Y_{i}^{4} \mid X_{i}=x]$ is bounded and the density of $X$
    is continuous and bounded away from zero.
  \item $\mu_{+}(x) = \E[ Y_{i}(1) \mid X_{i}=x ]$ and
    $\mu_{-}(x) = \E[ Y_{i}(0) \mid X_{i}=x ]$ are $p+2$ times
    continuously differentiable.
  \item $\sigma^{2}_{+}(x)=\V[ Y_{i}(1) \mid X_{i}=x ]$ and
    $\sigma^{2}_{-}(x)=\V[ Y_{i}(0) \mid X_{i}=x ]$ are continuous and
    bounded away from zero.
 \end{enumerate}
\end{assumption}

The conditions in Assumption \ref{A1} are the usual smoothness and existence conditions in which the RD literature relies. Its second part is important for the characterization of the leading bias term that will be the focus of the bias correction procedures described in CCT and in the novel  bootstrap procedure proposed in Section \ref{boot}.

\begin{assumption}[Bandwidth]\label{A2}
  Let $h_{n}$ be the bandwidth used to estimate the local linear model and let
  $b_{n}$ be the bandwidth used to estimate a local quadratic model. Then
  $n h_{n} \to \infty$, $n b_{n} \to \infty$, $n h_{n}^{5} b_{n}^{2} \to 0$, and
  $n b_{n}^{5} h_{n}^{2} \to 0$ as $n \to \infty$.\footnote{%
    Unless otherwise stated, all limits in this paper are assumed to hold as
    $n \to \infty$.} %
  The relationship $h_{n} \leq b_{n}$ also holds for all $n$.
\end{assumption}

CCT point out that the conventional approaches to construct confidence intervals for $\tau$ using the local linear estimator rely on a large-sample approximation for the standardized $t$-statistic that is valid only if the bandwidth shrink fast enough to eliminate the leading bias term contribution to the approximation. Hence, if $nh_{n}^{5}\rightarrow0$ and $nh_{n}\rightarrow \infty$, then
 \begin{align}
  T(h_{n}) &=\frac{\hat{\tau}(h_{n})-\tau}{\sqrt{V(h_{n})}}\rightarrow_{d}N(0,1), &
  V(h_{n}) &=\V[\tau(h_{n}) \mid X_{1},\dots,X_{n}]
 \end{align}
 The conditional variance $V(h_{n})$ depends on the $\sigma^{2}_{+}=lim_{x \rightarrow 0^{+}}\sigma^{2}_{+}(x)$ and $\sigma^{2}_{-}=lim_{x \rightarrow 0^{-}}\sigma^{2}_{-}(x)$ as well as $h_{n}$ and known quantities that depend on the kernel and order of polynomial used in the estimation. Under these conditions we could use a conventional confidence interval for $\tau$ given by
 \begin{align}
  I(h_{n})=\left[\hat{\tau}(h_{n})\pm \Phi^{-1}_{1-\frac{\alpha}{2}}\sqrt{V(h_{n})} \right]
 \end{align}
with $\Phi^{-1}_{1-\frac{\alpha}{2}}$ being the $\alpha$-quantile of the standard normal distribution. However, most approaches to select the bandwidth, $h_{n}$ in the literature, including the widely used optimal-AMSE bandwidth selector proposed by \cite{IK} lead to bandwidth choices that are ``too large'' since they do not satisfy the condition $nh_{n}^{5}\rightarrow0$, leading to a first-order bias in the distributional approximation used to construct the confidence intervals.

CCT solve the problem by deriving the analytical form of the first-order bias and correcting it directly. To obtain correct coverage the confidence interval needs not only to be re-centered to correct the bias, but also rescaled to allow for the additional variability introduced by the bias correction. Their approach relies on obtaining a bias-corrected  estimator, $\hat{\tau}^{bc}$ and its adjusted variance. It is important first to note that the first-order bias term itself will depend on the bandwidth $h_{n}$. The approximate bias term can be described as:
\begin{align*}
     \E[\hat{\tau}(h_{n}) \mid \chi_{n}]-\tau=& h_{n}^{2}\Bs(h_{n})\{1+o_{p}(1)\}\\
     \Bs(h_{n})=&\frac{\mu_{+}^{(2)}}{2!}\Bf_{+}(h_{n})-\frac{\mu_{-}^{(2)}}{2!}\Bf_{-}(h_{n})
   \end{align*}
where $\Bf_{+}(h_{n})$ and $\Bf_{-}(h_{n})$ are observed quantities that depend on the data, kernel and $h_{n}$. The plug-in bias-corrected estimator then requires estimates for the second derivatives of the conditional mean from above and below the cutoff, $\mu_{+}^{(2)}$ and $\mu_{-}^{(2)}$. CCT propose using a conventional local quadratic estimator, i.e. one order higher than the polynomial used to obtain $\hat{\tau}$,  using a (potentially) different pilot bandwidth $b_{n}$.
\begin{align*}
     \hat{\tau}^{bc}(h_{n}, b_{n})=& \hat{\tau}-h_{n}^{2}\hat{\Bs}(h_{n},b_{n})\\
     \hat{\Bs}(h_{n})=&\frac{\hat{\mu}_{+}^{(2)}(b_{n})}{2!}\Bf_{+}(h_{n})-\frac{\hat{\mu}_{-}^{(2)}(b_{n})}{2!}\Bf_{-}(h_{n})
   \end{align*}
By incorporating the contribution of both $\hat{\tau}(h_{n})$ and $\hat{\Bs}(h_{n},b_{n})$ to the asymptotic distribution of the estimator, CCT obtain a robust confidence interval with a different asymptotic variance in general. So under Assumptions \ref{A1} and \ref{A2}, if $n\min\{h_{n}^{5}, b_{n}^{5}\}max\{h_{n}^{2}, b_{n}^{2}\}\rightarrow 0$ and $n\min\{h_{n}, b_{n}\}\rightarrow \infty$ and $\kappa \max\{h_{n},b_{n}\}< \kappa_{0}$, they show that
\begin{align}
  T^{rbc}(h_{n}, b_{n})=\frac{\hat{\tau}^{bc}(h_{n}, b_{n})-\tau}{\sqrt{V^{bc}(h_{n}, b_{n})}}\rightarrow_{d}N(0,1), \text{        }V^{bc}(h_{n}, b_{n})=V(h_{n})+C^{bc}(h_{n}, b_{n})\\
 \end{align}
where the additional term $C^{bc}(h_{n}, b_{n})$ depends on the (asymptotic) variability of the bias estimate used for correction as well as its correlation with the original RD estimator $\hat{\tau}(h_{n})$.\footnote{The details of the formulas are currently ommitted from this paper since their form is not relevant to the bootstrap procedure innovation proposed. The details can be found at CCT Appendix.}  Under these conditions we can construct a valid confidence interval for $\tau$ given by
 \begin{align}
  I^{rbc}(h_{n}, b_{n})=\left[\left(\hat{\tau}(h_{n})-h_{n}^{2}\hat{\Bs}(h_{n},b_{n})\right)\pm \Phi^{-1}_{1-\frac{\alpha}{2}}\sqrt{V(h_{n})+C^{bc}(h_{n}, b_{n})} \right]
 \end{align}

This approach is shown by the authors to significantly improve coverage of the CIs constructed and provides practitioners with a new tool set to perform inference that is more robust to the choice of bandwidth. We build upon the insight provided by CCT bias-corrected estimator and propose a simple bootstrap procedure that can directly construct the robust CIs without requiring the derivation of analytical formulas and direct estimators for the bias, variance and covariance terms, while relying on the same first-order bias correction approximation.

\section{Bootstrap Bias Correction}\label{boot}

Building upon the results and intuition developed by CCT we propose to implement a first-order bias correction and obtain valid CIs through a bootstrap procedure.
Intuitively, for the local-linear RD estimator's case, we will rely on a local quadratic polynomial approximation for the conditional outcome around the cutoff to capture the potential bias present in the original RD estimate. This insight comes directly from CCT's results using the same local quadratic approximation to estimate bias term as presented in Section \ref{background}.

The first algorithm describes how to obtain a point estimate and perform the bias correction. The second algorithm obtains its confidence interval with correct coverage. For simplicity, we adopt the same optimal-AMSE bandwidth selector proposed by CCT.\footnote{An interesting question is whether one could use a bootstrap based procedure to select a bandwidth that would achieve similar (or improved) results. This is beyond the scope of this particular paper but is part of ongoing research effort.} 

Essentially, we apply CCT's insight that a polynomial one order higher can capture the first-order bias behavior and create a bootstrap procedure that can correctly mimic the behavior of the bias present in the original estimator. For such, for a chosen bandwidth, we estimate a local quadratic regression as suggested by CCT, but then bootstrap data by imposing the local quadratic functional form as the DGP. Then, in the bootstrap data the DGP is known. Let the ``true'' treatment effect under the bootstrap DGP be denoted by $\tau^{*}$. We then estimate the treatment effect using the (misspecified) local linear estimator, $\hat{\tau}^{*}$ to the generated data. Hence, the bias for a specific bootstrap data is given by $\tau^{*}-\hat{\tau}^{*}$. More importantly, by generating new bootstrapped data based on the same quadratic DGP and its residual's distribution, we can obtain an estimate of the first-order bias (the part that is captured by $\hat{\Bs}(h_{n},b_{n})$ in CCT's formulas on Section \ref{background}) in the original estimate by averaging across the bias in all $B$ bootstrapped data estimates, i.e., $\tau^{*} - \sum_{i = 1}^{B} \hat{\tau}_{ i}^{*}$. Algorithm~\ref{Alg1} describes this procedure step-by-step for clarity.

\begin{algorithm}[Bias estimation]\label{Alg1}
  Assume $h_{n}$ and $b_{n}$ are bandwidths as described by Assumption~\ref{A2} and define
  \begin{align}
    I^{-}(h) &= \{i : h < X_{i} < 0\}, &
    I^{+}(h) &= \{i : 0 < X_{i} < h\}.
  \end{align}
  Also define $M^{-}(h_{n})$ and $M^{+}(h_{n})$ to be the number of elements in
  $I^{-}(h_{n})$ and $I^{+}(h_{n})$ respectively.
  \begin{enumerate}
  \item Estimate the local polynomials $g_{p+1}^{+}$ and $g_{p+1}^{-}$ as defined by
    Equation --- and define the residuals
    \begin{equation}
      \label{eq:1}
      \hat\varepsilon_{i} =
      \begin{cases}
        Y_{i} - \hat g_{p+1}^-(X_{i}) & \textif\ X_{i} < 0 \\
        Y_{i} - \hat g_{p+1}^+(X_{i}) & \otherwise
      \end{cases}
    \end{equation}
  \item Repeat the following steps $B_1$ times to produce a sequence of bootstrap
    estimates $\hat\tau_1^*,\dots,\hat\tau_{B_1}^*$. For the $k$th step:
    \begin{enumerate}
    \item Draw an i.i.d. sample of size $M^-(b_n)$ from
      $\{\hat\varepsilon_i : i \in I^-(b_n)\}$ and one of size $M^+(b_n)$ from
      $\{\hat\varepsilon_i : i \in I^+(h_n)\}$. Let $\varepsilon_i^{*-}$ and
      $\varepsilon_i^{*+}$ denote the $i$th element of each sample and construct
      \begin{align}
        Y_i^{*-} &= \hat g_{p+1}^-(X_i^{*-}) + \varepsilon_i^{*-} &
        Y_i^{*+} &= \hat g_{p+1}^+(X_i^{*+}) + \varepsilon_i^{*+}
      \end{align}
    \item Calculate $\hat\mu_+^*$ and $\hat\mu_-^*$ as
      \begin{align}
        \hat\mu_-^*
        &= \argmin_{\beta_0} \min_{\beta_1,\dots,\beta_p}
          \sum_{i = 1}^{M^-(h_n)} (Y_i^{*-} -
          (\beta_0 + \beta_1 X_i^{*-} + \cdots + \beta_p (X_i^{*-})^p))^2 \\
        \hat\mu_+^*
        &= \argmin_{\beta_0} \min_{\beta_1,\dots,\beta_p}
          \sum_{i = 1}^{M^+(h_n)} (Y_i^{*+} -
          (\beta_0 + \beta_1 X_i^{*+} + \cdots + \beta_p (X_i^{*+})^p))^2
      \end{align}
    \item Save $\hat\mu_+^* - \hat\mu_-^*$ as $\hat\tau^*_k$.
    \end{enumerate}
  \item Estimate the bias as
    \begin{equation}
      \label{eq:2}
      \Delta^* = \tfrac{1}{B_1} \sum_{k=1}^{B_1} \hat\tau^*_k -
      \big[\hat g_{p+1}^+(0) - \hat g_{p+1}^-(0)\big]
    \end{equation}
  \end{enumerate}
\end{algorithm}
The distribution of $\Delta^{*}$ depends on $B_{1}$ and we could
incorporate that dependency into our notation by writing $\Delta^*_{B_1}$. However, we will follow standard practice in the bootstrap
literature and assume that $B_{1}$ is large enough that it can be ignored and define $\Delta^* = \plim_{B_{1} \to \infty} \Delta^*_{B_{1}}$

Under Assumptions~\ref{A1} and~\ref{A2} the procedure described by
Algorithm~\ref{Alg1} provides a consistent estimator of the bias component that
converges fast enough in probability that it can be be used as a correction. As is standard in the bootstrap literature, we will assume
that the number of bootstrap replications, $B_{1}$, is large enough that the simulation error can be ignored.

\begin{theorem}\label{T1}
  Under Assumptions~\ref{A1} and~\ref{A2},
\begin{align}
  \frac{(\hat\tau(h_{n},b_{n}) - \Delta^{*}(h_{n},b_{n}) - \tau)}{ V^{1/2}(h_n, b_n)} \to^{d} N(0,1)
\end{align}
\end{theorem}

With Algorithm~\ref{Alg1} at hand, we can move to generate the bias-corrected confidence intervals to perform inference about $\tau$. Here the idea is similar, but now we actually calculate a bias corrected estimate using Algorithm~\ref{Alg1} for every bootstrapped dataset generated, a bootstrap within the bootstrap. Each one of the bias corrected estimates for the generated data is denoted $\hat{\tau}^{bc*}$. These estimator's empirical distribution capture the variability induced by the first order bias correction that lead CCT to propose the rescaling of their confidence intervals by introducing $C^{bc}(h_{n},b_{n})$ in the asymptotic variance formula on Section \ref{background}.

\begin{algorithm}[Confidence intervals]\label{Alg2}
  Define $I^{-}$, $I^{+}$, $M^{-}$, and $M^{+}$ as in Algorithm~\ref{Alg1}.
  \begin{enumerate}
  \item Estimate the local polynomials $g_{p+1}^{+}$ and $g_{p+1}^{-}$ as defined by
    Equation --- and define the residuals
    \begin{equation}
      \label{eq:3}
      \hat\varepsilon_{i} =
      \begin{cases}
        Y_i - \hat g_{p+1}^-(X_{i}) & \textif\ X_{i} < 0 \\
        Y_i - \hat g_{p+1}^+(X_{i}) & \otherwise
      \end{cases}
    \end{equation}
  \item Repeat the following steps $B_{2}$ times to produce a sequence of bootstrap
    estimates $\hat\tau_1^{\prime*},\dots,\hat\tau_{B_{2}}^{\prime*}$. For the $k$th step:
    \begin{enumerate}
    \item Draw an i.i.d. sample of size $M^{-}(b_{n})$ from
      $\{\hat\varepsilon_{i} : i \in I^{-}(b_{n})\}$ and one of size $M^{+}(b_{n})$ from
      $\{\hat\varepsilon_i : i \in I^{+}(h_{n})\}$. Let $\varepsilon_{i}^{*-}$ and
      $\varepsilon_i^{*+}$ denote the $i$th element of each sample and construct
      \begin{align}
        Y_i^{*-} &= \hat g_{p+1}^-(X_i^{*-}) + \varepsilon_i^{*-} &
        Y_i^{*+} &= \hat g_{p+1}^+(X_i^{*+}) + \varepsilon_i^{*+}
      \end{align}
    \item Calculate $\hat\mu_+^*$ and $\hat\mu_-^*$ as
      \begin{align}
        \hat\mu_-^*
        &= \argmin_{\beta_0} \min_{\beta_1,\dots,\beta_p}
          \sum_{i = 1}^{M^-(h_n)} (Y_i^{*-} -
          (\beta_0 + \beta_1 X_i^{*-} + \cdots + \beta_p (X_i^{*-})^p))^2 \\
        \hat\mu_+^*
        &= \argmin_{\beta_0} \min_{\beta_1,\dots,\beta_p}
          \sum_{i = 1}^{M^+(h_n)} (Y_i^{*+} -
          (\beta_0 + \beta_1 X_i^{*+} + \cdots + \beta_p (X_i^{*+})^p))^2
      \end{align}
    \item Apply Algorithm~\ref{Alg1} to the bootstrapped data set,
      \begin{equation*}
        (Y_1^{*-}, X_1^{*-}),\dots,(Y_{M^-(b_n)}^{*-},X_{M^-(b_n)}^{*-}),
        (Y_1^{*+}, X_1^{*+}),\dots,(Y_{M^+(b_n)}^{*+},X_{M^+(b_n)}^{*+}),
      \end{equation*}
      using the same bandwidths $h_n$ and $b_n$ that are used in the rest of
      this algorithm but reestimating all of the local polynomials on the
      bootstrap data. Let $\Delta^{**}$ represent the bias estimator returned
      by Algorithm~\ref{Alg1}.
    \item Save the bias-corrected estimator
      $\hat\tau_k^{\prime*} = \hat\mu_+^* - \hat\mu_i^* - \Delta^{**}$.
    \end{enumerate}
  \item Define the confidence interval endpoints $\hat\ell(\alpha)$ and
    $\hat u(\alpha)$ as the $\alpha/2$ and $1-\alpha/2$ quantiles of the
    empirical CDF of $\hat\tau_1^{\prime*},\dots,\hat\tau_{B_2}^{\prime*}$.
  \end{enumerate}
\end{algorithm}

As earlier, we assume that $B_1$ and $B_2$ are large enough that simulation
error can be ignored. Define the limiting values
\begin{align*}
\Delta^{**} &= \plim_{B_1,B_2 \to \infty} \Delta^{**}_{B_1,B_2} \\
\hat\ell^*(\alpha) &= \plim_{B_1,B_2 \to \infty} \hat\ell^*_{B_1,B_2}(\alpha) \\
\hat u^*(\alpha) &= \plim_{B_1,B_2 \to \infty} \hat u^*_{B_1,B_2}(\alpha).
\end{align*}

Theorem~\ref{T2} establishes that this ``bootstrap-within-bootstrap''
approximates the asymptotic distribution of the bias-corrected
statistic proposed by Algorithm~\ref{Alg1} and justifies this second
algorithm.

\begin{theorem}\label{T2}
  Under Assumptions~\ref{A1} and~\ref{A2},
  \begin{gather}
    \label{eq:4}
    \V^*(\hat\tau^{*} - \Delta^{**})/V(h_n,b_n) \to^p 1
  \intertext{and}
  \label{eq:5}
    \sup_{x}
    \Big\rvert \Pr^*[\hat\tau^{*} - \Delta^{**} - \tau^* \leq x ]
    - \Pr[\hat\tau - \Delta^* - \tau \leq x] \Big\lvert \to^p 0.
  \end{gather}
\end{theorem}

Evidence of the usefulness of the procedures proposed above and their relative performance to the analytical bias correction proposed in CCT are presented in a series of Monte Carlo simulations in Section \ref{sim}.

\section{Simulation Evidence}\label{sim}

This section presents evidence from Monte Carlo simulations that the bootstrap bias correction procedures proposed in Section \ref{boot} produces valid, robust confidence intervals similar to the ones obtained by the analytical procedures established in CCT. The bootstrap CIs obtained compare favorably to the analytical alternative, with coverage closer to the nominal size of the desired test and shorter length of the intervals in the specifications implemented.

We adopt the same three data generating processes (DGP) used by CCT to make it easy to compare our results. Also, these DGPs are widely used in the RD literature to evaluate the performance of inference procedures and provide a good testing ground for bias correction due to their exacerbated curvature.

For all Monte Carlo experiments, the pseudo data are generated as 500 i.i.d. draws from:
\begin{align*}
& Y_{i} = \mu_{j}(X_{i}) + \epsilon_{i}, \;\;\; X_{i} \sim  (2 B (2,4) - 1)), \\
& \epsilon_{i} \sim N(0, \sigma^{2}), \;\;\; \sigma = 0.1295, \;\;\; j = 1,2,3.
\end{align*}
Regarding the functional form for $\mu(x)$ in each DGP, the first one is based on a fifth order polynomial fitted on the data observed by \cite{lee2008rand} when estimating the incumbency advantage in electoral races to the U.S. House of Representatives. Hence, $\mu_{1}(x)$ is given by,
\begin{align}
\mu_{1}(x) & = 
\begin{cases}
0.48 + 1.27x + 7.18x^{2} + 20.21x^{3} + 21.54x^{4} + 7.33x^{5}, \;\;\;\; & \text{if} \;\; x < 0, \\
0.52 + 0.84x - 3.00x^{2} + 7.99x^3 - 9.01x^4 + 3.56x^{5},  & \text{if} \;\; x \ge 0.
\end{cases}
\end{align}

The functional form for $\mu_{2}(x)$, in the second DGP follows the data observed by \cite{ludwig2007}, which we examine in more detail on Section \ref{application}, and follows
\begin{align}
\mu_{2}(x) & = 
\begin{cases}
3.71 + 2.30x + 3.28x^2 + 1.45x^3 + 0.23x^4 + 0.03x^5, \;\; & \text{if} \;\; x < 0, \\
0.26 + 18.49x - 54.81x^2 + 74.30x^3 - 45.02x^4 + 9.83x^5,  & \text{if} \;\; x \ge 0.
\end{cases}
\end{align}

Finally, the third DGP used in the simulations is given by a modified version of  \cite{lee2008rand} designed to exhibit more curvature and, hence, be more sensitive to the choice of bandwidth in terms of bias. This functional form is the same as used in CCT and serves as an useful example of the effectiveness of the bias correction procedures implemented.
\begin{align}
\mu_{3}(x) & =
\begin{cases}
0.48 + 1.27x + 0.5 \times 7.18x^{2}+ \\
+ 0.7 \times 20.21x^3 + 1.1 \times 21.54x^4 + 1.5 \times 7.33x^5, \;\;\;\;\;\;\;\;\;\;\;\; & \text{if} \;\; x < 0, \\
0.52 + 0.84x - 0.1 \times 3.00x^{2}+ \\
+ 0.3 \times 7.99x^3 - 0.1 \times 9.01x^4 + 3.56x^5, & \text{if} \;\; x \ge 0.
\end{cases}
\end{align}

For each DGP, we used four methods to obtain point estimate and confidence intervals.\footnote{Note that the true value of $\tau$ is $0.04$ for DGPs 1 and 3 and $-3.45$ for DGP 2} For each method, a total 1500 replications were performed. The first two methods follow CCT analytical bias-correction procedure with triangular ($CCT_{tri}$) and uniform ($CCT_{uni}$) kernel, respectively. The third and fourth methods follow the bootstrap procedure proposed in Section \ref{boot} with uniform kernel with ($bootstrap_{uni}$) and without ($bootstrap_{uni}^{naive}$) bias-correction, respectively.

The ``naive'' bootstrap estimator, which bootstraps the confidence intervals for the usual RD estimator without any bias-correction serves as a baseline comparison, making clear the need of the corrections proposed, especially for DGPs with higher (potential) bias.

In the bootstrap procedure, we used 500 bootstraps for bias correction, 999 bootstraps to calculate confidence interval. Table \ref{Tb: simulation 1} presents the main results in which a local-linear RD estimator ($p=1$) is employed with the bias-correction based on a local-quadratic approximation ($q=2$).

\begin{table}[t]
\centering
\begin{tabular}{rrrrrrrrr}
  \toprule
	DGP & Method    & Bias    & SD    & MSE   & CI Coverage & CI Length \\
  \midrule
	1 & $CCT_{tri}$ & -0.010 & 0.068 & 0.068 & 0.911 & 0.241 \\ 
	  & $CCT_{uni}$ & -0.016 & 0.067 & 0.069 & 0.924 & 0.246 \\ 
	  & $bootstrap_{uni}$ & -0.018 & 0.066 & 0.069 & 0.931 & 0.240 \\ 
	  & $bootstrap_{uni}^{naive}$ & -0.020 & 0.060 & 0.064 & 0.916 & 0.208 \\
	\midrule
	2 & $CCT_{tri}$ & -0.011 & 0.085 & 0.086 & 0.935 & 0.343 \\ 
	  & $CCT_{uni}$ & -0.010 & 0.087 & 0.088 & 0.938 & 0.350 \\ 
	  & $bootstrap_{uni}$       & -0.012 & 0.088 & 0.089 & 0.953 & 0.324 \\ 
	  & $bootstrap_{uni}^{naive}$ & -0.051 & 0.086 & 0.100 & 0.869 & 0.300 \\ 
	\midrule
	3 & $CCT_{tri}$ & -0.008 & 0.067 & 0.067 & 0.919 & 0.246 \\ 
	  & $CCT_{uni}$ & -0.004 & 0.065 & 0.065 & 0.943 & 0.250 \\ 
	  & $bootstrap_{uni}$       & -0.004 & 0.064 & 0.064 & 0.960 & 0.246 \\ 
	  & $bootstrap_{uni}^{naive}$ & 0.016 & 0.062 & 0.064 & 0.931 & 0.218 \\ 
  \bottomrule
\end{tabular}
\caption{Summary of estimates ($p = 1, q = 2$)}
\label{Tb: simulation 1}
\end{table}

Table \ref{Tb: simulation 1} presents the results for the local linear model, with three panels reporting on each DGP described above. For each panel, we report bias, standard deviation, MSE, confidence interval empirical coverage and lenght for the four methods. For the three DGPs coverage for
    both analytical and bootstrap based bias correction methods is near nominal size but with slight undercoverage for CCT's analytical correction. For DGP 1, our robust bootstrap method improves coverage slightly, obtaining 93.1\% coverage against 92.4\% from CCT's analytical correction using the uniform kernel. Also, the CI length is slightly shortened on average. Both robust approaches improve over the naive estimator, even in this case for which bias is relatively small.

For the second DGP, based on \cite{ludwig2007}, the importance of correcting for the leading bias term presence becomes clear, with the naive bootstrap CI displaying an empirical coverage of only 86.9\%. Once again, the CIs based on both analytical and bootstrap robust methods improves the coverage meaningfully, with moderate gains in terms of coverage and lenght from $0.324$ to $0.350$ when using the bootstrap estimator relative to CCT's with uniform kernel.

In the case of the modified \cite{lee2008rand} DGP that exacerbates the importance of bias, $\mu_{3}(x)$, once more the robust bootstrap procedure provides similar empirical coverage and interval length relative to its analytical alternatives. The naive method once again suffers in terms of coverage due to the presence of uncorrected first order bias.

 Overall, the bootstrap bias-correction procedure proposed in this paper provides a simple alternative to obtain valid robust confidence intervals in RD designs and performs well compared to the analytical
bias correction procedures proposed by CCT. The results also corroborates CCT in the importance of bias correction when high curvature is present at the cutoff. 

\section{Application}\label{application}

In this section, we apply the bootstrap procedure to the data\footnote{The data is publicly available from http://faculty.econ.ucdavis.edu/faculty/dlmiller/statafiles/.} used in \cite{ludwig2007}. In their paper, the effects of Head Start application assistance on federal spending, mortality and education attainment were investigated under standard RD design. They utilized the fact that grant-writing assistance were offered only to the poorest 300 counties to identify the average treatment effect (ATE) at the cutoff.

We estimate the ATE with alternative procedures and compare the new estimates with results in \cite{ludwig2007}. Bias-corrected point estimates and robust confidence intervals are reported. For completeness, we also include results following the procedure proposed by CCT. In \cite{ludwig2007}, analytical standard error and percentile-T bootstrapped p-value were reported\footnote{However, we were unable to reproduce their point estimates for the ATE under the nonparametric setting using triangular kernel, which was documented in their paper. Instead, we used uniform kernel and obtained exactely the same results as theirs.}. The procedure proposed by CCT provides analytical standard error and confidence interval. Our bootstrap procedure provides standard error and confidence interval from the bootstrap distribution. In both CCT and bootstrap procedures, we set $p = 1$, $q = 2$ and used uniform kernel. In the bootstrap procedure, we used 500 bootstraps for bias correction and 999 for confidence interval.

These results are shown in Table \ref{tab: head start spending}-\ref{tab: education 2000 ages 25-34}. We find that (1) the choice of bandwidth may significantly affect estimated ATE and its significance level; (2) the optimal bandwidths following CCT procedure are much smaller than actually being used in \cite{ludwig2007} and (3) our bootstrap procedure produces similar results to CCT.

\begin{table}[ht]
	\centering
	\begin{tabular}{cccccc}
		\toprule
		& \multicolumn{3}{c}{LM 2007} & CCT 2014 & Bootstrap \\
		\midrule
		& \multicolumn{5}{c}{Panel A: 1968 Head Start spending per child} \\ 
		Bandwidth & 9 & 18 & 36 & 5.414, 10.175 & 5.414, 10.175 \\
		Effect    & 137.251 & 114.711 & 134.491 & 143.811 & 148.990 \\
		SE        & 128.968 & 91.267  & 62.593 & 113.404 & 182.729 \\
		p-value   & 0.157   & 0.138   & 0.045  & & \\
		95\% CI   & & & & (-78.457 366.078) & (-179.120  547.755) \\
		
		&&&&& \\
		& \multicolumn{5}{c}{Panel B: 1972 Head Start spending per child} \\ 
		Bandwidth & 9 & 18 & 36 & 5.034, 12.020 & 5.034, 12.020 \\
		Effect    & 182.119 & 88.959 & 130.153 & 94.260 & 100.404 \\
		SE        & 148.321 & 101.697 & 67.613 & 122.224 & 196.008 \\
		p-value   & 0.085   & 0.352   & 0.090  & & \\
		95\% CI   & & & & (-145.294 333.814) & (-165.816 622.976) \\
		
		&&&&& \\
		& \multicolumn{5}{c}{Panel C: 1972 other social spending per capita} \\ 
		Bandwidth & 9 & 18 & 36 & 4.612, 8.461 & 4.612, 8.461 \\
		Effect    & 14.474 & 19.590 & 14.506 & 15.418 & 15.857 \\
		SE        & 28.356 & 19.612 & 14.929 & 29.041 & 39.902 \\
		p-value   & 0.459   & 0.222   & 0.478  & & \\
		95\% CI   & & & & (-41.501 72.337) & (-64.153 91.999) \\
		\bottomrule
	\end{tabular}
	\caption{The effect of Head Start assistance on Head Start spending. The first three columns come from Table 2 in \cite{ludwig2007}.}
	\label{tab: head start spending}
\end{table}

\begin{table}[ht]
	\centering
	\begin{tabular}{cccccc}
		\toprule
		& \multicolumn{3}{c}{LM 2007} & CCT 2014 & Bootstrap \\
		\midrule
		Bandwidth & 9 & 18 & 36 & 3.888, 6.807 & 3.888, 6.807 \\
		Effect    & -1.895 & -1.198 & -1.114 & -3.795 & -3.852 \\
		SE        & 0.980 & 0.796  & 0.544 & 1.654 & 1.526 \\
		p-value   & 0.036   & 0.081   & 0.027  & & \\
		95\% CI   & & & & (-7.037 -0.554) & (-6.565 -0.578) \\
		\bottomrule
	\end{tabular}
	\caption{The effect of Head Start assistance on mortality. The first three columns come from Table 3 in \cite{ludwig2007}.}
	\label{tab: mortality}
\end{table}


\begin{table}[ht]
	\centering
	\begin{tabular}{cccc}
		\toprule
		& LM 2007 & CCT 2014 & Bootstrap \\
		\midrule
		& \multicolumn{3}{c}{Panel A: Fraction high school or more} \\
		Bandwidth & 7 & 3.671, 8.618 & 3.671, 8.618 \\
		Effect    & 0.030 & 0.055 & 0.054 \\
		SE        & 0.016 & 0.021 & 0.021 \\
		p-value   & 0.032 & & \\
		95\% CI        & & (0.014 0.096) & (0.010 0.094) \\
		
		&&& \\
		& \multicolumn{3}{c}{Panel B: Fraction some college or more} \\
		Bandwidth & 7 & 5.076, 10.251 & 5.076, 10.251 \\
		Effect    & 0.037 & 0.051 & 0.051 \\
		SE        & 0.020 & 0.024 & 0.024 \\
		p-value   & 0.031 & & \\
		95\% CI        & & (0.004 0.099) & (0.001 0.092) \\
		\bottomrule
	\end{tabular}
	\caption{The effect of Head Start assistance on education (1990, ages 18-24). The first column is result for directly treated cohorts (1990, ages 18-24) in Table 4 in \cite{ludwig2007}.}
	\label{tab: education 1990 ages 18-24}
\end{table}

\begin{table}[ht]
	\centering
	\begin{tabular}{cccc}
		\toprule
		& LM 2007 & CCT 2014 & Bootstrap \\
		\midrule
		& \multicolumn{3}{c}{Panel A: Fraction high school or more} \\ 
		Bandwidth & 7 & 4.837, 9.85 & 4.837, 9.85 \\
		Effect    & 0.000 & 0.014 & 0.013 \\
		SE        & 0.016 & 0.019 & 0.019 \\
		p-value   & 0.974 & & \\
		CI        & & (-0.024 0.051) & (-0.026 0.051) \\
		
		&&& \\
		& \multicolumn{3}{c}{Panel B: Fraction some college or more} \\
		Bandwidth & 7 & 5.382, 10.621 & 5.382, 10.621 \\
		Effect    & 0.028 & 0.033 & 0.032 \\
		SE        & 0.019 & 0.023 & 0.022 \\
		p-value   & 0.017 & & \\
		CI        & & (-0.014 0.079) & (-0.009 0.077)\\
		\bottomrule
	\end{tabular}
	\caption{The effect of Head Start assistance on education (2000, ages 18-24). The first column is result for directly treated cohorts (2000, ages 18-24) in Table 4 in \cite{ludwig2007}.}
	\label{tab: education 2000 ages 18-24}
\end{table}

\begin{table}[ht]
	\centering
	\begin{tabular}{cccc}
		\toprule
		& LM 2007 & CCT 2014 & Bootstrap \\
		\midrule
		& \multicolumn{3}{c}{Panel A: Fraction high school or more} \\ 
		Bandwidth & 7 & 6.074, 11.477 & 6.074, 11.477 \\
		Effect    & 0.006 & 0.014 & 0.014 \\
		SE        & 0.014 & 0.015 & 0.015 \\
		p-value   & 0.666 & & \\
		CI        & & (-0.014 0.043) & (-0.023 0.036) \\
		
		&&& \\
		& \multicolumn{3}{c}{Panel B: Fraction some college or more} \\
		Bandwidth & 7 & 5.509, 11.463 & 5.509, 11.463 \\
		Effect    & 0.040 & 0.049 & 0.049 \\
		SE        & 0.017 & 0.020 & 0.021 \\ 
		p-value   & 0.009 & & \\
		CI        & & (0.010 0.089) & (0.003 0.083) \\
		\bottomrule
	\end{tabular}
	\caption{The effect of Head Start assistance on education (2000, ages 25-34). The first column is result for directly treated cohorts (2000, ages 25-34) in Table 4 in \cite{ludwig2007}.}
	\label{tab: education 2000 ages 25-34}
\end{table}

\section{Conclusion}\label{conclusion}
This paper proposes a novel bootstrap procedure to obtain robust bias-corrected
confidence intervals in regression discontinuity designs. 
The approach proposed builds upon the developments and intuition advanced by CCT and is based on a first-order bias correction.
Our procedure provides an alternative to the plug-in analytical methods in the literature and is simple to implement generating robust confidence intervals.
Simulation evidence is presented that the proposed bootstrap bias correction and confidence intervals have improved coverage and shorter length relative to the analytical alternatives proposed by CCT.

\newpage\appendix
\section{Mathematical Appendix}
\subsection{Proof of Theorem~\ref{T1}}
We have
\begin{equation*}
  \hat\tau - \Delta^* - \tau = (\hat\tau - \E \hat \tau) +
  (\E \hat \tau - \tau) - (\E^* \hat \tau^* - \tau^*)
\end{equation*}
The design of the bootstrap ensures that
\begin{equation*}
  \label{eq:6}
  \E^* \hat\mu_{\pm}^*(h_n) - \mu_{\pm}^{*} =
  h_n^{2} \mu_\pm^{*(2)} \Bf_{\pm}(h_n)/2,
\end{equation*}
almost surely, implying that
\begin{equation*}
  \label{eq:7}
    \E^* \hat\tau^{*} - \tau^{*} = h_n^2\, \mu_+^{*(2)} \Bf(h_n)/2
      - h_n^2\, \mu_{-}^{*(2)} \Bf_{-}(h_n)/2.
\end{equation*}
Combining this result with CCT's Lemma A1, gives
\begin{align*}
  \hat\tau &- \E \hat\tau + (\E \hat\tau - \tau) - (\E^* \hat\tau^* - \tau^*)\\
  &= \hat\tau - \E\hat\tau
   + h_n^2 \big((\mu_{-}^{*(2)} - \mu_{-}^{(2)}) \Bf_{-}(h_{n}) /2
   - (\mu_+^{*(2)}-\mu_+^{(2)}) \Bf_{+}(h_{n}) / 2 \big) + O_p(h_n^{3}) \\
  &= \hat\tau - \E\hat\tau
   + h_n^2 (\hat\mu_-^{(2)}(b_n) - \mu_-^{(2)}) \Bf_{-}(h_{n}) /2
   - h_n^2 (\hat\mu_+^{(2)}(b_n) - \mu_+^{(2)}) \Bf_{+}(h_{n}) / 2 + O_p(h_n^{3})
\end{align*}
The second equality holds because
$\mu_\pm^{*(2)} = \hat\mu_{\pm}^{(2)}(b_n)$ a.s. Asymptotic normality then
follows from normality of $\hat\tau - \E\hat\tau$ and
$\hat\mu_\pm^{(2)}(b_n) - \mu_\pm^{(2)}$ using
similar arguments to CCT's Lemma SA4.D.\qed

\subsection{Proof of Theorem~\ref{T2}}
We can repeat the steps from Theorem~\ref{T1}'s proof to show that
\begin{align*}
  \hat\tau^* - \Delta^{**} - \tau^* &= (\hat\tau^* - \E^* \hat \tau^*) +
  (\E^* \hat \tau^* - \tau^*) - (\E^{**} \hat \tau^{**} - \tau^{**}) \\
  &= \hat\tau^* - \E^*\hat\tau^*
   + h_n^2 (\hat\mu_-^{*(2)}(b_n) - \mu_-^{*(2)}) \Bf_{-}(h_{n}) /2
   - h_n^2 (\hat\mu_+^{*(2)}(b_n) - \mu_+^{*(2)}) \Bf_{+}(h_{n}) / 2.
\end{align*}
Consequently, the asymptotic behavior of
$\hat\tau^* - \Delta^** - \tau^*$ is determined by the behavior of
$\hat\tau^* - \E^*\hat\tau*$ and
$\hat\mu_{\pm}^{*(2)} - \mu_\pm^{*(2)}$ and these random variables are
weighted averages of the sums
\begin{equation*}
  \sum_{i \in I^{\pm}(b_n)} Z_i \varepsilon_i^*
\end{equation*}
with $Z_i = (1, X_i, X_i^2)'$. By construction,
\begin{equation*}
  \V^*\Big(\ssum{i}{b_n} Z_i \varepsilon_i^* \Big)
  = \hat\sigma_\pm^2(b_n) \Gamma_\pm(b_n)
\end{equation*}
almost surely, making~\eqref{eq:4} a consequence of CCT's SA?.

For~\eqref{eq:5}, since $\hat\tau - \Delta^*$ is itself asymptotically
normal, it suffices to prove the pointwise result
\begin{equation}
  \label{eq:8}
  \Big\rvert \Pr^*[(\hat\tau^{*} - \Delta^{**} - \tau^*)
  / V^{1/2}(h_n,b_n) \leq x ] - \Phi(x)] \Big\lvert \to^p 0.
\end{equation}
for all $x$. Since the limiting distribution is continuous, an
argument attributed to Poly\'a immediately extends the pointwise
convergence result to the uniform result stated in~\eqref{eq:5}. (See
van der vaart, p. for the details of the argument.) But~\eqref{eq:8}
follows from Lemma~\ref{L1} as well, completing the proof.\qed

\subsection{Supporting results}

\begin{lemma}\label{L1}
  Define $Z_i = (1, X_i, X_i^2)$.  Under Assumptions~\ref{A1} and~\ref{A2},
  \begin{equation}
    \label{eq:9}
    \Big\rvert \Pr^*\Big[ \Gamma_\pm(b_n)^{-1/2}
    \sum_{i \in I^{\pm}(b_n)} Z_i \varepsilon_i^* / \hat\sigma_\pm(b_n) < x \Big]
    - \Phi(x) \Big\lvert \to^p 0.
  \end{equation}
\end{lemma}

\begin{proof}
  Let $z_i = \alpha' Z_i$ for an arbitrary nonzero $\alpha$ and define
  $\gamma_\pm(b_n) = \alpha'\Gamma_\pm(b_n)\alpha$. We will
  establsh asymptotic normality for the scalar $z_i \varepsilon_i^*$;
  the multivariate case follows via the Cram\'er-Wold device. (cite
  van der vaart.)

  Define the filtration
  \begin{equation}
    \label{eq:10}
    \Fc_j^{*\pm} = \sigma(Y_1,\dots,Y_n; X_1,\dots,X_n;
    \varepsilon_{i_1}^{*\pm},\dots,\varepsilon_{i_j}^{*\pm})
  \end{equation}
  where the values $i_1, i_2,\dots, i_{M_h^{\pm}}$ form an ordering of
  all of the elements of $I_n^\pm$. (The specific order is
  unimportant.) Then
  $\{z_{i_j} \varepsilon_{i_j}^{*\pm}, \Fc_j^{*\pm}\}$ is a martingale
  difference sequence and~\eqref{eq:9} holds if
  \begin{equation}
    \label{eq:11}
    \savg{i}{h} z_{i}^2 \varepsilon_{i}^{*\pm\,2} -
    \gamma_{\pm}^*(b_n) \hat\sigma
    \to^p 0
  \end{equation}
  and, for all
  $\delta > 0$,
  \begin{equation}
    \label{eq:12}
    \oavg{j}{h} z_{i_j}^2 \E^*( \varepsilon_{i_j}^{*\pm\,2}
    \1\{z_{i_j}^2 \varepsilon_{i_j}^{*\pm\,2} / M_h^\pm > \delta \}
    \mid \Fc_{j-1}^{\pm}) \to^p 0.
  \end{equation}
  (cite Hall and Heyde.)

  For~\eqref{eq:11}, observe that
  $\{z_{i_j}^2 (\varepsilon_{i_j}^{*2} - \hat\sigma^2_\pm(b_n)), \Fc_j^\pm\}$
  is also a martingale difference sequence so convergence follows from
  the LLN.

  For~\eqref{eq:12}, we have
  \begin{align*}
    \oavg{j}{h} z_{i_j}^2 \E^*( \varepsilon_{i_j}^{*\pm\,2}
    \1\{z_{i_j}^2 \varepsilon_{i_j}^{*\pm\,2} / M_h^\pm > \delta \}
    \mid \Fc_{j-1}^{\pm})
    &= \oavg{i}{h_n} z_i^2 \oavg{j}{b_n} \hat\varepsilon_{j}^{\pm 2}
      \1\{\hat\varepsilon_{j}^{\pm 2} > \delta M_n^{\pm} / z_i^2 \} \\
  \end{align*}
  (Finish with: $\varepsilon_i^2$ is uniformly integrable,
  $\hat\varepsilon_i^2$ gets arbitrarily close to $\varepsilon_i^2$)
\end{proof}

\clearpage
\bibliographystyle{jpe}
\bibliography{tex/references}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
