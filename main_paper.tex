\documentclass[12pt,fleqn]{article}

% Add commit information from Git to the pdf. It is automatically
% generated with the R script 'R/version.R' and can be run from
% the command line in the project's top directory via:
%
% $ Rscript R/version.R
%
% If VERSION.tex does not exist we can't add information from
% Git, so we'll use today's date as a fallback.

\IfFileExists{./VERSION.tex}{\input{VERSION}}{%
\providecommand\VERSION{\today}}

\input{tex/setup}
\input{tex/macros}

\title{Bootstrap Confidence Intervals for Regression Discontinuity Designs with Uniform Kernels}
\author{Ot\'avio Bartalotti \and Gray Calhoun \and Yang He\thanks{All authors: 
Department of Economics, Iowa State University. 260 Heady Hall, Ames, IA  50011.
Bartalotti: Email: bartalot@iastate.edu. Calhoun: gcalhoun@iastate.edu.
He: yanghe@iastate.edu.} \\ PRELIMINARY AND INCOMPLETE \\PLEASE DO NOT CITE OR CIRCULATE WITHOUT PERMISSION}

\begin{document}
\maketitle

\begin{abstract}
This paper proposes an novel bootstrap procedure to obtain robust bias-corrected
confidence intervals in regression discontinuity designs. The approach proposed 
is based on a first-order bias correction and provides an alternative
to the plug-in analytical methods proposed by Calonico, Cattaneo and Titiunik (2014).
The algorithm is simple to implement and generates robust confidence intervals
with improved coverage relative to the analytical alternatives in simulations.
\end{abstract}

\section{Introduction}
Regression Discontinuity (RD) designs have emerged in the last decade as an 
important and popular identification strategy to analyze the impact of policies
and interventions in several fields of the social sciences, including economics,
political science, public policy, sociology among others.

The strategy exploits arbitrary rules used to assign treatment to units, usually
relying on some type of ``score,'' the so called running variable. In its basic 
version, units with score above a certain threshold receive treatment while the 
ones below that cutoff are left untreated.

The identification of the treatment effect at the threshold is then based on 
comparing treated and untreated units at the cutoff. As a practical matter this 
involves comparing units within a bandwidth just above and just below the 
threshold. That comparison is most commonly done by implementing a local linear
estimator above and below the cutoff within bandwidths chosen to minimize the 
asymptotic mean squared error (AMSE) as proposed by Imbens and Kalyanaraman(2012)
and extended by \cite{calonico2014} (CCT, henceforth).

Recently, CCT brought attention to the fact that the popular implementation of RD designs coupling optimal-AMSE bandwidths
and local polynomial estimation provides confidence intervals (CIs) with
incorrect coverage due to the presence of bias and propose an explicit analytical
bias correction procedure that re-centers and re-scale the CI by estimating a higher 
order approximation of the bias term and adjustment its lenght to account for 
the variation introduced by the bias estimation.

As pointed out by \cite{ccf2016}, ``(...)valid inference
requires the delicate balancing act of selecting a bandwidth small enough
to remove smoothing bias, yet large enough to ensure adequate precision.''
The authors compare the strategies of undersmoothing, and explicit bias 
correction concluding that direct bias-correction with the corresponding 
rescaling of the CI's is a superior approach.

This paper contributes to the bias-corrected RD literature by proposing a simple
bootstrap procedure to correct the first order bias term and directly obtain 
valid CIs that provide the correct coverage. The algorithm proposed is easy to
implement and rely on the same conditions as the explicit bias correction
proposed by CCT.

The novel bootstrap procedure is easy to implement and extends to RD designs 
using higher order polynomials, producing valid CIs which are robust to 
bandwidth choice and have similar properties to the analytical procedure in 
CCT while sidestepping the need to derive analytical formulas in each case of interest.

The paper is organized as follows. Section \ref{background} describes the basic
RD approach, its usual implementation, and the explicit analytical bias 
correction approach in the literature. Section \ref{boot} presents the proposed 
bootstrap bias corrected RD algorithm and discusses its properties. Simulation
evidence that the bootstrap procedure provides valid CIs and its relative 
performance to the analytical bias correction are presented in Section \ref{sim}.
Finally, Section \ref{conclusion} concludes.

\section{Background}\label{background}

In the typical sharp RD setting, a researcher wishes to estimate the local 
causal effect of treatment at a given threshold. The running variable, $X_{i}$, 
determines treatment assignment.  Given a known threshold, $\bar{x}$, set to 
zero without loss of generality, a unit receives treatment if $X_{i} \geq 0$ or 
does not receive treatment if $X_{i} < 0$. Let $Y_{i}(1)$ and $Y_{i}(0)$ denote 
the potential outcomes for unit $i$ given it receives treatment and
 in the absence of treatment, respectively. Hence, the observed sample is 
 comprised of the running variable, $X_{i}$, and
 \begin{align}
  Y_{i}=Y_{i}(0) \1\{X_{i}<0\}+Y_{i}(1) \1\{X_{i} \geq 0\}
 \end{align}
where $\1\{ \cdot\}$ denotes the indicator function. For convenience,
define
 \begin{align}
  \mu(x)= \E[Y_{i}|X_{i}=x]
 \end{align}
Also, let $\mu^{(\eta)}(x)=\frac{d^{\eta}\mu(x)}{dx^{\eta}}$ be the $\eta^{th}$ 
derivative of the unknown regression function and define 
$\mu^{(\eta)}_{+}=lim_{x \rightarrow 0^{+}}\mu^{(\eta)}(x)$ and 
$\mu^{(\eta)}_{-}=lim_{x \rightarrow 0^{-}}\mu^{(\eta)}(x)$.
In most cases the population parameter of interest is 
$\tau=\E[Y(1)-Y(0)|X=\bar{x}]$ (i.e., the average treatment effect
at the threshold). Under continuity and smoothness conditions on both the 
conditional distribution of $X_i$ and the first moments of $Y(0)$ and $Y(1)$ at
the cutoff, $\tau$ is nonparametrically identifiable (Hahn, Todd and Van der 
Klaauw, 2001) by:
\begin{align}
 \tau&= \mu_{+}- \mu_{-} \nonumber \\ 
&\text{ where } \mu_{+}=lim_{x \rightarrow 0^{+}}\mu(x),\text{ and } 
\mu_{-}=lim_{x \rightarrow 0^{-}}\mu(x)
\end{align}

Naturally, the estimation of $\tau$ in RD designs focuses on the 
problem of approximating $\E[Y(1)|X=x]$ and $\E[Y(0)|X=x]$
near the cutoff. Due to its desirable properties when estimating regression 
functions at boundary points, we consider the popular approach of fitting 
separate kernel-weighted local linear regressions in neighborhoods on both 
sides of the threshold.\footnote{See \cite{HTV2001}, \cite{Porter03} or
\cite{FanGijbels92} for discussions of the properties of local polynomial 
regressions for boundary problems.The results presented here are valid for
higher order polynomials and discontinuities at derivatives, like ``Kink RDD''. We focus on the linear case for ease in exposition.}
For the local linear model, we use the following estimator as described in 
\cite{calonico2014},\footnote{Throughout the paper we follow the
notation on CCT very closely.}
\begin{align*}
 \hat{\tau}(h_{n})&=\hat {\mu}_{+}(h_{n}) -\hat{\mu}_{-}(h_{n})\\
(\hat {\mu}_{+}(h_{n}),\hat {\mu}^{(1)}_{+}(h_{n}))'&= argmin_{b_{0},b_{1}} 
\sum_{i=1}^{N}\1\{X_{i} \geq 0\}(Y_{i}-b_{0}-X_{i}b_{1})^{2} \cdot K_{h}(X_{i})\\
(\hat {\mu}_{-}(h_{n}),\hat {\mu}^{(1)}_{-}(h_{n}))' &= argmin_{b_{0},b_{1}}
\sum_{i=1}^{N}\1\{X_{i} < 0\}(Y_{i}-b_{0}-X_{i}b_{1})^{2}\cdot K_{h}(X_{i})
\end{align*}
where $K_{h}(x_{i}) = K\left(\frac{x_{i}}{h}\right)\frac{1}{h}$.

As shown by \cite{HTV2001}, \cite{Porter03} and expanded by CCT identification and inference procedures can be developed based on the following assumptions\footnote{The assumptions below are the same as presented by CCT.}
\begin{assumption}[Behavior of the DGP near the cutoff]\label{A1}
 For some $\kappa_{0} > 0$, the following holds in the neighborhood $(-\kappa_{0},\kappa_{0})$ around the cutoff $ \bar{x}= 0$:
 \begin{enumerate}
  \item $E[\left.Y_{i}^{4}\right|X_{i}=x]$ is bounded and the density of $X$, $f(x)$, is continuous and bounded away from zero.
  \item $\mu_{+}(x)=\E[\left.Y_{i}(1)\right|X_{i}=x]$ and $\mu_{-}(x)=\E[\left.Y_{i}(0)\right|X_{i}=x]$ are $S$ times continuously differentiable.
  \item $\sigma^{2}_{+}(x)=\V[\left.Y_{i}(1)\right|X_{i}=x]$ and $\sigma^{2}_{-}(x)=\V[\left.Y_{i}(0)\right|X_{i}=x]$ are continuous and bounded away from zero.
 \end{enumerate}
\end{assumption}

The conditions in Assumption \ref{A1} are the usual smoothness and existence conditions in which the RD literature relies. Its second part is important for the characterization of the leading bias term that will be the focus of the bias correction procedures described in CCT and in the novel  bootstrap procedure proposed in Section \ref{boot}.

The second set of assumptions regards the possible kernel weights used in RD.

\begin{assumption}[Kernel]\label{A2}
 For some $\kappa>0$, the kernel function $k(\cdot):[0, \kappa] \rightarrow \RR$ is bounded and nonnegative, zero outside its support, and positive and continuous on $(0, \kappa)$.
\end{assumption}

Assumption \ref{A2} supports the kernels most commonly used in applications, in particular the broadly used uniform kernel $k(u)=\1[0\leq u\leq 1]$, which simplifies to the use of a local linear OLS on both sides of the treshold. Since the objective of this paper is to present a simple bootstrap procedure to obtain robust confidence intervals in the context of RD designs that can be easily implemented by practitioners, during the remainder of the paper we focus on estimators that use the rectangular kernel. The use of different weighting structures implied by different kernels complicates the bootstrap algorithm in meaningful ways and such analysis is left to future work.

\begin{assumption}[Bandwidth]\label{A3}
  Let $h_n$ be the bandwidth used to estimate the local polynomial of
  order $p$ and let $b_n$ be the bandwidth used for $p+1$. Then
  $h_n = o(n^{-1/5})$, $b_n = o(n^{-1/5})$, $n h_n \to \infty$, and
  $n b_n \to \infty$ as $n \to \infty$.\footnote{%
    Unless otherwise stated, all limits in this paper are assumed to
    hold as $n \to \infty$.} %
  The relationship $h_n \leq b_n$ also holds for all $n$.
\end{assumption}

\cite{calonico2014} point out that the conventional approaches to construct confidence intervals for $\tau$ using the local linear estimator rely on a large-sample approximation for the standardized $t$-statistic that is valid only if the bandwidth shrink fast enough to eliminate the leading bias term contribution to the approximation. Hence, if $nh_{n}^{5}\rightarrow0$ and $nh_{n}\rightarrow \infty$, then
 \begin{align}
  T(h_{n})=\frac{\hat{\tau}(h_{n})-\tau}{\sqrt{V(h_{n})}}\rightarrow_{d}N(0,1), \text{        }V(h_{n})=\V[\left.\tau(h_{n})\right|\chi_{n}], \text{        }\chi_{n}=[X_{1}, \dots X_{n}]^{\prime}
 \end{align}
 The conditional variance $V(h_{n})$ depends on the $\sigma^{2}_{+}=lim_{x \rightarrow 0^{+}}\sigma^{2}_{+}(x)$ and $\sigma^{2}_{-}=lim_{x \rightarrow 0^{-}}\sigma^{2}_{-}(x)$ as well as $h_{n}$ and known quantities that depend on the kernel and order of polynomial used in the estimation. Under these conditions we could use a conventional confidence interval for $\tau$ given by
 \begin{align}
  I(h_{n})=\left[\hat{\tau}(h_{n})\pm \Phi^{-1}_{1-\frac{\alpha}{2}}\sqrt{V(h_{n})} \right]
 \end{align}
with $\Phi^{-1}_{1-\frac{\alpha}{2}}$ being the $\alpha$-quantile of the standard normal distribution. However, most approaches to select the bandwidth, $h_{n}$ in the literature, including the widely used optimal-AMSE bandwidth selector proposed by \cite{IK} lead to bandwidth choices that are ``too large'' since they do not satisfy the condition $nh_{n}^{5}\rightarrow0$, leading to a first-order bias in the distributional approximation used to construct the confidence intervals.

\cite{calonico2014} resolve the problem by deriving the analytical form of the first-order bias and correcting it directly. To obtain correct coverage the confidence interval needs not only to be re-centered to correct the bias, but also rescaled to allow for the additional variability introduced by the bias correction. Their approach relies on obtaining a bias-corrected  estimator, $\hat{\tau}^{bc}$ and its adjusted variance. It is important first to note that the first-order bias term itself will depend on the bandwidth $h_{n}$. The approximate bias term can be described as:
\begin{align*}
     \E[\hat{\tau}(h_{n})|\chi_{n}]-\tau=& h_{n}^{2}\Bs(h_{n})\{1+o_{p}(1)\}\\
     \Bs(h_{n})=&\frac{\mu_{+}^{(2)}}{2!}\Bf_{+}(h_{n})-\frac{\mu_{-}^{(2)}}{2!}\Bf_{-}(h_{n})
   \end{align*}
where $\Bf_{+}(h_{n})$ and $\Bf_{-}(h_{n})$ are observed quantities that depend on the data, kernel and $h_{n}$. The plug-in bias-corrected estimator then requires estimates for the second derivatives of the conditional mean from above and below the cutoff, $\mu_{+}^{(2)}$ and $\mu_{-}^{(2)}$. CCT propose using a conventional local quadratic estimator, i.e. one order higher than the polynomial used to obtain $\hat{\tau}$,  using a (potentially) different pilot bandwidth $b_{n}$.
\begin{align*}
     \hat{\tau}^{bc}(h_{n}, b_{n})=& \hat{\tau}-h_{n}^{2}\hat{\Bs}(h_{n},b_{n})\\
     \hat{\Bs}(h_{n})=&\frac{\hat{\mu}_{+}^{(2)}(b_{n})}{2!}\Bf_{+}(h_{n})-\frac{\hat{\mu}_{-}^{(2)}(b_{n})}{2!}\Bf_{-}(h_{n})
   \end{align*}
By incorporating the contribution of both $\hat{\tau}(h_{n})$ and $\hat{\Bs}(h_{n},b_{n})$ to the asymptotic distribution of the estimator, CCT obtain a robust confidence interval with a different asymptotic variance in general. So under Assumptions \ref{A1} and \ref{A2}, if $n\min\{h_{n}^{5}, b_{n}^{5}\}max\{h_{n}^{2}, b_{n}^{2}\}\rightarrow 0$ and $n\min\{h_{n}, b_{n}\}\rightarrow \infty$ and $\kappa \max\{h_{n},b_{n}\}< \kappa_{0}$, they show that
\begin{align}
  T^{rbc}(h_{n}, b_{n})=\frac{\hat{\tau}^{bc}(h_{n}, b_{n})-\tau}{\sqrt{V^{bc}(h_{n}, b_{n})}}\rightarrow_{d}N(0,1), \text{        }V^{bc}(h_{n}, b_{n})=V(h_{n})+C^{bc}(h_{n}, b_{n})\\
 \end{align}
where the additional term $C^{bc}(h_{n}, b_{n})$ depends on the (asymptotic) variability of the bias estimate used for correction as well as its correlation with the original RD estimator $\hat{\tau}(h_{n})$.\footnote{The details of the formulas are currently ommitted from this paper since their form is not relevant to the bootstrap procedure innovation proposed. The details can be found at CCT Appendix.}  Under these conditions we can construct a valid confidence interval for $\tau$ given by
 \begin{align}
  I^{rbc}(h_{n}, b_{n})=\left[\left(\hat{\tau}(h_{n})-h_{n}^{2}\hat{\Bs}(h_{n},b_{n})\right)\pm \Phi^{-1}_{1-\frac{\alpha}{2}}\sqrt{V(h_{n})+C^{bc}(h_{n}, b_{n})} \right]
 \end{align}

This approach is shown by the authors to significantly improve coverage of the CIs constructed and provides practitioners with a new tool set to perform inference that is more robust to the choice of bandwidth. We build upon the insight provided by CCT bias-corrected estimator and propose a simple bootstrap procedure that can directly construct the robust CIs without requiring the derivation of analytical formulas and direct estimators for the bias, variance and covariance terms, while relying on the same first-order bias correction approximation proposed by that paper.

\section{Bootstrap Bias Correction}\label{boot}

Building upon the results and intuition developed by CCT we propose to implement a first-order bias correction and obtain valid CIs through a bootstrap procedure.
Intuitively, for the local-linear RD estimator's case, we will rely on a local higher-order (quadratic) polynomial approximation for the conditional outcome around the cutoff to capture the potential bias present in the original RD estimate. This insight comes directly from CCT using the same local quadratic approximation to estimate bias term as presented in Section \ref{background}.

The first algorithm describes how to use local linear regression to obtain a point estimate and perform bias correction with local quadratic regression. The second algorithm describes how to obtain its confidence interval with correct coverage. For simplicity, we adopt the same optimal-AMSE bandwidth selector proposed by CCT.\footnote{An interesting question is whether one could use a bootstrap based procedure to select a bandwidth that would achieve similar (or improved) results. This is beyond the scope of this particular paper but is part of ongoing research effort.} 

Essentially, we apply CCT's insight that a polynomial one order higher can capture the first-order bias behavior and create a bootstrap procedure that can correctly mimic the behavior of the bias present in the original estimator. For such, for a chosen bandwidth, we estimate a local quadratic regression the same way CCT suggested but then create bootstrap data by imposing the local quadratic functional form as the DGP. Then, in the ``bootstrap data'' the DGP is known. Let the ``true'' treatment effect under the bootstrap DGP be denoted by $\tau^{*}$. We then estimate the treatment effect using the (misspecified) local linear estimator, $\hat{\tau}^{*}$ to the generated data. Hence, the bias for a specific bootstrap data is given by $\tau^{*}-\hat{\tau}^{*}$. More importantly, by generating new bootstrapped data based on the same quadratic DGP and its residual's distribution, we can obtain an estimate of the first-order bias (the part that is captured by $\hat{\Bs}(h_{n},b_{n})$ in CCT's formulas on Section \ref{background}) in the original estimate by averaging across the bias in all $B$ bootstrapped data estimates,i.e., $\tau^{*} - \sum_{i = 1}^{B} \hat{\tau}_{ i}^{*}$. Algorithm~\ref{Alg1} describes this procedure step-by-step for clarity.

\begin{algorithm}[Bias estimation]\label{Alg1}
  Assume $h_n$ and $b_n$ are bandwidths as described by Assumption~\ref{A3} and
  define
  \begin{align}
    I^-(h) &= \{i : h < X_i < 0\}, &
    I^+(h) &= \{i : 0 < X_i < h\}.
  \end{align}
  Also define $M^-(h_n)$ and $M^+(h_n)$ to be the number of elements in
  $I^-(h_n)$ and $I^+(h_n)$ respectively.
  \begin{enumerate}
  \item Estimate the local polynomials $g_{p+1}^+$ and $g_{p+1}^-$ as defined by
    Equation --- and define the residuals
    \begin{equation}
      \label{eq:1}
      \hat\varepsilon_i =
      \begin{cases}
        Y_i - \hat g_{p+1}^-(X_i) & \textif\ X_i < 0 \\
        Y_i - \hat g_{p+1}^+(X_i) & \otherwise
      \end{cases}
    \end{equation}
  \item Repeat the following steps $B_1$ times to produce a sequence of bootstrap
    estimates $\hat\tau_1^*,\dots,\hat\tau_{B_1}^*$. For the $k$th step:
    \begin{enumerate}
    \item Draw an i.i.d. sample of size $M^-(b_n)$ from
      $\{\hat\varepsilon_i : i \in I^-(b_n)\}$ and one of size $M^+(b_n)$ from
      $\{\hat\varepsilon_i : i \in I^+(h_n)\}$. Let $\varepsilon_i^{*-}$ and
      $\varepsilon_i^{*+}$ denote the $i$th element of each sample and construct
      \begin{align}
        \label{eq:2}
        Y_i^{*-} &= \hat g_{p+1}^-(X_i^{*-}) + \varepsilon_i^{*-} &
        Y_i^{*+} &= \hat g_{p+1}^+(X_i^{*+}) + \varepsilon_i^{*+}
      \end{align}
    \item Calculate $\hat\mu_+^*$ and $\hat\mu_-^*$ as
      \begin{align}
        \hat\mu_-^*
        &= \argmin_{\beta_0} \min_{\beta_1,\dots,\beta_p}
          \sum_{i = 1}^{M^-(h_n)} (Y_i^{*-} -
          (\beta_0 + \beta_1 X_i^{*-} + \cdots + \beta_p (X_i^{*-})^p))^2 \\
        \hat\mu_+^*
        &= \argmin_{\beta_0} \min_{\beta_1,\dots,\beta_p}
          \sum_{i = 1}^{M^+(h_n)} (Y_i^{*+} -
          (\beta_0 + \beta_1 X_i^{*+} + \cdots + \beta_p (X_i^{*+})^p))^2
      \end{align}
    \item Save $\hat\mu_+^* - \hat\mu_-^*$ as $\hat\tau^*_k$.
    \end{enumerate}
  \item Estimate the bias as
    \begin{equation}
      \label{eq:4}
      \Delta^* = \tfrac{1}{B_1} \sum_{k=1}^{B_1} \hat\tau^*_k -
      \big[\hat g_{p+1}^+(0) - \hat g_{p+1}^-(0)\big]
    \end{equation}
  \end{enumerate}
\end{algorithm}
Obviously, the distribution of $\Delta^*$ depends on $B_1$ and we could
incorporate that dependency into our notation by wrting
$\Delta^*_{B_1}$. However, we will follow standard practice in the bootstrap
literature and assume that $B_1$ is large enough that it can be ignored and
define $\Delta^* = \plim_{B_1 \to \infty} \Delta^*_{B_1}$

Under Assumptions~\ref{A1} and~\ref{A2} the procedure described by
Algorithm~\ref{Alg1} provides a consistent estimator of the bias component and
this estimator converges to zero fast enough in probability that it can be be
used as a correction. As is standard in the bootstrap literature, we will assume
that the number of bootstrap replications, $B_1$, is large enough that the
simulation error can be ignored.

\begin{lemma}\label{L1}
  Under Assumptions~\ref{A1} and~\ref{A3},
  $\Delta^* = \E \hat\tau - \tau + o_p()$.
\end{lemma}

With Algorithm~\ref{Alg1} at hand, we can move to generate the bias-corrected confidence intervals to perform inference about $\tau$. Here the idea is similar, but now we actually calculate a fully bias corrected estimate using Algorithm~\ref{Alg1} for every bootstrapped dataset generated, a bootstrap within the bootstrap. Each one of the bias corrected estimates for the generated data is denoted $\hat{\tau}^{bc*}$. These estimator's empirical distribution capture the variability induced by the first order bias correction that lead CCT to propose the rescaling of their confidence intervals by introducing $C^{bc}(h_{n},b_{n})$ in the asymptotic variance formula on Section \ref{background}.

\begin{algorithm}[Confidence intervals]\label{Alg2}
  Define $I^-$, $I^+$, $M^-$, and $M^+$ as in Algorithm~\ref{Alg1}.
  \begin{enumerate}
  \item Estimate the local polynomials $g_{p+1}^+$ and $g_{p+1}^-$ as defined by
    Equation --- and define the residuals
    \begin{equation}
      \label{eq:6}
      \hat\varepsilon_i =
      \begin{cases}
        Y_i - \hat g_{p+1}^-(X_i) & \textif\ X_i < 0 \\
        Y_i - \hat g_{p+1}^+(X_i) & \otherwise
      \end{cases}
    \end{equation}
  \item Repeat the following steps $B_2$ times to produce a sequence of bootstrap
    estimates $\hat\tau_1^{\prime*},\dots,\hat\tau_B^{\prime*}$. For the $k$th step:
    \begin{enumerate}
    \item Draw an i.i.d. sample of size $M^-(b_n)$ from
      $\{\hat\varepsilon_i : i \in I^-(b_n)\}$ and one of size $M^+(b_n)$ from
      $\{\hat\varepsilon_i : i \in I^+(h_n)\}$. Let $\varepsilon_i^{*-}$ and
      $\varepsilon_i^{*+}$ denote the $i$th element of each sample and construct
      \begin{align}
        \label{eq:2}
        Y_i^{*-} &= \hat g_{p+1}^-(X_i^{*-}) + \varepsilon_i^{*-} &
        Y_i^{*+} &= \hat g_{p+1}^+(X_i^{*+}) + \varepsilon_i^{*+}
      \end{align}
    \item Calculate $\hat\mu_+^*$ and $\hat\mu_-^*$ as
      \begin{align}
        \hat\mu_-^*
        &= \argmin_{\beta_0} \min_{\beta_1,\dots,\beta_p}
          \sum_{i = 1}^{M^-(h_n)} (Y_i^{*-} -
          (\beta_0 + \beta_1 X_i^{*-} + \cdots + \beta_p (X_i^{*-})^p))^2 \\
        \hat\mu_+^*
        &= \argmin_{\beta_0} \min_{\beta_1,\dots,\beta_p}
          \sum_{i = 1}^{M^+(h_n)} (Y_i^{*+} -
          (\beta_0 + \beta_1 X_i^{*+} + \cdots + \beta_p (X_i^{*+})^p))^2
      \end{align}
    \item Apply Algorithm~\ref{Alg1} to the bootstrapped data set,
      \begin{equation*}
        (Y_1^{*-}, X_1^{*-}),\dots,(Y_{M^-(b_n)}^{*-},X_{M^-(b_n)}^{*-}),
        (Y_1^{*+}, X_1^{*+}),\dots,(Y_{M^+(b_n)}^{*+},X_{M^+(b_n)}^{*+}),
      \end{equation*}
      using the same bandwidths $h_n$ and $b_n$ that are used in the rest of
      this algorithm but reestimating all of the local polynomials on the
      bootstrap data. Let $\Delta^{**}$ represent the bias estimator returned
      by Algorithm~\ref{Alg1}.
    \item Save the bias-corrected estimator
      $\hat\tau_k^{\prime*} = \hat\mu_+^* - \hat\mu_i^* - \Delta^{**}$.
    \end{enumerate}
  \item Define the confidence interval endpoints $\hat\ell(\alpha)$ and
    $\hat u(\alpha)$ as the $\alpha/2$ and $1-\alpha/2$ quantiles of the
    empirical CDF of $\hat\tau_1^{\prime*},\dots,\hat\tau_{B_2}^{\prime*}$.
  \end{enumerate}
\end{algorithm}

As earlier, we assume that $B_1$ and $B_2$ are large enough that simulation
error can be ignored. Define the limiting values
\begin{align*}
\Delta^{**} &= \plim_{B_1,B_2 \to \infty} \Delta^{**}_{B_1,B_2} \\
\hat\ell^*(\alpha) &= \plim_{B_1,B_2 \to \infty} \hat\ell^*_{B_1,B_2}(\alpha) \\
\hat u^*(\alpha) &= \plim_{B_1,B_2 \to \infty} \hat u^*_{B_1,B_2}(\alpha).
\end{align*}
Two results follow. The first is that the secondary bootstrap step produces
a consistent estimator of the bias under the bootstrap distribution. The
second is that the intervals overall are asymptotically valid.

\begin{lemma}\label{L2}
  Under Assumptions~\ref{A1} and~\ref{A3},
  $\Delta^{**} = \E^* \hat\tau^* - \tau^* + o_{p^*}(-)$.
\end{lemma}


\begin{theorem}\label{T1}
  Let $\Pr_\tau$ represent the probability distribution of the random variables
  under the assumption that $\tau$ is the true value of the ATE and suppose
  Assumptions~\ref{A1} and~\ref{A3} hold. Then
  \begin{equation}
    \label{eq:5}
    \plim_{n \to \infty} \sup_\tau
    \Pr_\tau[\hat\ell(\alpha) \leq \tau \leq \hat u(\alpha)] \geq 1 - \alpha.
  \end{equation}
  Both of the one-sided intervals are asymptotically valid under the same
  assumptions as well:
  \begin{align}
    \plim_{n \to \infty} \sup_\tau
    \Pr_\tau[\hat\ell(\alpha) \leq \tau ] &\geq 1 - \alpha / 2, &
    \plim_{n \to \infty} \sup_\tau
    \Pr_\tau[\tau \leq \hat u(\alpha)] &\geq 1 - \alpha / 2.
  \end{align}
\end{theorem}

Evidence of the usefulness of the procedures proposed above and their relative performance to the analytical bias correction proposed in CCT are presented in a series of Monte Carlo simulations in Section \ref{sim}.

\section{Simulation Evidence}\label{sim}

This section presents evidence from Monte Carlo simulations that the bootstrap bias correction procedures proposed in Section \ref{boot} produces valid, robust confidence intervals similar to the ones obtained by the analytical procedures established in CCT. The bootstrap CIs obtained compare favorably to the analytical alternative, with coverage closer to the nominal size of the desired test and shorter length of the intervals in the specifications implemented.

We selected three data generating processes (DGP) that are widely used in the RD literature to evaluate the performance of the proposed bootstrap procedure, those are the same chosen by CCT and \cite{IK} and provide a good testing ground for bias correction due to their exacerbated curvature. To be specific, the pseudo data are generated as 500 i.i.d. draws from:
\begin{align*}
& Y_{i} = \mu_{j}(X_{i}) + \epsilon_{i}, \;\;\; X_{i} \sim  (2 B (2,4) - 1)), \\
& \epsilon_{i} \sim N(0, \sigma^{2}), \;\;\; \sigma = 0.1295, \;\;\; j = 1,2,3.
\end{align*}
Three different choices of functional forms for $\mu_{j}(x)$ are as follows:
\begin{align}
\mu_{1}(x) & = 
\begin{cases}
0.48 + 1.27x + 7.18x^{2} + 20.21x^{3} + 21.54x^{4} + 7.33x^{5}, \;\;\;\; & \text{if} \;\; x < 0, \\
0.52 + 0.84x - 3.00x^{2} + 7.99x^3 - 9.01x^4 + 3.56x^{5},  & \text{if} \;\; x \ge 0.
\end{cases}
\\
\mu_{2}(x) & = 
\begin{cases}
3.71 + 2.30x + 3.28x^2 + 1.45x^3 + 0.23x^4 + 0.03x^5, \;\; & \text{if} \;\; x < 0, \\
0.26 + 18.49x - 54.81x^2 + 74.30x^3 - 45.02x^4 + 9.83x^5,  & \text{if} \;\; x \ge 0.
\end{cases}
\\
\mu_{3}(x) & =
\begin{cases}
0.48 + 1.27x + 0.5 \times 7.18x^{2}+ \\
+ 0.7 \times 20.21x^3 + 1.1 \times 21.54x^4 + 1.5 \times 7.33x^5, \;\;\;\;\;\;\;\;\;\;\;\; & \text{if} \;\; x < 0, \\
0.52 + 0.84x - 0.1 \times 3.00x^{2}+ \\
+ 0.3 \times 7.99x^3 - 0.1 \times 9.01x^4 + 3.56x^5, & \text{if} \;\; x \ge 0.
\end{cases}
\end{align}

The functional form for $\mu_{1}(x)$ and $\mu_{2}(x)$ follow the data observed by \cite{lee2008} and \cite{ludwig2007}, respectively, while $\mu_{3}(x)$ is a transformation of $\mu_{1}(x)$ designed to emphasize its curvature and therefore bias in the usual RD approach. For the simulation studies, we used 250 bootstraps for bias correction, 999 bootstraps to calculate confidence interval and a total 500 replications. All results presented are based on a 0.95 desired confidence level. Table1 presents the main results in which a local-linear RD estimator ($p=1$) is employed with the bias-correction based on a local-quadratic approximation ($q=2$). Table 2 presents similar results for $p=2$ and $q=3$.

\begin{table}[ht] \label{Tb1}
	\caption{Summary of estimates ($p = 1, q = 2$)}
	\centering
	\begin{tabular}{ccccccc}
		\hline
		DGP & True & Bias & SD & MSE & CI coverage & CI length \\ 
		\hline
		\multicolumn{7}{c}{Panel A: Bootstrap Bias-Correction} \\
		1 & 0.04 & -0.013 & 0.067 & 0.068 & 0.938 & 0.243 \\ 
		2 & -3.45 & -0.014 & 0.083 & 0.084 & 0.954 & 0.319 \\ 
		3 & 0.04 & -0.006 & 0.069 & 0.070 & 0.956 & 0.247 \\ 
		&&&&&& \\
		\multicolumn{7}{c}{Panel B: Analytical Bias Correction (CCT 2014)} \\
		1 & 0.04 & -0.016 & 0.068 & 0.070 & 0.908 & 0.244 \\ 
		2 & -3.45 & -0.014 & 0.092 & 0.093 & 0.928 & 0.350 \\ 
		3 & 0.04 & -0.004 & 0.071 & 0.071 & 0.918 & 0.249 \\ 
		\hline
	\end{tabular}
\end{table}

\begin{table}[ht]\label{Tb2}
	\caption{Summary of estimates from bootstrap ($p = 2, q = 3$)}
	\centering
	\begin{tabular}{ccccccc}
		\hline
		DGP & True & Bias & SD & MSE & CI coverage & CI length \\ 
		\hline
		\multicolumn{7}{c}{Panel A: Uniform Kernel} \\
		1 & 0.04 & -0.002 & 0.075 & 0.075 & 0.958 & 0.295 \\ 
		2 & -3.45 & 0.002 & 0.087 & 0.086 & 0.934 & 0.302 \\ 
		3 & 0.04 & -0.008 & 0.083 & 0.083 & 0.942 & 0.292 \\ 
		&&&&&& \\
		\multicolumn{7}{c}{Panel B: Analytical Bias Correction (CCT 2014)} \\
		1 & 0.04 & -0.004 & 0.082 & 0.082 & 0.926 & 0.295 \\ 		
		2 & -3.45 & -0.005 & 0.078 & 0.078 & 0.960 & 0.323 \\ 		
		3 & 0.04 & -0.003 & 0.080 & 0.080 & 0.938 & 0.295 \\ 
		\hline
	\end{tabular}
\end{table}
As can be seen on the tables above, the bootstrap bias-correction procedure
proposed in this paper provides a simple alternative to obtain valid robust
confidence intervals in RD designs and performs well compared to the analytical 
bias correction procedures proposed by CCT.


\section{Conclusion}\label{conclusion}
This paper proposes a novel bootstrap procedure to obtain robust bias-corrected
confidence intervals in regression discontinuity designs. 
The approach proposed builds upon the developments and intuition advanced by CCT and is based on a first-order bias correction.
Our procedure provides an alternative to the plug-in analytical methods in the literature and is simple to implement generating robust confidence intervals.
Simulation evidence is presented that the proposed bootstrap bias correction and confidence intervals have improved coverage and shorter length relative to the analytical alternatives proposed by CCT.


\clearpage
\bibliographystyle{jpe}
\bibliography{tex/references}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
