\documentclass[12pt,fleqn]{article}

% Add commit information from Git to the pdf. It is automatically
% generated with the R script 'R/version.R' and can be run from
% the command line in the project's top directory via:
%
% $ Rscript R/version.R
%
% If VERSION.tex does not exist we can't add information from
% Git, so we'll use today's date as a fallback.

\IfFileExists{./VERSION.tex}{\input{VERSION}}{%
\providecommand\VERSION{\today}}

\input{tex/setup}
\input{tex/macros}

\title{Bootstrap Confidence Intervals for Sharp Regression Discontinuity Designs
  with the Uniform Kernel}

\author{Ot\'avio Bartalotti \and Gray Calhoun \and Yang He\thanks{All authors:
Department of Economics, Iowa State University. 260 Heady Hall, Ames, IA  50011.
Bartalotti: Email: bartalot@iastate.edu. Calhoun: gcalhoun@iastate.edu.
He: yanghe@iastate.edu.}}

\begin{document}
\maketitle

\begin{abstract}\noindent
  This paper develops a novel bootstrap procedure to obtain robust
  bias-corrected confidence intervals in regression discontinuity designs that
  use the uniform kernel. The procedure uses a residual bootstrap to estimate
  the bias of the RD estimator, which is then removed from the original
  estimator.  The bias-corrected estimator is then bootstrapped itself to
  generate valid confidence intervals. This procedure is valid for empirically
  relevant bandwidths when the treatment effect is estimated with a local linear
  model and the residual bootstraps use a second order polynomial; validity
  holds under similar conditions to Calonico, Cattaneo and Titiunik's (2014,
  \textit{Econometrica}) analytical correction.
\end{abstract}

\section{Introduction}
Regression Discontinuity (RD) designs have emerged in the last decade as an
important and popular research design strategy for analyzing the causal impact
of policies and interventions in several fields of the social sciences,
including economics, political science, public policy, and sociology.
This research strategy exploits the fact that many programs use somewhat
arbitrary thresholds to determine whether or not to provide a ``treatment,''
usually relying on some type of score, which this literature calls a ``running
variable.'' In its basic version, the sharp RD,  individuals or groups with score above a
certain threshold receive treatment while the ones below that cutoff are left
untreated.
The identification of the treatment effect at the threshold is then based on
comparing treated and untreated units at the cutoff. When a subject's
position just above or below the cutoff is credibly not related to
other factors impacting outcomes, including their unobserved
characteristics, differences between treated and untreated individuals
at the cutoff can be plausibly attributed to the treatment alone. As a
practical matter this involves comparing units within a bandwidth just
above and just below the threshold.

The RD design strategy was introduced by \cite{thistlethwaite1960} to study
educational outcomes, and many of its first recent applications in economics
were to estimate the effects of other educational policies: evaluating the
impact of investments in school facilities, class sizes, remedial education,
early childhood education, and financial aid effects on student achievement and
later outcomes, for example.\footnote{%
  See, for example, \cite{vdk2002}, \cite{jacoblefgren2004}, \cite{ludwig2007},
  \cite{urquiola2009}, \cite{cellini2010}} %
But the underlying identification strategy has proven to apply much more widely
and RD has been used in health economics;\footnote{%
  \cite{Card2009,barreca2011saving}} %
political science, providing evidence regarding incumbency advantage in
elections, electoral enfranchising and policy changes associated with the
introduction of electronic voting, strategic voting behavior, and local media
effects and ad expenditure in presidential elections in the U.S.\footnote{%
  \cite{lee2008}, \cite{caughey2011elections}, \cite{keele2014geographic},
  \cite{erikson2015}, \cite{Fujiwara2011,Fujiwara2015}}, %
among other fields. \cite{imbens2008} and \cite{lee2010} provide recent
overviews of this literature with many more examples.

In these studies, identification occurs exactly at the cutoff, so the
treatment effect is typically estimated by fitting separate local linear models
above and below the cutoff, then extrapolating the models to the exact point of
discontinuity. The difference between the estimated outcomes at that point is
taken to be an estimate of the treatment effect. As a practical matter, a key
econometric issue is determining the bandwidth for the local linear models.  One
very popular choice is the bandwidth estimator proposed by \cite{IK} and
extended by \cite{calonico2014}, which minimizes the Asymptotic Mean Squared
Error (AMSE) of difference in the models' point estimators at the cutoff.

As \cite{calonico2014}\footnote{%
  We will refer to \cite{calonico2014} frequently, so we will reference it as
  ``CCT'' for the rest of the paper.} %
point out, however, the \cite{IK} bandwidth estimator has the serious drawback
that, although it is AMSE-optimal for point estimation, it produces invalid
confidence intervals and hypothesis tests. The AMSE-optimal bandwidth
converges to zero slowly, so the remaining bias term is large enough to affect
the asymptotic distribution of the estimator. In that case the usual ``naive''
confidence intervals for the RD treatment effects may be biased and have
coverage well below their nominal level.
CCT propose an estimator of the bias resulting from undersmoothing and provide a
treatment effect estimator that remains asymptotically unbiased even when the
bandwidth converges to zero slowly, at the AMSE-optimal rate. They also show
that this bias-correction term contributes to the asymptotic variance of the
resulting treatment effect estimator and provide a new formula for the
asymptotic variance of the bias-corrected estimator. The resulting confidence
intervals have accurate coverage even when the naive RD interval does not.

In this paper, we propose a bootstrap alternative to CCT's analytical
corrections. CCT justify their estimator by showing that the bias and variance
components for a local linear model can be accounted for by estimating a local
second order polynomial with bandwidth of the same order.\footnote{%
  More generally, they show that the bias and variance of a local polynomial of
  order $p$ can be accounted for by estimating the $p+1$ local polynomial. We
  will restrict our analysis to the case with $p = 1$ in this paper because
  of its widespread use.} %
They use a Taylor expansion around the cutoff to show that the bias associated
with the second order polynomial converges to zero at a faster rate, fast
enough that the bias of the local linear model can be estimated and removed
using the second order polynomial. The second order polynomial also provides
fast enough convergence that it can be used to estimate the correct variance
correction as well.

Our approach exploits CCT's theoretical insight through a new residual
bootstrap. In particular, we propose estimating the local linear model as usual,
then estimating a local second order polynomial and generating bootstrap
datasets by resampling the residuals of that polynomial. Since the second order
polynomial is the true Data Generating Process (DGP) for the bootstrapped data,
its estimate of the treatment effect is the true value of the treatment effect
under the distribution induced by this bootstrap. The bias of the linear model
is therefore known
under this distribution and can be calculated by averaging the error of the
linear model's estimates across many bootstrap replications. This approach is
described in detail by our Algorithm~\ref{Alg1} and the resulting bias corrected
estimator is shown to be asymptotically normal with mean zero in our
Theorem~\ref{T1} under AMSE-optimal bandwidth rates.

Just as in CCT, our bias correction step introduces additional
variability. However, the second order polynomial again adequately estimates the
features of the true DGP that are necessary for estimating and accommodating that
additional variability. So we propose
an iterated bootstrap procedure \citep{hall1988}: use the second order
polynomial residual bootstrap to produce many bootstrap replications of the bias
corrected estimator, and then use the resulting bootstrap distribution to
produce confidence intervals. This procedure, which requires bootstrapping the
datasets produced by an initial bootstrap procedure, is described in Algorithm~\ref{Alg2}, and the resulting confidence intervals are shown to be asymptotically valid in Theorem~\ref{T2}.

This bootstrap procedure offers some advantages over analytical methods.  In
particular, both this paper and CCT assume that the observations are generated
independently of each other; however, extending these bias correction methods to
other forms of dependence is relatively straightforward for the bootstrap but
can be substantially more complicated for analytical
corrections, which have to be explicitly derived by the researcher. Similarly,
although we only provide results for the local linear model in this paper, it is
trivial to implement this procedure for higher order polynomials, for
covariate-adjusted estimators (\citealp{frolich2007}, \citealp{calonico2015}),
or for other local smoothers. \citep{loader2006} However, in this paper we focus
on the baseline case of sharp RD with a local linear model and uniform
kernel. Extensions to fuzzy RD designs\footnote{%
  ``Fuzzy'' regression discontinuity design \citep{HTV2001}, as opposed to
  ``sharp'' RD, describes situations where the probability of treatment changes
  discontinuously at a known threshold, but by less than 100\%. Then there are
  treated and untreated subjects above and below the discontinuity but the
  treatment effect remains identified.} %
and nonuniform kernels, which require nontrivial changes to the underlying
bootstrap algorithm, as well as developments to address cross sectional
dependence, are the subject of ongoing research.

The paper is organized as follows. Section~\ref{background} describes the basic
RD approach, its usual implementation, and the explicit analytical bias
correction approach in the literature. Section~\ref{boot} presents our proposed
bootstrap bias corrected RD algorithm and discusses its asymptotic
properties. Simulation evidence that the bootstrap procedure provides valid CIs
and its relative performance to the analytical bias correction are presented in
Section~\ref{sim}.
Finally, Section~\ref{conclusion} concludes.

\section{Background}\label{background}

This section provides additional details of RD estimators in general and of
CCT's proposed bias correction. It also defines some of the notation and
presents the assumptions that will be used for our theoretical analysis in
Section~\ref{boot}.

In the typical sharp RD setting, a researcher wishes to estimate the local
causal effect of treatment at a given threshold. A running variable, $X_{i}$,
determines treatment assignment.  Given a known threshold, $\bar{x}$, which can
be set to zero without loss of generality, the $i$th subject receives the
treatment of interest if $X_{i} \geq 0$ and does not receive treatment if
$X_{i} < 0$.

Subject $i$'s potential outcomes are denoted by the variable $Y_i(\cdot)$;
$Y_i(1)$ indicates that the subject has received treatment and $Y_i(0)$
indicates that the subject has not received treatment. Since only one of the two
outcomes is observed, the sample is comprised of the running variable, $X_{i}$,
and the observed outcome $Y_i$, where
\begin{equation*}
  Y_{i}=Y_{i}(0) \1\{X_{i}<0\}+Y_{i}(1) \1\{X_{i} \geq 0\}
\end{equation*}
and $\1\{ \cdot\}$ denotes the indicator function.

In most cases the population parameter of interest is the Average Treatment
Effect (ATE) at the cutoff, which we will denote $\tau$. This parameter is the
difference in expected potential outcomes given $X_i = 0$; formally,
\begin{equation*}
  \tau=\E(Y(1)-Y(0) \mid X=0).
\end{equation*}
\cite{HTV2001} show that the effect $\tau$ is identified under continuity and
smoothness conditions on the joint distribution of $X_i$, $Y_i(0)$, and $Y_i(1)$
around the cutoff $X_i = 0$. Under these conditions, which are made precise
in our Assumption~\ref{A1}, $\tau$ is equal to
\begin{equation*}
  \tau = \lim_{x \to 0+} \mu(x) - \lim_{x \to 0-}(x)
\end{equation*}
where
\begin{equation*}
  \mu(x)= \E(Y_{i} \mid X_{i}=x).
\end{equation*}
For convenience, also define
\begin{equation*}
  \mu^{(\eta)}(x)=\frac{d^{\eta}\mu(x)}{dx^{\eta}}
\end{equation*}
and let
\begin{align*}
  \mu_{+}(x)
  &= \E( Y_{i}(1) \mid X_{i}=x )
  &\mu_{-}(x)
  &= \E( Y_{i}(0) \mid X_{i}=x ) \\
  \sigma^{2}_{+}(x) &= \V( Y_{i}(1) \mid X_{i}=x )
  &\sigma^{2}_{-}(x)&=\V( Y_{i}(0) \mid X_{i}=x ) \\
\intertext{and}
  \mu^{(\eta)}_{+}
  &= \lim_{x \rightarrow 0^{+}}\mu^{(\eta)}(x),
  &\mu^{(\eta)}_{-}
  &= \lim_{x \rightarrow 0^{-}}\mu^{(\eta)}(x),
\end{align*}
where the symbol $\V(\cdot)$ represents the variance.  The effect $\tau$ is
nonparametrically identified because both $\mu_-$ and $\mu_+$ can be estimated
consistently under Assumption~\ref{A1}, which lists standard conditions in the
RD literature. (See, in particular, \citealp{HTV2001}, \citealp{Porter03}, and
CCT.)

\begin{assumption}[Behavior of the DGP near the cutoff]\label{A1}
  The random variables $Y_i, X_i$ form a random sample of size $n$.
  There exists a positive number $\kappa_0$ such that the following
  conditions hold for all $x$ in the neighborhood $(-\kappa_{0},\kappa_{0})$
  around zero:
  \begin{enumerate}
  \item The density of each $X_i$ is continuous and bounded away from zero.
  \item $\E(Y_{i}^{4} \mid X_{i}=x)$ is bounded.
  \item $\mu_+(x)$ and $\mu_-(x)$ are both 3 times continuously differentiable.
  \item $\sigma_+^2(x)$ and $\sigma_-^2(x)$ are both continuous and bounded away
    from zero.
 \end{enumerate}
\end{assumption}

Since the conditions for identification only need to hold in a neighborhood
around the cutoff, $\mu_+$ and $\mu_-$ can be estimated by extrapolating from a
local polynomial regression.  We will focus here on local linear
regression.\footnote{%
  See \cite{HTV2001}, \cite{Porter03} or \cite{FanGijbels92} for discussions of
  the properties of local polynomial regressions for boundary problems. The
  bootstrap algorithm proposed in this paper can be extended to accommodate
  higher order polynomials discontinuities in the derivatives of the conditional
  expectation, like ``Kink RD'' design \citep{card2009b}.} %
For this model, if $h$ represents a bandwidth parameter, the estimator of
$\tau$, $\hat\tau(h)$, is defined as
\begin{equation*}
  \hat{\tau}(h) = \hat {\mu}_{+}(h) -\hat{\mu}_{-}(h)
\end{equation*}
with
\begin{align*}
  (\hat {\mu}_{+}(h),\ \hat {\mu}^{(1)}_{+}(h))'
  &= \argmin_{\mu, \beta} \sum_{i=1}^{n}
  \1\{h > X_{i} \geq 0\} (Y_{i} - \mu - X_{i} \beta)^{2}
\intertext{and}
  (\hat {\mu}_{-}(h),\hat {\mu}^{(1)}_{-}(h))'
  &= \argmin_{\mu,\beta} \sum_{i=1}^{n}
  \1\{0 > X_{i} > -h \} (Y_{i} - \mu - X_{i} \beta)^{2}.
\end{align*}
Conventional (naive) confidence intervals can be calculated by using an
asymptotic approximation for $\hat\tau(h)$. In particular, if
\begin{gather}
  \label{eq:1}
  \frac{\hat{\tau}(h)-\tau}{\sqrt{V(h)}} \to^d N(0,1),
  \intertext{with}
  \notag
  V(h) \big/ \V(\hat\tau(h) \mid X_{1},\dots,X_{n}) \to^p 1
\end{gather}
then valid confidence intervals can be constructed through the usual method of
inverting the $t$-test.\footnote{%
  The mathematical appendix of this paper gives a precise definition for
  $V(h)$.} %
This procedure gives the widely-used interval estimator
\begin{equation*}
  \hat{\tau}(h) \pm q_{1-\alpha/2} V(h)^{1/2}
\end{equation*}
where $q_{1 - \alpha/2}$ is the $1 - \alpha/2$ quantile of the standard normal
distribution.

The statistical properties of these estimators, however, clearly depend on the
bandwidth parameter $h$, and bandwidths that have desirable properties for point
estimation may not have desirable properties for hypothesis testing or interval
estimation. In particular, for~\eqref{eq:1} to hold, $h$ must satisfy
$n h \to \infty$ and $n h^5 \to 0$. (\citealp{HTV2001}; \citealp{Porter03})
Otherwise, the finite-sample bias of $\hat\tau(h)$ does not converge in
probability to zero quickly enough and it contributes non-negligibly to the
asymptotic distribution in~\eqref{eq:1}. This result holds even though
$\hat\tau(h)$ can be consistent under those conditions.  These bandwidth issues
are relevant in practice because many widely-used bandwidth selection
procedures, most notably the AMSE-optimal bandwidth and cross-validation
bandwidth \citep{IK} do not produce $o_{p}(n^{-1/5})$ bandwidths.

CCT solve this problem by deriving the analytical form of the first-order bias
and explicitly recentering $\hat\tau(h)$. Under weaker assumptions on the
asymptotic behavior of the bandwidth, which we will specify in
Assumption~\ref{A2}, CCT show that the approximate bias of $\hat\tau(h)$ has the
form
\begin{equation*}
  \E(\hat{\tau}(h) \mid X_1,\dots,X_n) - \tau =
  h^{2}\Big[ \tfrac{\mu_{+}^{(2)}}{2}\Bf_{+}(h)
  - \tfrac{\mu_{-}^{(2)}}{2}\Bf_{-}(h) \Big]
  (1+o_{p}(1))
\end{equation*}
where $\Bf_{+}(h)$ and $\Bf_{-}(h)$ are observed quantities that depend on the
kernel, bandwidth, and running variables $X_1,\dots,X_n$; formal definitions of
these terms are given in the Mathematical Appendix as Equations~\eqref{eq:5}
and~\eqref{eq:6}. The plug-in bias-corrected estimator then requires estimates
for the second derivatives of the conditional mean from above and below the
cutoff, $\mu_{+}^{(2)}$ and $\mu_{-}^{(2)}$, and CCT show that these derivatives
can be estimated by fitting a second order local polynomial, i.e. one order
higher than the polynomial used to obtain $\hat{\tau}$, using a (potentially)
different pilot bandwidth $b$. Their procedure gives the bias-corrected
estimator
\begin{gather*}
  \hat{\tau}'(h, b) = \hat{\tau}(h) - h^{2}
  \Big[\tfrac{\hat{\mu}_{+}^{(2)}(b)}{2} \Bf_{+}(h)
  - \tfrac{\hat{\mu}_{-}^{(2)}(b)}{2}\Bf_{-}(h) \Big]
\end{gather*}
The variance introduced by the bias-correction term does not vanish, so the
naive confidence interval needs not only to be re-centered to correct the bias,
but also rescaled to allow for the additional variability introduced by the bias
correction, resulting in the following asymptotic approximation:
\begin{equation*}
  \frac{\hat{\tau}'(h, b) - \tau}{V'(h, b)^{1/2}} \to^d N(0,1)
\end{equation*}
with $V'(h, b) = V(h) + C(h, b)$ and $C(h, b)$ an additional variance component
generated by the bias-correction term.\footnote{%
  $C(h,b)$ and $V(h)$ are defined precisely in the mathematical appendix.} %
This new approximation can be used
instead of~\eqref{eq:1} to construct ``bandwidth robust'' confidence intervals,
and CCT provide simulation evidence that their intervals perform well in finite
samples even when the naive interval performs badly.

Assumption~\ref{A2} specifies the bandwidth conditions assumed by CCT, which we
will also use in this paper.
\begin{assumption}[Bandwidth]\label{A2}
  Let $h$ be the bandwidth used to estimate the local linear model and let
  $b$ be the bandwidth used to estimate a second local quadratic model. Then
  $n h \to \infty$, $n b \to \infty$, $n h^{5} b^{2} \to 0$, and
  $n b^{5} h^{2} \to 0$ as $n \to \infty$.\footnote{%
    Unless otherwise stated, all limits in this paper are assumed to hold as
    $n \to \infty$.} %
  The relationship $h \leq b$ also holds for all $n$.
\end{assumption}

In the next section, we build upon the insight provided by CCT bias-corrected
estimator and propose a simple bootstrap procedure that can directly construct
the robust CIs without requiring the derivation of analytical formulas and
direct estimators for the bias, variance and covariance terms, while relying on
the same first-order bias correction approximation. The requirement $h < b$ is
one additional restriction that we impose in this paper because the bootstrap
can not be implemented for the uniform kernel without it, but the other parts of
Assumption~\ref{A2} are identical to CCT.\footnote{%
  This assumption can be relaxed by considering other kernels, which is the
  subject of current research by the authors.} %

\section{Bootstrap Bias Correction}\label{boot}

This section presents our theoretical contributions. We propose two algorithms
in this section. The first uses a residual bootstrap based on a second
order local polynomial to estimate the bias of the local linear model. That
estimate can be subtracted from the biased original estimator to provide an
asymptotically unbiased estimator with the same asymptotic distribution as
CCT's. As in CCT, this bias correction term introduces a new source of variance,
invalidating standard (naive) critical values. Consequently, the second
algorithm we propose an iterated bootstrap to estimate the correct
critical values of the bias corrected estimator.

The intuition behind both procedures is straightforward. CCT show that a
local second order polynomial captures the aspects of the DGP
necessary for constructing valid confidence intervals. Our proposed algorithms
estimate and embed the second order behavior in the bootstrap DGP through a
residual bootstrap. Throughout this section and the rest of the paper, we will
let $\E^{*}$, $\Pr^{*}$, etc. denote expectations and probabilities taken with
respect to the distribution induced by the bootstrap (which implicitly
conditions on $X_{1},\dots,X_{n}$ and $Y_{1},\dots,Y_{n}$) and let parameters with
$^{*}$ superscripts be the parameter values under the distribution induced by
the bootstrap. Two $*$ superscripts indicate that the parameter or probability
measure corresponds to a secondary bootstrap distribution.

Algorithm~\ref{Alg1} explains the bias-correction steps in detail.

\begin{algorithm}[Bias estimation]\label{Alg1}
  Assume $h$ and $b$ are bandwidths as defined by Assumption~\ref{A2} and define
  \begin{align*}
    I_{-}(h) &= \{i : -h < X_{i} < 0\}, &
    I_{+}(h) &= \{i : 0 \leq X_{i} < h\}.
  \end{align*}
  Also define $M_{-}(h)$ and $M_{+}(h)$ to be the number of elements in $I_{-}(h)$ and
  $I_{+}(h)$ respectively, and $m_{-}(h,1),\dots,m_{-}(h,M_{-}(h))$ and
  $m_{+}(h,1),\dots,m_{+}(h,M_+(h))$ to be subsequences of $1,\dots,n$ that index
  $I_{-}(h)$ and $I_{+}(h)$, respectively.
  \begin{enumerate}
  \item Estimate local second order polynomials $\hat g_{-}$ and $\hat g_{+}$ using
    the observations in $I_{-}(b)$ and $I_{+}(b)$:
    \begin{align}
      \label{eq:2}
      \hat g_{-}(x)
      &= \hat\beta_{-,0} + \hat\beta_{-,1} x + \hat\beta_{-,2} x^{2},
      &\hat g_{+}(x)
      &= \hat\beta_{+,0} + \hat\beta_{+,1} x + \hat\beta_{+,2} x^{2}
    \end{align}
    with
    \begin{align*}
      \hat\beta_{-} &= \argmin_\beta \sum_{i \in I_-(b)}
      (Y_i - \beta_0 - \beta_1 X_i - \beta_2 X_i^2)^2 \\
      \hat\beta_{+} &= \argmin_\beta \sum_{i \in I_+(b)}
      (Y_i - \beta_0 - \beta_1 X_i - \beta_2 X_i^2)^2.
    \end{align*}
    Calculate the residuals
    \begin{equation}
      \label{eq:3}
      \hat\varepsilon_{i} =
      \begin{cases}
        Y_{i} - \hat g_-(X_{i}) & \textif\ X_{i} < 0 \\
        Y_{i} - \hat g_+(X_{i}) & \otherwise.
      \end{cases}
    \end{equation}
  \item Repeat the following steps $B_{1}$ times to produce the
    bootstrap estimates $\hat{\tau}_{1}^{*}(h),\dots,\hat\tau_{B_{1}}^{*}(h)$. For the
    $k$th value:
    \begin{enumerate}
    \item Draw an i.i.d. sample of size $M_-(b)$ from
      $\{\hat\varepsilon_i : i \in I_-(b)\}$ and one of size $M_+(b)$ from
      $\{\hat\varepsilon_i : i \in I_+(h)\}$. Let $\varepsilon_{-,i}^{*}$ and
      $\varepsilon_{+,i}^{*}$ denote the $i$th element of each sample and
      construct
      \begin{align*}
        Y_{-,m_-(h,i)}^* &= \hat g_-(X_{m_-(h,i)}) + \varepsilon_{-,i}^{*} &
        Y_{+,m_+(h,i)}^* &= \hat g_+(X_{m_-(h,i)}) + \varepsilon_{+,i}^{*}.
      \end{align*}
    \item Calculate $\hat\mu_+^*(h)$ and $\hat\mu_-^*(h)$ by estimating the
      local linear model on the bootstrap data set:\footnote{%
        Note that the indices of summation are chosen to correspond to the
        indices of the variables generated in the previous step.} %
      \begin{align*}
        \hat\mu_-^*(h)
        &= \argmin_{\mu} \min_{\beta} \ssum[-]{i}{h}
          (Y_i^* - \mu - \beta X_i^*)^2 \\
        \hat\mu_+^*(h)
        &= \argmin_{\mu} \min_{\beta} \ssum[+]{i}{h}
          (Y_i^* - \mu - \beta X_i^*)^2.
      \end{align*}
    \item Save $\hat\tau^*_k(h) = \hat\mu_+^*(h) - \hat\mu_-^*(h)$.
    \end{enumerate}
  \item Estimate the bias as
    \begin{equation*}
      \Delta^*(h,b) = \tfrac{1}{B_1} \sum_{k=1}^{B_1} \hat\tau^*_k(h) -
      \big[\hat g_+(0) - \hat g_-(0)\big].
    \end{equation*}
  \end{enumerate}
\end{algorithm}
Note that $\hat g_+(0) - \hat g_-(0)$ is the true treatment effect under the
distribution induced by this bootstrap. The bootstrap estimator works by
constructing an approximate DGP with known properties. As the dataset gets
larger, the approximate DGP mimics the unknown real DGP more closely, and the
population parameter values in the bootstrap DGP can become accurate estimates
of the true parameter values in the real DGP.

Under Assumptions~\ref{A1} and~\ref{A2} the procedure described by
Algorithm~\ref{Alg1} provides a consistent estimator of the bias component that
converges fast enough in probability that it can be be used as a correction. As
is standard in the bootstrap literature, we will assume that the number of
bootstrap replications, $B_{1}$, is large enough that the simulation error can
be ignored. Theorem~\ref{T1} presents the result formally.

\begin{theorem}\label{T1}
  Under Assumptions~\ref{A1} and~\ref{A2},
\begin{equation}
  \label{eq:4}
  \frac{(\hat\tau(h) - \Delta^{*}(h,b) - \tau)}{ V'(h, b)^{1/2}}
  \to^{d} N(0,1)
\end{equation}
\end{theorem}

Note that the variance component of~\eqref{eq:4} is the same value used by CCT
and introduced in~\eqref{eq:1}. Theorem~\ref{T1} implies that our bias-corrected
estimator has the same asymptotic distribution as CCT's. This equivalence should
be unsurprising; both estimators use a second order polynomial to directly estimate
the bias of the local linear model, so they should behave very similarly.

Since the second order polynomial captures the relevant aspects of the DGP for
estimating the variance as well as the bias, the asymptotic distribution of the
bias corrected estimator $\hat\tau(h) - \Delta^*(h,b)$ can also be approximated
with a bootstrap. We propose bootstrapping $\hat\tau(h) - \Delta^*(h,b)$ using
the same residual bootstrap method used in
Algorithm~\ref{Alg1}. Algorithm~\ref{Alg2} provides the details of our procedure
and Theorem~\ref{T2} establishes its theoretical properties.

\begin{algorithm}[Confidence intervals]\label{Alg2}
  Define the same notation as in Algorithm~\ref{Alg1}.
  \begin{enumerate}
  \item Estimate $\hat g_+$ and $g_-$ and generate the residuals
    $\hat\varepsilon_i$ just as in Algorithm~\ref{Alg1}; Equations~\eqref{eq:2}
    and~\eqref{eq:3}.
  \item Repeat the following steps $B_{2}$ times to produce the
    bootstrap estimates
    $\hat\tau_1^{\prime*}(h,b), \dots, \hat\tau_{B_{2}}^{\prime*}(h,b)$. For the
    $k$th value:
    \begin{enumerate}
    \item Draw an i.i.d. sample of size $M_-(b)$ from
      $\{\hat\varepsilon_i : i \in I_-(b)\}$ and one of size $M_+(b)$ from
      $\{\hat\varepsilon_i : i \in I_+(h)\}$. Let $\varepsilon_{-,i}^{*}$ and
      $\varepsilon_{+,i}^{*}$ denote the $i$th element of each sample and
      construct
      \begin{align*}
        Y_{-,m_-(h,i)}^* &= \hat g_-(X_{m_-(h,i)}) + \varepsilon_{-,i}^{*} &
        Y_{+,m_+(h,i)}^* &= \hat g_+(X_{m_{+}(h,i)}) + \varepsilon_{+,i}^{*}.
      \end{align*}
    \item Calculate $\hat\mu_+^*(h)$ and $\hat\mu_-^*(h)$ by estimating the
      local linear model on the bootstrap data set:
      \begin{align*}
        \hat\mu_-^*(h)
        &= \argmin_{\mu} \min_{\beta} \ssum[-]{i}{h}
          (Y_i^* - \mu - \beta X_i^*)^2 \\
        \hat\mu_+^*(h)
        &= \argmin_{\mu} \min_{\beta} \ssum[+]{i}{h}
          (Y_i^* - \mu - \beta X_i^*)^2.
      \end{align*}
    \item Apply Algorithm~\ref{Alg1} to the bootstrapped data set,
      \begin{multline*}
        (Y_{-,m_-(b,1)}^{*}, X_{-,m_-(b,1)}),\dots,
        (Y_{-,m_-(b,M_-(b))}^{*},X_{-,m_-(b,M_-(b))}), \\
        (Y_{+,m_+(b,1)}^{*}, X_{+,m_+(b,1)}),\dots,
        (Y_{+,m_+(b,M_+(b))}^{*},X_{+,m_+(b,M_+(b))})
      \end{multline*}
      using the same bandwidths $h$ and $b$ that are used in the rest of this
      algorithm but reestimating all of the local polynomials on the bootstrap
      data. Generate $B_1$ new bootstrap samples and let $\Delta^{**}(h,b)$
      represent the bias estimator returned by Algorithm~\ref{Alg1}.
    \item Save the bias-corrected estimator
      $\hat\tau_k^{\prime*}(h,b) = \hat\mu_+^*(h) - \hat\mu_i^*(h)
      - \Delta^{**}(h,b)$.
    \end{enumerate}
  \item Use the empirical CDF of
    $\hat\tau_1^{\prime*}(h,b),\dots \hat\tau_{B_2}^{\prime*}(h,b)$ to
    construct confidence intervals, etc.
  \end{enumerate}
\end{algorithm}


Theorem~\ref{T2} establishes that this iterated bootstrap approximates the
asymptotic distribution of the bias-corrected statistic proposed by
Algorithm~\ref{Alg1} and justifies this second algorithm.  As before, we assume
that $B_1$ and $B_2$ are large enough that simulation error can be ignored.

\begin{theorem}\label{T2}
  Under Assumptions~\ref{A1} and~\ref{A2},
  \begin{gather*}
    \V^*(\hat\tau^{*}(h) - \Delta^{**}(h,b))/V'(h,b) \to^p 1
  \intertext{and}
    \sup_{x}
    \Big\rvert \Pr^*[\hat\tau^{*}(h) - \Delta^{**}(h,b) - \tau^* \leq x ]
    - \Pr[\hat\tau(h) - \Delta^*(h,b) - \tau \leq x] \Big\lvert \to^p 0.
  \end{gather*}
\end{theorem}

Evidence of the usefulness of the procedures proposed above and their relative performance to the analytical bias correction proposed in CCT are presented in a series of Monte Carlo simulations in Section \ref{sim}.

\section{Simulation Evidence}\label{sim}

This section presents evidence from Monte Carlo simulations that the bootstrap
procedures proposed in Section~\ref{boot} produce valid, robust confidence
intervals similar to the ones obtained by the analytical procedures established
in CCT. The bootstrap CIs obtained compare favorably to the analytical
alternative, with coverage slightly closer to nominal coverage and shorter
length of the intervals in the specifications implemented.\footnote{%
  All of the simulations were carried out in the R programming language
  \citep{R} and rely on the \textit{rdrobust} \citep{rdrobust}, \textit{foreach}
  \citep{foreach}, \textit{doParallel} \citep{doparallel}, and \textit{doRNG}
  \citep{dorng} packages.} %

The Monte Carlo experiments have a similar structure. For all of them, we
generate 500 i.i.d.\ observations from the DGP
\begin{align*}
Y_{i}           &= \mu_{j}(X_{i}) + \varepsilon_{i} \\
X_{i}           &\sim  2 \times \betarv(2,4) - 1 \\
\varepsilon_{i} &\sim \normal(0, 0.1295^2),
\end{align*}
where $j$ will index the specific DGP. This is the experimental design used by
\citet{IK} and CCT, which we adopt here to make our simulation results directly
comparable with theirs and the rest of the literature. We
will use the same three functional forms for $\mu_{j}$ as CCT as well.

\nocite{lee2008rand}%
The first DGP is designed to match features of Lee's (2008) analysis of U.S.\
congressional elections. Lee estimates the incumbency advantage in electoral
races for the House of Representatives --- candidates who received the largest
vote share in the previous election are the incumbents, which creates the
discontinuity. The conditional expectation is a fifth order polynomial fit to
that dataset, (after excluding a small number of extreme observations; see
\citealp{IK}, or CCT for further details) giving
\begin{equation*}
  \mu_{1}(x) =
  \begin{cases}
    0.48 + 1.27x + 7.18x^{2} + 20.21x^{3} + 21.54x^{4} + 7.33x^{5}
    & \textif\ x < 0 \\
    0.52 + 0.84x - 3.00x^{2} + 7.99x^3 - 9.01x^4 + 3.56x^{5}
    & \otherwise.
  \end{cases}
\end{equation*}
The population ATE for this DGP is $0.04$ $(= 0.52 - 0.48)$.

\nocite{ludwig2007}%
The second DGP is based on Ludwig and Miller's (2007) analysis of the Head Start
program. Funding eligibility is determined at the county level using the
county's historical poverty rate, with a sharp threshold that determines the
provision of services. We use the fifth order polynomial estimated on Ludwig and
Miller's dataset as the conditional expectation for the second DGP:
\begin{equation*}
  \mu_{2}(x) =
  \begin{cases}
    3.71 + 2.30x + 3.28x^2 + 1.45x^3 + 0.23x^4 + 0.03x^5
    & \textif\ x < 0, \\
    0.26 + 18.49x - 54.81x^2 + 74.30x^3 - 45.02x^4 + 9.83x^5
    & \otherwise
  \end{cases}
\end{equation*}
and the population ATE is $-3.45$ $(= 0.26 - 3.71)$.

Finally, for the third DGP, we use CCT's modification of $\mu_1$, given by
\begin{equation*}
  \mu_{3}(x) =
  \begin{cases}
    0.48 + 1.27x + 3.59 x^{2} + 14.147 x^3 + 23.694 x^4 + 10.995 x^5
    & \textif\ x < 0 \\
    0.52 + 0.84x - 0.30 x^{2} + 2.397 x^3 - 0.901 x^4 + 3.56 x^5
    & \otherwise
\end{cases}
\end{equation*}
and the population ATE is again $0.04$. CCT introduce this DGP because it has
high curvature and local linear models are likely to exhibit high bias, making
it a natural test case for both their analytical corrections and our bootstrap.

To estimate the finite-sample coverage of our new bootstrap based confidence interval, we
simulate 1500 samples from each of the three DGPs and calculate nominal 95\%
two-sided confidence intervals. We use 999 bootstrap replications ($B_2$) to
calculate the asymptotic distribution of the bias corrected estimator, and each
of those replications uses an additional 500 replications ($B_1$) to estimate
the bias. We use the $0.025$ and $0.975$ quantiles of the bootstrap distribution
as the lower and upper bound of the interval, and the bandwidths, $h$ and $b$,
are chosen using the AMSE-optimal rule proposed by CCT.

We also show the coverage of two of CCT's interval estimators for comparison.
We estimate their bias corrected procedure with AMSE-optimal bandwidths for both
the uniform kernel and the triangular kernel. The uniform kernel provides a
direct comparison with our bootstrap, while the triangular kernel may be more
appealing to practitioners. Since CCT establish conclusively that the ``naive''
uncorrected interval estimator has poor coverage in these settings we do not
report results for that estimator.

\begin{table}[t]
  \centering
  \begin{tabular}{rlrrrrrrr}
    \toprule
    DGP & Method     & Bias   & SD    & RMSE   & CI Coverage (\%) & CI Length \\
    \midrule
    1   & \bootuni   & --0.018 & 0.066 & 0.069 & 93.1       & 0.240     \\
        & \cctuni    & --0.016 & 0.067 & 0.069 & 92.4       & 0.246     \\
        & \ccttri    & --0.010 & 0.068 & 0.068 & 91.1       & 0.241     \\\\
    2   & \bootuni   & --0.012 & 0.088 & 0.089 & 95.3       & 0.324     \\
        & \cctuni    & --0.010 & 0.087 & 0.088 & 93.8       & 0.350     \\
        & \ccttri    & --0.011 & 0.085 & 0.086 & 93.5       & 0.343     \\\\
    3   & \bootuni   & --0.004 & 0.064 & 0.064 & 96.0       & 0.246     \\
        & \cctuni    & --0.004 & 0.065 & 0.065 & 94.3       & 0.250     \\
        & \ccttri    & --0.008 & 0.067 & 0.067 & 91.9       & 0.246     \\
    \bottomrule
  \end{tabular}
  \caption{%
    Experimental coverage probabilities for each interval estimator based on 1500
    simulations; nominal coverage probabilities are 95\% for each estimator. The column
    ``CI Coverage'' lists the coverage frequency in these simulations and ``CI Length''
    lists the average length of the confidence interval across simulations.}
  \label{tbl:1}
\end{table}

Table~\ref{tbl:1} presents the results of these simulations. The rows labeled
``\bootuni'' show results for our proposed bootstrap interval; ``\cctuni'' and
``\ccttri'' show results for the analytically-corrected intervals. The first
three columns give the bias, standard deviation, and Root-MSE of the \emph{bias corrected} point
estimators corresponding to each confidence interval.  As expected, these
estimators are close to unbiased, with their standard deviation largely
determining the estimators' RMSE. Moreover, as Theorem~\ref{T1} suggests, the
bootstrap and analytically-corrected estimators have essentially the same
standard deviation across all of the DGPs.

The column ``CI Coverage'' lists the experimental coverage of each interval.
All of the intervals are reasonably close to their nominal coverage, although
the analytically corrected estimator using the triangular kernel seems to
persistently under-cover by a few percentage points. Our proposed bootstrap
procedure consistently performs well, as it is about a percentage point closer
to nominal coverage than the other interval estimators in DGPs 1 and 2, and is
slightly conservative (96\% coverage) in DGP 3. The final column, ``CI Length,''
indicates that all three of these procedures generate intervals that are
approximately the same length on average.

It is important not to read too much into these simulation results. Where there
are differences between these procedures, they are small enough that randomness
in the simulations is likely a factor in their precise rankings.\footnote{%
  To help address this concern, we are currently running a more extensive set of
  simulations and will update this paper when they are complete.} %
These are DGPs where the naive confidence intervals are known to perform poorly,
and this set of simulation results indicates that our proposed bootstrap approach
is quite competitive with analytical methods for producing valid intervals.
Overall, the bootstrap bias-correction procedure proposed in this paper provides
a simple alternative to obtain valid robust confidence intervals in RD designs
and performs well compared to the analytical bias correction procedures proposed
by CCT.

\section{Conclusion}\label{conclusion}
This paper proposes a novel bootstrap procedure to obtain robust bias-corrected confidence intervals in regression discontinuity designs.
The approach proposed builds upon the developments and intuition advanced by CCT and is based on a first-order bias correction.
 We exploit CCT's theoretical insight through a new residual bootstrap. In particular, we propose estimating the local linear model as usual,
then estimating a local second order polynomial and generating bootstrap datasets by resampling the residuals of that polynomial.

The bootstrap sample is used to estimate the bias of the RD estimator, which is then removed from the original estimator.
 This approach is described in detail by our Algorithm~\ref{Alg1} and the resulting bias corrected estimator is shown to be asymptotically normal with mean zero in our
Theorem~\ref{T1} under AMSE-optimal bandwidth rates.

Just as in CCT, our bias correction step introduces additional variability.  So we propose then bootstrap the bias-corrected estimator itself to
 generate valid confidence intervals. This procedure, which requires bootstrapping the datasets produced by an initial bootstrap procedure, is described in Algorithm~\ref{Alg2},
 and the resulting confidence intervals are shown to be asymptotically valid in Theorem~\ref{T2}.

 Hence, our procedure provides an alternative to the plug-in analytical methods in the literature and is simple to implement generating robust confidence intervals.

Simulation evidence is presented that the proposed bootstrap bias correction and confidence intervals have similar empirical coverage and average interval length relative to
 the analytical alternatives proposed by CCT, making it a suitable option to practitioners.

\newpage
\appendix
\section{Mathematical appendix}
Let $e_p$ be the selection vector with 1 in element $p+1$ and 0
everywhere else. Define the following additional notation (largely
introduced by CCT): $r_p(x) = (1,\ x,\dots,\ x^p)'$,
\begin{align*}
  (\hat{\mu}_{+}(h),\ \hat{\mu}_{+}^{(1)}(h),\ \hat\mu_{+}^{(2)}(h))'
  &= \argmin_{\mu_0, \mu_1, \mu_2} \ssum[+]{i}{h}
    (Y_i - \mu_0 - \mu_1 X_i/h - \mu_2 (X_i/h)^2) \\
  (\hat{\mu}_{-}(h),\ \hat{\mu}_{-}^{(1)}(h),\ \hat\mu_{-}^{(2)}(h))'
  &= \argmin_{\mu_0, \mu_1, \mu_2} \ssum[-]{i}{h}
    (Y_i - \mu_0 - \mu_1 X_i/h - \mu_2 (X_i/h)^2)
\end{align*}
\begin{align}
  \notag
  \Gamma_{+,p}(h) &= \tfrac{1}{nh} \ssum[+]{i}{h} r_p(X_i/h) r_p(X_i/h)' \\
  \notag
  \Gamma_{-,p}(h) &= \tfrac{1}{nh} \ssum[-]{i}{h} r_p(X_i/h) r_p(X_i/h)' \\
  \notag
  \Psi_{+,p,q}(h,b)
  &= \tfrac{1}{nhb} \ssum[+]{i}{\min(h,b)} r_p(X_i/h) r_q(X_i/b)'
    \V(Y_i \mid X_i) \\
  \notag
  \Psi_{-,p,q}(h,b)
  &= \tfrac{1}{nhb} \ssum[-]{i}{\min(h,b)} r_p(X_i/h) r_q(X_i/b)'
    \V(Y_i \mid X_i) \\
  \label{eq:5}
  \Bf_+(h)
  & = e_0' \Gamma_{+,1}^{-1}
    \ssum[+]{i}{h} r_1(X_i/h) \; X_i^2 / h^2, \\
  \label{eq:6}
  \Bf_-(h)
  & = e_0' \Gamma_{-,1}^{-1}
    \ssum[-]{i}{h} r_1(X_i/h) \; X_i^2 / h^2.
\end{align}

Also, for reference, define the variance terms
\begin{equation*}
  V(h)
  = e_0' \Big(\tfrac{1}{n} \Gamma_{-,1}^{-1} \Psi_{-,1} \Gamma_{-,1}^{-1}
  + \tfrac{1}{n} \Gamma_{+,1}^{-1} \Psi_{+,1} \Gamma_{+,1}^{-1}\Big) e_0
\end{equation*}
and
\begin{multline*}
  C(h,b) =
  n^{-1} e_2' \Big[
    \Gamma_{+,2}^{-1}(b) \Psi_{+,2,2}(b,b) \Gamma_{+,2}^{-1}(b) \Bf^2_+(h) +
    \Gamma_{-,2}^{-1}(b) \Psi_{-,2,2}(b,b) \Gamma_{-,2}^{-1}(b) \Bf^2_-(h)
    \Big] e_2 \\
  - 2 h^2 n^{-1} b^{-2}
    e_0' \Big[
    \Gamma^{-1}_{+,1}(h) \Psi_{+,1,2}(h,b) \Gamma_{+,2}^{-1}(b) \Bf_+(h) +
    \Gamma^{-1}_{-,1}(h) \Psi_{-,1,2}(h,b) \Gamma_{-,2}^{-1}(b) \Bf_-(h)
    \Big] e_2.
\end{multline*}
See CCT for the derivation and justification of these formulas.

\subsection{Proof of Theorem~\ref{T1}}
We have
\begin{equation*}
  \hat\tau(h) - \Delta^*(h,b) - \tau = (\hat\tau(h) - \E \hat \tau(h)) +
  (\E \hat \tau(h) - \tau) - (\E^* \hat \tau^*(h) - \tau^*).
\end{equation*}
The design of the bootstrap ensures that
\begin{align*}
  \E^* \hat\mu_{+}^*(h) - \mu_{+}^{*}
  &= h^{2} \mu_+^{*(2)} \Bf_{+}(h)/2,
  &\E^* \hat\mu_{-}^*(h) - \mu_{-}^{*}
  &= h^{2} \mu_-^{*(2)} \Bf_{-}(h)/2,
\end{align*}
almost surely, implying that
\begin{equation}
  \label{eq:7}
  \E^* \hat\tau^*(h) - \tau^{*} = h^2\, \mu_+^{*(2)} \Bf_+(h)/2
  - h^2\, \mu_{-}^{*(2)} \Bf_{-}(h)/2.
\end{equation}
Combining this result with CCT's Lemma A1, gives
\begin{align}
  \hat\tau(h) - & \E \hat\tau(h) + (\E \hat\tau(h) - \tau) - (\E^* \hat\tau^*(h) - \tau^*)\notag \\
  &= \hat\tau(h) - \E\hat\tau(h)
   + h^2 \big((\mu_{-}^{*(2)} - \mu_{-}^{(2)}) \Bf_{-}(h) /2
   - (\mu_+^{*(2)}-\mu_+^{(2)}) \Bf_{+}(h) / 2 \big) + O_p(h^{3}) \notag \\
  \label{eq:8}
  &= \hat\tau(h) - \E\hat\tau(h)
   + h^2 (\hat\mu_-^{(2)}(b) - \mu_-^{(2)}) \Bf_{-}(h) /2 \\
  \notag
  &\pushright{- h^2 (\hat\mu_+^{(2)}(b) - \mu_+^{(2)}) \Bf_{+}(h) / 2 + O_p(h^{3})}
\end{align}
The second equality holds because
$\mu_+^{*(2)} = \hat\mu_{+}^{(2)}(b)$ and
$\mu_-^{*(2)} = \hat\mu_{-}^{(2)}(b)$ almost surely. Asymptotic
normality then follows from normality of $\hat\tau(h) - \E\hat\tau(h)$,
$\hat\mu_+^{(2)}(b) - \mu_+^{(2)}$, and
$\hat\mu_-^{(2)}(b) - \mu_-^{(2)}$ using similar arguments to
CCT's Lemma SA4.D.\qed

\subsection{Proof of Theorem~\ref{T2}}
Repeat the steps from Theorem~\ref{T1}'s proof through~\eqref{eq:8} to get
\begin{align*}
  \hat\tau(h)^* - &\Delta^{**}(h,b) - \tau^*
  = (\hat\tau^*(h) - \E^* \hat \tau^*(h)) +
  (\E^* \hat \tau^*(h) - \tau^*) - (\E^{**} \hat \tau^{**}(h) - \tau^{**}) \\
  &= \hat\tau^*(h) - \E^*\hat\tau^*(h)
   + h^2 (\hat\mu_-^{*(2)}(b) - \mu_-^{*(2)}) \Bf_{-}(h) /2
   - h^2 (\hat\mu_+^{*(2)}(b) - \mu_+^{*(2)}) \Bf_{+}(h) / 2 \\
  &= \Omega_{+}(h,b)' \epb_{+}^{*} - \Omega_{-}(h,b)' \epb_{-}^{*},
\end{align*}
where
$\epb_{+}^{*} =
(\varepsilon_{+,1}^{*},\dots,\varepsilon_{+,M_+(b)}^{*})'$ and
$\Omega_+(h,b)$ is an $M_+(b)$-dimensional vector with $i$th element
$\Omega_{+,1i}(h,b) - \Omega_{+,2i}(h,b)$, which are defined as
\begin{align*}
  \Omega_{+,1i}(h,b) &=
  (1\quad 0)
  \Big(\ssum[+]{j}{h} r_1(X_j / h) r_1(X_j / h)' \Big)^{-1}
    r_1(X_{m_+(b,i)} / h) \1\{h > X_{m_+(b,i)} \geq 0\}
\intertext{and}
  \Omega_{+,2i}(h,b) &= (0\quad 0 \quad h^2)
  \Big(\ssum[+]{j}{b} r_2(X_j / b) r_2(X_j / b)' \Big)^{-1}
    r_2(X_{m_+(b,i)} / b).
\end{align*}
The definitions of $\epb_{-}^{*}$ and $\Omega_-(h,b)$ have the same definitions as
$\epb_{+}^{*}$ and $\Omega_+(h,b)$ after making the obvious substitutions of
``$-$'' for ``$+$.''

Given~\eqref{eq:8}, it suffices to prove that
\begin{gather}
  \label{eq:9}
  \rho(V'(h,b)^{-1/2} \Omega_+(h,b)'\epb_{+}^{*},\
    V'(h,b)^{-1/2}\Omega_+(h,b)'\epb_{+}) \to^p 0
  \intertext{and}
  \label{eq:10}
  \rho(V'(h,b)^{-1/2}\Omega_-(b)'\epb_{-}^{*},\
    V'(h,b)^{-1/2}\Omega_-(b)'\epb_{-}) \to^p 0
\end{gather}
with $\epb_+$ an i.i.d. random vector of length $M_+(b)$, the $i$th element of
which is distributed as
\[
  \varepsilon_{i,+} =^d
  \begin{cases}
    Y_{m_+(1,b)} - \E(Y_{m_+(1,b)} \mid X_{m_+(1,b)})
    & \text{with probability\ } 1/M_+(b) \\
    \vdots \\
    Y_{m_+(M_+(b),b)} - \E(Y_{m_+(M_+(b),b)} \mid X_{m_+(1,b)})
    & \text{with probability\ } 1/M_+(b),
  \end{cases}
\]
$\epb_-$ the corresponding quantity after replacing ``+'' with ``--,'' and
$\rho$ the Mallows metric \citep{bickel1981}.\footnote{%
  For two random vectors $u$ and $v$ with finite 2nd moments, $\rho(u, v)$
  is defined as
  \begin{equation*}
    \rho(u, v) = \inf\nolimits_{(U, V)\ s.t.\ U =^d u,\ V =^d v}
    \big(\E \lVert U - V \rVert^2\big)^{1/2}.
  \end{equation*}}

We will only prove~\eqref{eq:9} since the proof of~\eqref{eq:10} is
identical. By Theorem 2.1 of \cite{freedman1981}, we have
\begin{align*}
  \rho(V'(h,b)^{-1/2} \Omega_+(h,b)'\epb_{+}^{*},\
  &V'(h,b)^{-1/2} \Omega_+(h,b)'\epb_{+}) \\
  &\leq \rho(\varepsilon_{+,1}^{*}, \varepsilon_{+,1}) \times
    \tr(V'(h,b)^{-1/2} \Omega_+(h,b)' \Omega_+(h,b)V'(h,b)^{-1/2}) \\
  &= \rho(\varepsilon_{+,1}^{*}, \varepsilon_{+,1}) \times O_{p^{*}}(1),
\end{align*}
the second line holding as a consequence of CCT's Lemma SA1. So it suffices
to show that $\rho(\varepsilon_{+,1}^{*}, \varepsilon_{+,1}) \to^{p^{*}} 0$. Let
$\dot{\varepsilon}_i$ be an i.i.d. sequence randomly drawn from $\epb_+$
with replacement and let
$\bar{\varepsilon} = (1/M_+(b)) \osum[+]{i}{b} \dot{\varepsilon}_i$. Then,
using the same arguments as \cite{freedman1981}, we have the upper bound
\begin{equation*}
  \rho(\varepsilon_{+,1}^*, \varepsilon_{+,1})
  \leq
  \bar\varepsilon^2 +
  (2/M_+(b)) \osum[+]{i}{b} (\hat\varepsilon_i - \varepsilon_i)^2
  + \rho(\dot\varepsilon_1, \varepsilon_{+,1}).
\end{equation*}
But $\bar\varepsilon^2 \to^p 0$ by the LLN,
\[
  (1/M_+(b)) \osum[+]{i}{b}
  \E((\hat\varepsilon_i - \varepsilon_i)^2 \mid X_i) \to 0
\]
by CCT's Lemma SA1, and
$\rho(\dot\varepsilon_1, \varepsilon_{+,1}) \to^p 0$ by Lemma~8.4 of
\cite{bickel1981}.\qed

\clearpage
\bibliographystyle{jpe}
\bibliography{tex/references}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:

%  LocalWords:  Calonico Cattaneo Titiunik's Econometrica CCT AMSE CCT's DGP
%  LocalWords:  datasets frolich calonico HTV CIs
