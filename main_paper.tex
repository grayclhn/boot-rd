\documentclass[12pt,fleqn]{article}
\input{VERSION}
\input{tex/setup}
\input{tex/macros}

\author{Ot\'avio Bartalotti \and Gray Calhoun \and Yang He\thanks{All authors: 
Department of Economics, Iowa State University. 260 Heady Hall, Ames, IA  50011.
Bartalotti: Email: bartalot@iastate.edu. Calhoun: gcalhoun@iastate.edu.
He: yanghe@iastate.edu.}}

\begin{document}
\maketitle

\begin{abstract}
This paper proposes an novel bootstrap procedure to obtain robust bias-corrected
confidence intervals in regression discontinuity designs. The approach proposed 
is based on a first-order bias correction and provides an alternative
to the plug-in analytical methods proposed by Calonico, Cattaneo and Titiunik (2014).
The algorithm is simple to implement and generates robust confidence intervals
with improved coverage relative to the analytical alternatives in simulations.
\end{abstract}

\section{Introduction}
Regression Discontinuity (RD) designs have emerged in the last decade as an 
important and popular identification strategy to analyze the impact of policies
and interventions in several fields of the social sciences, including economics,
political science, public policy, sociology among others.

The strategy exploits arbitrary rules used to assign treatment to units, usually
relying on some type of ``score,'' the so called running variable. In its basic 
version, units with score above a certain treshold receive treatment while the 
ones below that cutoff are left untreated.

The identification of the treatment effect at the treshold is then based on 
comparing treated and untreated units at the cutoff. As a practical matter this 
involves comparing units within a bandwidth just above and just below the 
treshold. That comparison is most commonly done by implementing a local linear
estimator above and below the cutoff within bandwidths chosen to minimize the 
asymptotic mean squared error (AMSE) as proposed by Imbens and Kalyanaraman(2012)
and extensions by \cite{calonico2014} (CCT, henceforth).

Recently, CCT brought attention to the fact that the popular implementation of RD designs coupling optimal-AMSE bandwidths
and local polynomial estimation provides confidence intervals (CIs) with
incorrect coverage due to the presence of bias and propose an explicit analytical
bias correction procedure that re-centers and re-scale the CI by estimating a higher 
order approximation of the bias term and adjustment its lenght to account for 
the variation introduced by the bias estimation.

As pointed out by \cite{ccf2016}, ``(...)valid inference
requires the delicate balancing act of selecting a bandwidth small enough
to remove smoothing bias, yet large enough to ensure adequate precision.''
The authors compare the strategies of undersmoothing, and explicit bias 
correction concluding that direct bias-correction with the corresponding 
rescalling of the CI's is a superior approach.

This paper contributes to the bias-corrected RD literature by proposing a simple
bootstrap procedure to correct the first order bias term and directly obtain 
valid CIs that provide the correct coverage. The algorithm proposed is easy to
implement and rely on the same conditions as the explicit bias correction
proposed by CCT.

The novel bootstrap procedure is easy to implement and extends to RD designs 
using higher order polynomials, producing valid CIs which are robust to 
bandwidth choice and have similar properties to the analytical procedure in 
CCT while sidestepping the need to derive analytical formulas in each case of interest.

The paper is organized as follows. Section \ref{background} describes the basic
RD approach, its usual implementation, and the explicit analytical bias 
correction approach in the literature. Section \ref{boot} presents the proposed 
bootstrap bias corrected RD algorithm and discusses its properties. Simulation
evidence that the bootstrap procedure provides valid CIs and its relative 
performance to the analytical bias correction are presented in Section \ref{sim}.
Finally, Section \ref{conclusion} concludes.

\section{Background}\label{background}

In the typical sharp RD setting, a researcher wishes to estimate the local 
causal effect of treatment at a given threshold. The running variable, $X_{i}$, 
determines treatment assignment.  Given a known threshold, $\bar{x}$, set to 
zero without loss of generality, a unit receives treatment if $X_{i} \geq 0$ or 
does not receive treatment if $X_{i} < 0$. Let $Y_{i}(1)$ and $Y_{i}(0)$ denote 
the potential outcomes for unit $i$ given it receives treatment and
 in the absence of treatment, respectively. Hence, the observed sample is 
 comprised of the running variable, $X_{i}$, and
 \begin{align}
  Y_{i}=Y_{i}(0) \mathbbm{1}\{X_{i}<0\}+Y_{i}(1) \mathbbm{1}\{X_{i} \geq 0\}
 \end{align}
where $\mathbbm{1}\{ \cdot\}$ denotes the indicator function. For convenience, 
define
 \begin{align}
  \mu(x)= \mathbbm{E}[Y_{i}|X_{i}=x]
 \end{align}
Also, let $\mu^{(\eta)}(x)=\frac{d^{\eta}\mu(x)}{dx^{\eta}}$ be the $\eta^{th}$ 
derivative of the unknown regression function and define 
$\mu^{(\eta)}_{+}=lim_{x \rightarrow 0^{+}}\mu^{(\eta)}(x)$ and 
$\mu^{(\eta)}_{-}=lim_{x \rightarrow 0^{-}}\mu^{(\eta)}(x)$.
In most cases the population parameter of interest is 
$\tau=\mathbbm{E}[Y(1)-Y(0)|X=\bar{x}]$ (i.e., the average treatment effect 
at the threshold). Under continuity and smoothness conditions on both the 
conditional distribution of $X_i$ and the first moments of $Y(0)$ and $Y(1)$ at
the cutoff, $\tau$ is nonparametrically identifiable (Hahn, Todd and Van der 
Klaauw, 2001) by:
\begin{align}
 \tau&= \mu_{+}- \mu_{-} \nonumber \\ 
&\text{ where } \mu_{+}=lim_{x \rightarrow 0^{+}}\mu(x),\text{ and } 
\mu_{-}=lim_{x \rightarrow 0^{-}}\mu(x)
\end{align}

Naturally, the estimation of $\tau$ in RD designs focuses on the 
problem of approximating $\mathbbm{E}[Y(1)|X=x]$ and $\mathbbm{E}[Y(0)|X=x]$ 
near the cutoff. Due to its desirable properties when estimating regression 
functions at boundary points, we consider the popular approach of fitting 
separate kernel-weighted local linear regressions in neighborhoods on both 
sides of the threshold.\footnote{See \cite{HTV2001}, \cite{Porter03} or
\cite{FanGijbels92} for discussions of the properties of local polynomial 
regressions for boundary problems.The results presented here are valid for
higher order polynomials and discontinuities at derivatives, like ``Kink RDD.'' We focus on the linear case for ease in exposition.}
For the local linear model, we use the following estimator as described in 
CCT,\footnote{Throughout the paper we follow the notation on \cite{calonico2014} very closely.}
\begin{align*}
 \hat{\tau}(h_{n})&=\hat {\mu}_{+}(h_{n}) -\hat{\mu}_{-}(h_{n})\\
(\hat {\mu}_{+}(h_{n}),\hat {\mu}^{(1)}_{+}(h_{n}))'&= argmin_{b_{0},b_{1}} 
\sum_{i=1}^{N}\mathbbm{1}\{X_{i} \geq 0\}(Y_{i}-b_{0}-X_{i}b_{1})^{2} \cdot K_{h}(X_{i})\\
(\hat {\mu}_{-}(h_{n}),\hat {\mu}^{(1)}_{-}(h_{n}))' &= argmin_{b_{0},b_{1}}
\sum_{i=1}^{N}\mathbbm{1}\{X_{i} < 0\}(Y_{i}-b_{0}-X_{i}b_{1})^{2}\cdot K_{h}(X_{i})
\end{align*}
where $K_{h}(x_{i}) = K\left(\frac{x_{i}}{h}\right)\frac{1}{h}$.

As shown by \cite{HTV2001}, \cite{Porter03} and expanded by CCT 
identification and inference procedures can be developed based on the following 
assumptions\footnote{The assumptions below are the same as presented by CCT.}
\begin{assumption}\label{A1}
 For some $\kappa_{0} > 0$, the following holds in the neighborhood 
 $(-\kappa_{0},\kappa_{0})$ around the cutoff $ \bar{x}= 0$:
 \begin{enumerate}
  \item $E[\left.Y_{i}^{4}\right|X_{i}=x]$ is bounded and the density of $X$, 
  $f(x)$, is continuous and bounded away from zero.
  \item $\mu_{+}(x)=\mathbbm{E}[\left.Y_{i}(1)\right|X_{i}=x]$ and $\mu_{-}(x)=
  \mathbbm{E}[\left.Y_{i}(0)\right|X_{i}=x]$ are $S$ times continuously 
  differentiable.
  \item $\sigma^{2}_{+}(x)=\mathbbm{V}[\left.Y_{i}(1)\right|X_{i}=x]$ and 
  $\sigma^{2}_{-}(x)=\mathbbm{V}[\left.Y_{i}(0)\right|X_{i}=x]$ are continuous 
  and bounded away from zero.
 \end{enumerate}
\end{assumption}

The conditions in Assumption \ref{A1} are the usual smoothness and existence 
conditions in which the RD literature relies. Its second part is important for 
the characterization of the leading bias term that will be the focus of the bias
correction procedures described in CCT and in the novel
 bootstrap procedure proposed in Section \ref{boot}.

The second set of assumptions regards the possible kernel weights used in RD.

\begin{assumption}\label{A2}
 For some $\kappa>0$, the kernel function $k(\cdot):[0, \kappa] \rightarrow 
 \mathbbm{R}$ is bounded and nonnegative, zero outside its support, and positive
 and continuous on $(0, \kappa)$.
\end{assumption}

Assumption \ref{A2} supports the kernels most commonly used in applications, in 
particular the broadly used uniform kernel $k(u)=\mathbbm{1}[0\leq u\leq 1]$, 
which simplifies to the use of a local linear OLS on both sides of the treshold.
Since the objective of this paper is to present a simple bootstrap procedure to 
obtain robust confidence intervals in the context of RD designs that can be 
easily implemented by practitioners, during the remainder of the paper we focus 
on estimators that use the rectangular kernel. The use of different weighting 
structures implied by different kernels complicate the bootstrap algorithm in
meaningful ways and such analysis is left to future work.

CCT point out that the conventional approaches to construct 
confidence intervals for $\tau$ using the local linear estimator rely on a 
large-sample approximation for the standardized $t$-statistic that is valid only
if the bandwidth shrink fast enough to eliminate the leading bias term 
contribution to the approximation. Hence, if $nh_{n}^{5}\rightarrow0$ and 
$nh_{n}\rightarrow \infty$, then
 \begin{align}
  T(h_{n})=\frac{\hat{\tau}(h_{n})-\tau}{\sqrt{V(h_{n})}}\rightarrow_{d}N(0,1),
  \text{        }V(h_{n})=\mathbbm{V}[\left.\tau(h_{n})\right|\chi_{n}],
  \text{        }\chi_{n}=[X_{1}, \dots X_{n}]^{\prime}
 \end{align}
 The conditional variance $V(h_{n})$ depends on the $\sigma^{2}_{+}=
 lim_{x \rightarrow 0^{+}}\sigma^{2}_{+}(x)$ and $\sigma^{2}_{-}=
 lim_{x \rightarrow 0^{-}}\sigma^{2}_{-}(x)$ as well as $h_{n}$ and known 
 quantities that depend on the kernel and order of polynomial used in the 
 estimation. Under these conditions we could use a conventional confidence 
 interval for $\tau$ given by
 \begin{align}
  I(h_{n})=\left[\hat{\tau}(h_{n})\pm \Phi^{-1}_{1-\frac{\alpha}{2}}\sqrt{V(h_{n})} \right]
 \end{align}
with $\Phi^{-1}_{1-\frac{\alpha}{2}}$ being the $\alpha$-quantile of the 
standard normal distribution. However, most approaches to select the bandwidth, 
$h_{n}$ in the literature, including the widely used optimal-AMSE bandwidth 
selector proposed by \cite{IK} lead to bandwidth choices that are ``too large'' 
since they do not satisfy the condition $nh_{n}^{5}\rightarrow0$, leading to a 
first-order bias in the distributional approximation used to construct the 
confidence intervals.

CCT resolve the problem by deriving the analytical form of the 
first-order bias and correcting it directly. To obtain correct coverage the 
confidence interval needs not only to be recentered to correct the bias, but 
also rescaled to allow for the additional variability introduced by the bias 
correction. Their approach rely on obtaining a bias-corrected  estimator, 
$\hat{\tau}^{bc}$ and its adjusted variance. It is important first to note that 
the first-order bias term itself will depend on the bandwidth $h_{n}$. The 
approximate bias term can be described as:
\begin{align*}
     E[\hat{\tau}(h_{n})|\chi_{n}]-\tau=& h_{n}^{2}\mathsf{B}(h_{n})\{1+o_{p}(1)\}\\
     \mathsf{B}(h_{n})=&\frac{\mu_{+}^{(2)}}{2!}\mathfrak{B}_{+}(h_{n})-\frac{\mu_{-}^{(2)}}{2!}\mathfrak{B}_{-}(h_{n})
   \end{align*}
where $\mathfrak{B}_{+}(h_{n})$ and $\mathfrak{B}_{-}(h_{n})$ are observed 
quantities that depend on the data, kernel and $h_{n}$. The plug-in 
bias-corrected estimator then requires estimates for the second derivatives of 
the conditional mean from above and below the cutoff, $\mu_{+}^{(2)}$ and 
$\mu_{-}^{(2)}$. CCT propose using a conventional local quadratic
estimator, i.e. one order higher than the polynomial used to obtain $\hat{\tau}$,
using a (potentially) different pilot bandwidth $b_{n}$.
\begin{align*}
     \hat{\tau}^{bc}(h_{n}, b_{n})=& \hat{\tau}-h_{n}^{2}\hat{\mathsf{B}}(h_{n},b_{n})\\
     \hat{\mathsf{B}}(h_{n})=&\frac{\hat{\mu}_{+}^{(2)}(b_{n})}{2!}
     \mathfrak{B}_{+}(h_{n})-\frac{\hat{\mu}_{-}^{(2)}(b_{n})}{2!}\mathfrak{B}_{-}(h_{n})
   \end{align*}
By incorporating the contribution of both $\hat{\tau}(h_{n})$ and 
$\hat{\mathsf{B}}(h_{n},b_{n})$ to the asymptotic distribution of the estimator,
\cite{calonico2014} obtain a robust confidence interval with a different 
asymptotic variance in general. So under Assumptions\ref{A1} and \ref{A2}, if 
$n\min\{h_{n}^{5}, b_{n}^{5}\}max\{h_{n}^{2}, b_{n}^{2}\}\rightarrow 0$ and 
$n\min\{h_{n}, b_{n}\}\rightarrow \infty$ and $\kappa \max\{h_{n},b_{n}\}< 
\kappa_{0}$, they show that
\begin{align}
  T^{rbc}(h_{n}, b_{n})=\frac{\hat{\tau}^{bc}(h_{n}, b_{n})-\tau}{\sqrt{V^{bc}(h_{n}, b_{n})}}\rightarrow_{d}N(0,1),
  \text{        }V^{bc}(h_{n}, b_{n})=V(h_{n})+C^{bc}(h_{n}, b_{n})\\
 \end{align}
where the additional term $C^{bc}(h_{n}, b_{n})$ depends on the (asymptotic) 
variability of the bias estimate used for correction as well as its correlation 
with the original RD estimator $\hat{\tau}(h_{n})$.\footnote{The details of the 
formulas are currently ommitted from this paper since their form is not relevant
to the bootstrap procedure innovation proposed. The details can be found at 
CCT Appendix}  Under these conditions we can construct a valid 
confidence interval for $\tau$ given by
 \begin{align}
  I^{rbc}(h_{n}, b_{n})=\left[\left(\hat{\tau}(h_{n})-h_{n}^{2}\hat{\mathsf{B}}
  (h_{n},b_{n})\right)\pm \Phi^{-1}_{1-\frac{\alpha}{2}}\sqrt{V(h_{n})+C^{bc}
  (h_{n}, b_{n})} \right]
 \end{align}

This approach is shown by the authors to significantly coverage of the CIs 
constructed and provides practitioners with a new toolset to perform inference 
that is more robust to the choice of bandwidth. We build upon the insight 
provided by CCT bias-corrected estimator and propose a simple 
bootstrap procedure that can directly construct the robust CIs without requiring
the derivation of analytical formulas and direct estimators for the bias, 
variance and covariance terms, while relying on the same first-order bias 
correction approximation proposed by that paper.

\section{Bootstrap Bias Correction}\label{boot}
Building upon the results and intuition developed by CCT we propose to implement a first-order bias correction and obtain valid CIs through a bootstrap procedure.
Intuitively, for the local-linear RD estimator's case, we will rely on a local higher-order (quadratic) polynomial approximation for the conditional outcome around the cutoff to capture the potential bias present in the original RD estimate. This insight comes directly from CCT using the same local quadratic approximation to estimate bias term as presented in Section \ref{background}.

The first algorithm describes how to use local linear regression to obtain a point estimate and perform bias correction with local quadratic regression. The second algorithm describes how to obtain its confidence interval with correct coverage. For simplicity, we adopt the same optimal-AMSE bandwidth selector proposed by CCT.\footnote{An interesting question is whether one could use a bootstrap based procedure to select a bandwidth that would achieve similar (or improved) results. This is beyond the scope of this particular paper but is part of ongoing research effort.} 

Essentially, we apply CCT's insight that a polynomial one order higher can capture the first-order bias behavior an create a bootstrap procedure that can correctly mimic the behavior of the bias present in the original estimator. For such, for a chosen bandwidth, we estimate a local quadratic regression the same way CCT suggested but then create bootstrap data by imposing the local quadratic functional form as the DGP. Then, in the ``bootstrap data'' the DGP is known. Let the ``true'' treatment effect under the bootstrap DGP be denoted by $\tau^{*}$. We then estimate the treatment effect using the (misspecified) local linear estimator, $\hat{\tau}^{*}$ to the generated data. Hence, the bias for a specific bootstrap data is given by $\tau^{*}-\hat{\tau}^{*}$. More importantly, by generating new bootstrapped data based on the same quadratic DGP and its residual's distribution, we can obtain an estimate of the first-order bias (the part that is captured by $\hat{\mathsf{B}}(h_{n},b_{n})$ in CCT's formulas on Section \ref{background}) in the original estimate by averaging across the bias in all $B$ bootstrapped data estimates,i.e., $\tau^{*} - \sum_{i = 1}^{B} \hat{\tau}_{ i}^{*}$. Algorithm 1 describes this procedure step-by-step for clarity.

\textbf{Algorithm 1}: Bootstrap Bias Corrected Point Estimator
 \begin{enumerate}
	\item Obtain bandwidths for both local linear and quadratic regressions $(h_{n}, b_{n})$ using CCT bandwidth choice algorithm procedure.
	\item Estimate local linear regression on both sides of cutoff using bandwidth from step 1 and calculate $\hat{\tau}(h_{n})$.
	\item Estimate local quadratic regression on both sides of cutoff using bandwidth $b_{n}$ from step 1 and calculate $\tau^{*}$, the difference of two regression functions evaluated at cutoff.
	\item Generate bootstrap data $[Y^{*},\chi^{*}]$ imposing the DGP and and residual's distribution estimated in Step 3 on each sides of cutoff.
	\item Apply step 2 on bootstrap data and obtain $\hat{\tau}^{*}$.
	\item Repeat step 4 and 5 for $B$ times and the bias corrected point estimate is given by $\hat{\tau}^{bc}=\hat{\tau} + \tau^{*} - \sum_{i = 1}^{B} \hat{\tau}_{ i}^{*}$.
 \end{enumerate}

With Algorithm 1 at hand, we can move to generate the bias-corrected confidence intervals to perform inference about $\tau$. Here the idea is similar, but now we actually calculate a fully bias corrected estimate using Algorithm 1 for every bootstrapped dataset generated, a bootstrap within the bootstrap. Each one of the bias corrected estimates for the generated data is denoted $\hat{\tau}^{bc*}$. These estimator's empirical distribution capture the variability induced by the first order bias correction that lead CCT to propose the rescaling of their confidence intervals by introducing $C^{bc}(h_{n},b_{n})$ in the asymptotic variance formula on Section \ref{background}.

\textbf{Algorithm 2}: Confidence Interval
 \begin{enumerate}
	\item Obtain bandwidths for both local linear and quadratic regressions $(h_{n}, b_{n})$ using CCT bandwidth choice algorithm procedure.
	\item Estimate local quadratic regression on both sides of cutoff using bandwidth $b_{n}$ from step 1.
	\item Generate bootstrap data $[Y^{*},\chi^{*}]$ imposing the DGP and and residual's distribution estimated in Step 2 on each sides of cutoff.
	\item Apply Algorithm 1 (starting at step 2) to bootstrap data generated in step 3 and obtain bias corrected estimate $\hat{\tau}^{bc*}$.
	\item Repeat step 3 and 4 for $B$ times and obtain confidence interval formed by the $\frac{\alpha}{2}$ and $\left(1-\frac{\alpha}{2}\right)$ percentiles of the empirical distribution of $\hat{\tau}^{bc*}$ .
 \end{enumerate}

Evidence of the usefulness of the procedures proposed above and their relative 
performance to the analytical bias correction proposed in CCT are presented in a
series of Monte Carlo simulations in Section \ref{sim}.

\section{Simulation Evidence}\label{sim}
This section presents evidence from Monte Carlo simulations that the bootstrap 
bias correction procedures proposed in Section \ref{boot} produces valid, robust
confidence intervals similar to the ones obtained by the analytical procedures 
established in CCT. The bootstrap CIs obtained compare favourably
to the analytical alternative, with coverage closer to the nominal size of the 
desired test and shorter lenght of the intervals in the specifications implemented.

We selected three data generating processes (DGP) that are widely used in the RD
literature to evaluate the performance of the proposed bootstrap procedure, 
those are the same chosen by CCT and \cite{IK} and provide a 
good testing ground for bias correction due to their exacerbated curvature. 
To be specific, the pseudo data are generated as 500 i.i.d. draws from:
\begin{align*}
& Y_{i} = \mu_{j}(X_{i}) + \epsilon_{i}, \;\;\; X_{i} \sim  (2 B (2,4) - 1)), \\
& \epsilon_{i} \sim N(0, \sigma^{2}), \;\;\; \sigma = 0.1295, \;\;\; j = 1,2,3.
\end{align*}
Three different choices of functional forms for $\mu_{j}(x)$ are as follows:
\begin{align}
\mu_{1}(x) & = 
\begin{cases}
0.48 + 1.27x + 7.18x^{2} + 20.21x^{3} + 21.54x^{4} + 7.33x^{5}, \;\;\;\; & \text{if} \;\; x < 0, \\
0.52 + 0.84x - 3.00x^{2} + 7.99x^3 - 9.01x^4 + 3.56x^{5},  & \text{if} \;\; x \ge 0.
\end{cases}
\\
\mu_{2}(x) & = 
\begin{cases}
3.71 + 2.30x + 3.28x^2 + 1.45x^3 + 0.23x^4 + 0.03x^5, \;\; & \text{if} \;\; x < 0, \\
0.26 + 18.49x - 54.81x^2 + 74.30x^3 - 45.02x^4 + 9.83x^5,  & \text{if} \;\; x \ge 0.
\end{cases}
\\
\mu_{3}(x) & =
\begin{cases}
0.48 + 1.27x + 0.5 \times 7.18x^{2}+ \\
+ 0.7 \times 20.21x^3 + 1.1 \times 21.54x^4 + 1.5 \times 7.33x^5, \;\;\;\;\;\;\;\;\;\;\;\; & \text{if} \;\; x < 0, \\
0.52 + 0.84x - 0.1 \times 3.00x^{2}+ \\
+ 0.3 \times 7.99x^3 - 0.1 \times 9.01x^4 + 3.56x^5, & \text{if} \;\; x \ge 0.
\end{cases}
\end{align}

The functional form for $\mu_{1}(x)$ and $\mu_{2}(x)$ follow the data observed 
by by\cite{lee2008} and \cite{ludwig2007}, respectively, while $\mu_{3}(x)$ is
a transformation of $\mu_{1}(x)$ designed to emphasize its curvature and 
therefore bias in the usual RD approach. For the simulation studies, we used 
250 bootstraps for bias correction, 999 bootstraps to calculate confidence 
interval and a total 500 replications. All results presented are based on a 
0.95 desired confidence level. Table \ref{Tb1} presents the main results in which
a local-linear RD estimator ($p=1$) is employed with the bias-correction based 
on a local-quadratic approximation ($q=2$). Table \ref{Tb2} presents similar 
results for $p=2$ and $q=3$.

\begin{table}[ht] \label{Tb1}
	\caption{Summary of Results ($p = 1, q = 2$)}
	\centering
	\begin{tabular}{ccccccc}
		\hline
		DGP & True & Bias & SD & MSE & CI coverage & CI length \\ 
		\hline
		\multicolumn{7}{c}{Panel A: Bootstrap Bias-Correction} \\
		1 & 0.04 & -0.013 & 0.067 & 0.068 & 0.938 & 0.243 \\ 
		2 & -3.45 & -0.014 & 0.083 & 0.084 & 0.954 & 0.319 \\ 
		3 & 0.04 & -0.006 & 0.069 & 0.070 & 0.956 & 0.247 \\ 
		&&&&&& \\
		\multicolumn{7}{c}{Panel B: Analytical Bias Correction (CCT 2014)} \\
		1 & 0.04 & -0.016 & 0.068 & 0.070 & 0.908 & 0.244 \\ 
		2 & -3.45 & -0.014 & 0.092 & 0.093 & 0.928 & 0.350 \\ 
		3 & 0.04 & -0.004 & 0.071 & 0.071 & 0.918 & 0.249 \\ 
		\hline
	\end{tabular}
\end{table}

\begin{table}[ht]\label{Tb2}
	\caption{Summary of Results ($p = 2, q = 3$)}
	\centering
	\begin{tabular}{ccccccc}
		\hline
		DGP & True & Bias & SD & MSE & CI coverage & CI length \\ 
		\hline
		\multicolumn{7}{c}{Panel A: Uniform Kernel} \\
		1 & 0.04 & -0.002 & 0.075 & 0.075 & 0.958 & 0.295 \\ 
		2 & -3.45 & 0.002 & 0.087 & 0.086 & 0.934 & 0.302 \\ 
		3 & 0.04 & -0.008 & 0.083 & 0.083 & 0.942 & 0.292 \\ 
		&&&&&& \\
		\multicolumn{7}{c}{Panel B: Analytical Bias Correction (CCT 2014)} \\
		1 & 0.04 & -0.004 & 0.082 & 0.082 & 0.926 & 0.295 \\ 		
		2 & -3.45 & -0.005 & 0.078 & 0.078 & 0.960 & 0.323 \\ 		
		3 & 0.04 & -0.003 & 0.080 & 0.080 & 0.938 & 0.295 \\ 
		\hline
	\end{tabular}
\end{table}

As can be seen on the tables above, the bootstrap bias-correction procedure
proposed in this paper provides a simple alternative to obtain valid robust
confidence intervals in RD designs and performs well compared to the analytical 
bias correction procedures proposed by CCT.

\section{Conclusion}\label{conclusion}

%%%%%REFERENCES%%%%%%%%%
\clearpage
\bibliographystyle{jpe}
\bibliography{bootrd}

\end{document}